<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Probability and Information Theory - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../../deeptech.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Probability and Information Theory</h1>

<h2>Chapter Overview</h2>

<p>Deep learning is fundamentally a probabilistic framework. Neural networks learn probability distributions over data, make predictions with uncertainty, and are trained using probabilistic objectives. This chapter develops the probability theory and information theory necessary to understand these probabilistic aspects of deep learning.</p>

<p>We cover probability distributions, conditional probability, expectation, and variance‚Äîthe building blocks for understanding neural network outputs as probabilistic models. We then introduce information theory concepts like entropy, cross-entropy, and KL divergence, which form the basis for loss functions used in training.</p>

<h3>Learning Objectives</h3>

<p>After completing this chapter, you will be able to:</p>

<ol>
    <li>Work with probability distributions and compute expectations
    <li>Apply Bayes' theorem to understand conditional probabilities
    <li>Understand entropy as a measure of uncertainty
    <li>Derive and apply cross-entropy loss for classification
    <li>Use KL divergence to measure distribution differences
    <li>Interpret neural network outputs as probability distributions
</ol>

<div class="architecture-diagram">
<h3>Information-Theoretic Framework for Deep Learning</h3>
<pre class="mermaid">
graph TD
    A["Data Distribution<br/>P(X)"] --> B["Encoder<br/>P(Z|X)"]
    B --> C["Latent Space<br/>Z"]
    C --> D["Decoder<br/>P(XÃÇ|Z)"]
    D --> E["Reconstruction<br/>XÃÇ"]

    A --> F["Entropy<br/>H(X) = -Œ£ p log p"]
    B --> G["KL Divergence<br/>D_KL(P‚ÄñQ)"]
    G --> H["Cross-Entropy Loss<br/>H(P,Q) = H(P) + D_KL(P‚ÄñQ)"]

    F --> H
    H --> I["Training Objective:<br/>Minimize Cross-Entropy"]

    style A fill:#e8f5e9,stroke:#4caf50,color:#000
    style C fill:#fff3e0,stroke:#ff9800,color:#000
    style I fill:#e3f2fd,stroke:#2196f3,color:#000
</pre>
<p class="diagram-caption">Figure: Information theory provides the mathematical foundation for loss functions and representation learning in deep learning.</p>
</div>



<h2>Probability Fundamentals</h2>

<h3>Random Variables and Distributions</h3>

<div class="definition"><strong>Definition:</strong> 
A <strong>random variable</strong> $X$ is a function that maps outcomes from a sample space to real numbers. We distinguish between:
<ul>
    <li><strong>Discrete random variables</strong>: Take countable values (e.g., class labels)
    <li><strong>Continuous random variables</strong>: Take values in continuous ranges
</ul>
</div>

<div class="definition"><strong>Definition:</strong> 
For discrete random variable $X$, the <strong>probability mass function</strong> is:
<div class="equation">
$$
P(X = x) = p(x)
$$
</div>
satisfying: (1) $0 \leq p(x) \leq 1$ for all $x$, and (2) $\sum_x p(x) = 1$
</div>

<div class="example"><strong>Example:</strong> 
In image classification with 10 classes (digits 0-9), a neural network outputs a probability distribution using softmax:
<div class="equation">
$$
P(Y = k | \vx) = \frac{\exp(z_k)}{\sum_{j=1}^{10} \exp(z_j)}
$$
</div>

<p>For logits $\vz = [2.1, 0.5, -1.2, 3.4, 0.8, -0.5, 1.1, -2.0, 0.3, 1.8]$, the model predicts class 3 with highest probability $\approx 68.9\%$.
</div>

<h3>Conditional Probability and Bayes' Theorem</h3>

<div class="definition"><strong>Definition:</strong> 
The probability of event $A$ given event $B$:
<div class="equation">
$$
P(A|B) = \frac{P(A \cap B)}{P(B)} \quad \text{if } P(B) > 0
$$
</div>
</div>

<div class="theorem"><strong>Theorem:</strong> 
For events $A$ and $B$ with $P(B) > 0$:
<div class="equation">
$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
$$
</div>
where $P(A|B)$ is the posterior, $P(B|A)$ is the likelihood, $P(A)$ is the prior, and $P(B)$ is the evidence.
</div>

<h2>Information Theory</h2>

<h3>Entropy</h3>

<div class="definition"><strong>Definition:</strong> 
For discrete random variable $X$ with PMF $p(x)$:
<div class="equation">
$$
H(X) = -\sum_x p(x) \ln p(x) = \mathbb{E}[-\ln P(X)]
$$
</div>
</div>

<p>Entropy measures average uncertainty. Higher entropy means more uncertainty.</p>

<div class="example"><strong>Example:</strong> 
<strong>Fair coin:</strong> $P(\text{heads}) = P(\text{tails}) = 0.5$
<div class="equation">
$$
H = -[0.5 \log_2(0.5) + 0.5 \log_2(0.5)] = 1 \text{ bit (maximum)}
$$
</div>

<p><strong>Biased coin:</strong> $P(\text{heads}) = 0.9$, $P(\text{tails}) = 0.1$
<div class="equation">
$$
H \approx 0.469 \text{ bits (lower, more predictable)}
$$
</div>
</div>

<h3>Cross-Entropy</h3>

<div class="definition"><strong>Definition:</strong> 
For true distribution $p$ and predicted distribution $q$:
<div class="equation">
$$
H(p, q) = -\sum_x p(x) \log q(x) = \mathbb{E}_{x \sim p}[-\log q(x)]
$$
</div>
</div>

<div class="theorem"><strong>Theorem:</strong> 
For true label $y$ and predicted probabilities $\hat{\mathbf{p}}$:
<div class="equation">
$$
L = -\log \hat{p}_y
$$
</div>
</div>

<div class="example"><strong>Example:</strong> 
For 3-class classification with true label $y=2$:
<ul>
    <li>Predicted: $\hat{\mathbf{p}} = [0.2, 0.6, 0.2]$ $\Rightarrow$ $L = -\log(0.6) \approx 0.511$
    <li>More confident: $\hat{\mathbf{p}} = [0.1, 0.8, 0.1]$ $\Rightarrow$ $L = -\log(0.8) \approx 0.223$ (better)
    <li>Wrong prediction: $\hat{\mathbf{p}} = [0.7, 0.2, 0.1]$ $\Rightarrow$ $L = -\log(0.2) \approx 1.609$ (bad)
</ul>
</div>

<div class="implementation">
PyTorch cross-entropy loss:
<pre><code>import torch
import torch.nn as nn

# Logits: shape (batch_size, num_classes)
logits = torch.tensor([[2.0, 1.0, 0.1],
                       [0.5, 2.5, 1.0]])
labels = torch.tensor([0, 1])

# CrossEntropyLoss applies softmax internally
criterion = nn.CrossEntropyLoss()
loss = criterion(logits, labels)
print(f"Loss: {loss.item():.4f}")
</code></pre>
</div>

<h3>Kullback-Leibler Divergence</h3>

<div class="definition"><strong>Definition:</strong> 
The KL divergence from distribution $q$ to $p$:
<div class="equation">
$$
D_{\text{KL}}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = H(p, q) - H(p)
$$
</div>
</div>

<p>Properties: (1) $D_{\text{KL}}(p \| q) \geq 0$ with equality iff $p = q$, (2) Not symmetric: $D_{\text{KL}}(p \| q) \neq D_{\text{KL}}(q \| p)$</p>

<div class="keypoint">
Minimizing KL divergence is equivalent to minimizing cross-entropy when $p$ is fixed. Training neural networks with cross-entropy loss is maximum likelihood estimation.
</div>

<h2>Cross-Entropy Loss: Computational and Memory Analysis</h2>

<p>While cross-entropy loss appears simple mathematically, its implementation in large-scale language models presents significant computational and memory challenges. Understanding these practical considerations is essential for training modern transformers efficiently.</p>

<h3>Memory Requirements for Logits</h3>

<p>The memory footprint of cross-entropy computation is dominated by the logits tensor before softmax normalization. For a language model with batch size $B$, sequence length $n$, and vocabulary size $V$, the logits tensor has shape $B \times n \times V$ and requires $4BnV$ bytes in FP32 (or $2BnV$ bytes in FP16). This memory requirement becomes the primary bottleneck for models with large vocabularies.</p>

<p>Consider BERT-base with vocabulary size $V = 30{,}000$. For a batch of $B = 32$ sequences with length $n = 512$, the logits tensor requires $32 \times 512 \times 30{,}000 \times 4 = 1{,}966{,}080{,}000$ bytes, or approximately 1.83 GB of GPU memory just for the unnormalized scores. This is before computing the softmax normalization, which requires storing both the exponentials and the normalization constants. The total memory for the forward pass of cross-entropy loss approaches 3.7 GB for this single operation, consuming a substantial fraction of an NVIDIA V100's 16 GB memory or nearly half of a consumer RTX 3090's 24 GB.</p>

<p>The situation becomes more severe for larger models. GPT-2 with vocabulary size $V = 50{,}257$ requires 3.1 GB for logits alone with the same batch configuration. GPT-3 and modern large language models often use vocabularies exceeding 50,000 tokens, making the logits tensor one of the largest single memory allocations during training. This explains why vocabulary size is a critical hyperparameter that directly impacts the maximum batch size and sequence length that can fit in GPU memory.</p>

<p>The memory pressure is particularly acute during the backward pass. The gradient of the cross-entropy loss with respect to the logits has the same shape $B \times n \times V$ as the forward logits, effectively doubling the memory requirement. Additionally, the softmax operation requires storing its output for the backward pass, adding another $4BnV$ bytes. The total memory for cross-entropy loss computation (forward and backward) approaches $12BnV$ bytes, or approximately 11 GB for BERT-base with the configuration above.</p>

<h3>Computational Cost of Softmax</h3>

<p>The softmax operation itself has computational complexity $O(BnV)$ for the forward pass, requiring one exponential operation per logit and a sum reduction over the vocabulary dimension. Modern GPUs can compute exponentials efficiently, but the memory bandwidth required to read and write the large logits tensor often becomes the bottleneck rather than the arithmetic operations themselves.</p>

<p>For BERT-base processing a batch of 32 sequences with 512 tokens each, the softmax operation must compute $32 \times 512 \times 30{,}000 = 491{,}520{,}000$ exponentials. On an NVIDIA A100 GPU with peak FP32 throughput of 19.5 TFLOPS, the arithmetic alone would take approximately 25 microseconds if perfectly parallelized. However, the actual runtime is dominated by memory bandwidth: reading 1.83 GB of logits and writing 1.83 GB of probabilities requires approximately 2.4 milliseconds at the A100's memory bandwidth of 1.5 TB/s, making the operation 100√ó slower than the arithmetic-limited case. This memory bandwidth bottleneck is characteristic of softmax and explains why vocabulary size has such a direct impact on training speed.</p>

<p>The backward pass of softmax requires computing the Jacobian matrix, but due to the structure of the softmax function, this can be done efficiently without materializing the full $V \times V$ Jacobian. The gradient computation has the same $O(BnV)$ complexity as the forward pass and similar memory bandwidth requirements. The total time for forward and backward softmax over large vocabularies typically accounts for 15-25\% of the total training time per batch, making it a significant optimization target.</p>

<h3>Optimizations for Large Vocabularies</h3>

<p>Several techniques have been developed to address the memory and computational costs of cross-entropy loss with large vocabularies. These optimizations are essential for training modern language models efficiently.</p>

<p><strong>Sampled Softmax</strong> reduces computational cost by approximating the full softmax using only a subset of the vocabulary. During training, instead of computing the normalization constant over all $V$ tokens, we sample a small set of $K$ negative examples (typically $K \approx 1{,}000$ to $10{,}000$) and compute softmax over only the true token plus the $K$ samples. This reduces the per-token computational cost from $O(V)$ to $O(K)$, providing speedups of 5-50√ó for large vocabularies. The sampling is typically done using a proposal distribution that approximates the unigram frequency of tokens, ensuring that common words are sampled more frequently than rare words.</p>

<p>The memory savings are equally dramatic. With sampled softmax, the logits tensor has shape $B \times n \times (K+1)$ instead of $B \times n \times V$, reducing memory from 1.83 GB to approximately 61 MB for BERT-base with $K = 1{,}000$ samples. This 30√ó memory reduction enables much larger batch sizes or longer sequences, directly improving training efficiency. However, sampled softmax introduces bias in the gradient estimates, which can slow convergence. In practice, it is most effective during the early stages of training and is often replaced with full softmax for final fine-tuning.</p>

<p><strong>Adaptive Softmax</strong> exploits the Zipfian distribution of natural language, where a small number of frequent words account for the majority of token occurrences. The vocabulary is partitioned into clusters based on frequency: frequent words in a small head cluster, and rare words in larger tail clusters. The model first predicts which cluster the token belongs to, then predicts the specific token within that cluster. This hierarchical approach reduces the effective vocabulary size for most predictions from $V$ to approximately $\sqrt{V}$, providing substantial speedups for very large vocabularies.</p>

<p>For a vocabulary of $V = 100{,}000$ tokens partitioned into clusters of sizes $[1{,}000, 9{,}000, 90{,}000]$, the average computational cost per token is approximately $3 + 1{,}000 = 1{,}003$ operations (cluster prediction plus within-cluster prediction for the head cluster) compared to $100{,}000$ for full softmax. This represents a 100√ó speedup for the most common tokens. The memory requirements are similarly reduced, as the model only needs to store logits for the predicted cluster rather than the full vocabulary. Adaptive softmax is particularly effective for language modeling tasks where the token distribution is highly skewed, and it has been successfully used in models like Transformer-XL and adaptive input representations.</p>

<p><strong>Vocabulary Pruning and Subword Tokenization</strong> address the problem at its source by reducing the vocabulary size $V$ itself. Subword tokenization methods like Byte-Pair Encoding (BPE) and WordPiece decompose rare words into common subword units, allowing models to use vocabularies of 30,000-50,000 tokens instead of 100,000+ word-level tokens while maintaining good coverage. This directly reduces both memory and computation by a factor of 2-3√ó. Modern models like GPT-3 and BERT use vocabularies of approximately 50,000 tokens, carefully chosen to balance vocabulary size against the increased sequence length from subword splitting.</p>

<p>The choice of vocabulary size involves a fundamental trade-off: smaller vocabularies reduce memory and computation per token but increase sequence length (more tokens per sentence), while larger vocabularies reduce sequence length but increase per-token costs. For transformer models where attention has $O(n^2)$ complexity, reducing sequence length by using larger vocabularies can actually improve overall efficiency despite the higher per-token costs. Empirical studies suggest that vocabularies of 32,000-50,000 tokens provide a good balance for most languages, though this varies with the specific language and domain.</p>

<h2>KL Divergence in Practice</h2>

<p>KL divergence appears throughout modern deep learning as a measure of distribution similarity. Understanding its computational properties and applications is essential for implementing techniques like variational autoencoders, knowledge distillation, and reinforcement learning from human feedback.</p>

<h3>Applications in Modern Deep Learning</h3>

<p><strong>Variational Autoencoders (VAEs)</strong> use KL divergence as a regularization term to ensure that the learned latent distribution $q(z|x)$ remains close to a prior distribution $p(z)$, typically a standard Gaussian. The VAE loss function combines reconstruction loss with a KL divergence term:
<div class="equation">
$$
\mathcal{L}_{\text{VAE}} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{\text{KL}}(q(z|x) \| p(z))
$$
</div>

<p>For a Gaussian encoder with mean $\mu$ and variance $\sigma^2$, the KL divergence to a standard Gaussian has a closed form:
<div class="equation">
$$
D_{\text{KL}}(q \| p) = \frac{1}{2} \sum_{i=1}^{d} (\mu_i^2 + \sigma_i^2 - \log \sigma_i^2 - 1)
$$
</div>

<p>This closed form makes VAEs computationally efficient, as the KL term requires only $O(d)$ operations for a $d$-dimensional latent space, typically much smaller than the reconstruction loss computation. For a VAE with 512-dimensional latent space, the KL divergence computation takes less than 1 microsecond on a modern GPU, making it negligible compared to the encoder and decoder networks.</p>

<p><strong>Knowledge Distillation</strong> transfers knowledge from a large teacher model to a smaller student model by minimizing the KL divergence between their output distributions. The student is trained to match not just the hard labels but the full probability distribution produced by the teacher:
<div class="equation">
$$
\mathcal{L}_{\text{distill}} = \alpha \mathcal{L}_{\text{CE}}(y, \hat{y}_{\text{student}}) + (1-\alpha) T^2 D_{\text{KL}}(\hat{y}_{\text{teacher}} \| \hat{y}_{\text{student}})
$$
</div>

<p>where $T$ is a temperature parameter that softens the distributions. The KL divergence term encourages the student to learn the relative confidences between classes that the teacher has learned, not just the most likely class. This is particularly valuable when the teacher assigns non-negligible probability to multiple classes, indicating genuine ambiguity or similarity between categories.</p>

<p>The computational cost of knowledge distillation is dominated by running both teacher and student models, with the KL divergence computation itself being relatively cheap at $O(BnV)$ for batch size $B$, sequence length $n$, and vocabulary size $V$. For BERT-base distillation with vocabulary size 30,000, computing the KL divergence over a batch of 32 sequences with 512 tokens requires approximately 1.5 milliseconds on an A100 GPU, compared to 50-100 milliseconds for the forward passes of the teacher and student models. The memory overhead is also modest, requiring storage of both teacher and student logits but no additional activations for backpropagation through the teacher.</p>

<p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong> uses KL divergence to constrain the policy learned through reinforcement learning to remain close to the original supervised fine-tuned model. This prevents the model from exploiting the reward model by generating adversarial outputs that score highly but are nonsensical. The RLHF objective includes a KL penalty term:
<div class="equation">
$$
\mathcal{L}_{\text{RLHF}} = \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta}[r(x, y)] - \beta D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})
$$
</div>

<p>where $\pi_\theta$ is the policy being optimized, $\pi_{\text{ref}}$ is the reference model, and $\beta$ controls the strength of the KL constraint. Computing this KL divergence requires running both the policy and reference models on the same inputs and computing the divergence over the vocabulary at each token position. For large language models with vocabularies of 50,000+ tokens and sequences of 1,000+ tokens, this KL computation can consume 10-20\% of the total training time, making it a non-negligible cost in RLHF training pipelines.</p>

<h3>Numerical Stability Considerations</h3>

<p>Computing KL divergence naively can lead to numerical instability due to the logarithm of very small probabilities. When $q(x)$ is close to zero, $\log q(x)$ approaches negative infinity, and the product $p(x) \log q(x)$ can produce NaN values or catastrophic cancellation. Similarly, when computing $\log(p(x)/q(x))$, direct division can lose precision for very small probabilities.</p>

<p>The numerically stable approach computes KL divergence in log-space using the log-sum-exp trick. Instead of computing probabilities via softmax and then taking logarithms, we work directly with log-probabilities:
<div class="equation">
$$
D_{\text{KL}}(p \| q) = \sum_x p(x) (\log p(x) - \log q(x)) = \sum_x \exp(\log p(x)) \cdot (\log p(x) - \log q(x))
$$
</div>

<p>This formulation avoids computing very small probabilities explicitly. Modern deep learning frameworks like PyTorch provide <code>F.kl\_div</code> that operates on log-probabilities directly, ensuring numerical stability even when probabilities span many orders of magnitude.</p>

<p>Another source of instability arises when $p(x) > 0$ but $q(x) = 0$, which makes the KL divergence infinite. In practice, this occurs when the model assigns zero probability to an event that actually occurs in the data. To prevent this, implementations typically add a small epsilon ($\epsilon \approx 10^{-8}$) to probabilities before computing logarithms, or use label smoothing to ensure that the target distribution $p$ never assigns exactly zero probability to any class. Label smoothing replaces hard targets with a mixture of the true label and a uniform distribution:
<div class="equation">
$$
p_{\text{smooth}}(x) = (1 - \epsilon) p_{\text{true}}(x) + \epsilon / V
$$
</div>

<p>where $\epsilon \approx 0.1$ is typical. This not only improves numerical stability but also acts as a regularizer that prevents overconfident predictions and often improves generalization.</p>

<h2>Hardware Implications of Softmax and Large Vocabularies</h2>

<p>The softmax operation and large vocabulary sizes have profound implications for hardware utilization and training efficiency. Understanding these hardware-level considerations is essential for optimizing transformer training and deployment.</p>

<h3>Softmax Computation on GPUs</h3>

<p>Softmax is a memory-bandwidth-bound operation rather than compute-bound on modern GPUs. The operation requires reading the input logits, computing exponentials, summing the exponentials, and writing the normalized probabilities. For a vocabulary of size $V$, this involves reading and writing $2V$ values while performing only $2V$ arithmetic operations (one exponential and one division per element). On an NVIDIA A100 GPU with 312 TFLOPS of FP16 compute but only 1.5 TB/s of memory bandwidth, the arithmetic could theoretically complete in nanoseconds, but the memory transfers take microseconds.</p>

<p>Consider computing softmax over a vocabulary of $V = 50{,}000$ tokens for a single position. Reading 50,000 FP32 values (200 KB) and writing 50,000 FP32 values (200 KB) requires 400 KB of memory bandwidth. At the A100's bandwidth of 1.5 TB/s, this takes approximately 0.27 microseconds. The arithmetic operations (50,000 exponentials and 50,000 divisions) would take approximately 0.003 microseconds at peak throughput, making the operation 90√ó memory-bandwidth-limited. This ratio worsens for smaller vocabularies and improves slightly for larger ones, but softmax remains fundamentally bandwidth-bound across all practical vocabulary sizes.</p>

<p>The memory-bandwidth-bound nature of softmax has several implications. First, reducing precision from FP32 to FP16 provides nearly 2√ó speedup by halving the memory traffic, with minimal impact on accuracy for most applications. Second, fusing the softmax operation with subsequent operations (like the cross-entropy loss computation) can eliminate intermediate memory traffic by keeping values in registers, providing additional speedups of 1.5-2√ó. Modern deep learning frameworks implement fused softmax-cross-entropy kernels that compute both operations in a single GPU kernel, reducing memory traffic from $4V$ to $2V$ values per position.</p>

<p>Third, the memory-bandwidth bottleneck means that vocabulary size has a nearly linear impact on softmax runtime. Doubling the vocabulary from 25,000 to 50,000 tokens approximately doubles the softmax computation time, as the memory traffic doubles while the arithmetic remains negligible. This linear scaling makes vocabulary size one of the most direct levers for controlling training speed, and it explains why careful vocabulary selection is critical for efficient training.</p>

<h3>Memory Bandwidth and Large Vocabularies</h3>

<p>The memory bandwidth requirements of large vocabularies extend beyond just the softmax operation to the entire forward and backward pass of the language model head. The final linear layer that projects from the model dimension $d$ to the vocabulary size $V$ has weight matrix $\mathbf{W} \in \mathbb{R}^{d \times V}$, which must be read from memory for every forward pass and written during every backward pass.</p>

<p>For BERT-base with $d = 768$ and $V = 30{,}000$, this weight matrix contains $768 \times 30{,}000 = 23{,}040{,}000$ parameters, requiring 92 MB in FP32 or 46 MB in FP16. For a batch of $B = 32$ sequences with $n = 512$ tokens, the forward pass must read this 92 MB matrix once and perform $32 \times 512 \times 768 \times 30{,}000 = 377$ billion multiply-accumulate operations. On an A100 GPU, reading 92 MB takes approximately 61 microseconds at 1.5 TB/s bandwidth, while the arithmetic takes approximately 600 microseconds at 312 TFLOPS FP16 throughput. In this case, the operation is compute-bound rather than bandwidth-bound, but the memory bandwidth still accounts for approximately 10\% of the total time.</p>

<p>The situation changes dramatically for smaller batch sizes or shorter sequences. With batch size $B = 1$ and sequence length $n = 1$ (as in autoregressive generation), the forward pass performs only $768 \times 30{,}000 = 23$ million operations, taking 0.07 microseconds at peak throughput. The memory bandwidth to read the weight matrix remains 61 microseconds, making the operation 870√ó bandwidth-bound. This explains why autoregressive generation is so much slower than parallel training: the small batch size prevents amortizing the memory bandwidth cost over many operations, and the model spends most of its time waiting for memory rather than computing.</p>

<p>The backward pass has similar memory bandwidth requirements but must also write the gradient of the weight matrix, doubling the memory traffic. For the BERT-base example, the backward pass requires reading 92 MB (weights) and writing 92 MB (gradients), totaling 184 MB of memory traffic. This takes approximately 122 microseconds, compared to approximately 600 microseconds for the arithmetic, making the backward pass approximately 17\% bandwidth-bound. The total memory bandwidth for the forward and backward pass of the language model head is 276 MB per batch, which accumulates to significant overhead over thousands of training steps.</p>

<h3>Why Vocabulary Size Impacts Training Speed</h3>

<p>Vocabulary size impacts training speed through three primary mechanisms: memory capacity, memory bandwidth, and arithmetic operations. Understanding the relative contribution of each mechanism helps guide optimization strategies.</p>

<p><strong>Memory Capacity:</strong> As discussed in Section~[ref], the logits tensor requires $4BnV$ bytes, which directly limits the maximum batch size and sequence length that fit in GPU memory. For BERT-base with $V = 30{,}000$, reducing the vocabulary to $V = 15{,}000$ would halve the logits memory from 1.83 GB to 915 MB, allowing a 2√ó larger batch size or 1.4√ó longer sequences (since attention memory scales as $O(n^2)$). Larger batch sizes improve GPU utilization and reduce the number of training steps required for convergence, directly improving training efficiency.</p>

<p>The memory capacity constraint is particularly severe for large language models. GPT-3 with 175 billion parameters requires approximately 700 GB of memory just for the model weights and optimizer states in FP32 (or 350 GB in FP16 with mixed precision). Adding 1.83 GB for logits might seem negligible, but when training across multiple GPUs with model parallelism, the logits must be replicated on each GPU that computes the language model head, multiplying the memory cost. For a model parallelized across 8 GPUs, the logits consume 14.6 GB of total memory, which becomes significant relative to the per-GPU memory budget.</p>

<p><strong>Memory Bandwidth:</strong> The softmax operation and language model head are bandwidth-bound, as discussed above. Reducing vocabulary size from 50,000 to 25,000 tokens reduces the memory traffic for softmax by 2√ó, directly improving runtime by approximately 2√ó for this operation. Since softmax and the language model head together account for 20-30\% of total training time, this translates to an overall speedup of approximately 1.15-1.2√ó for the entire training pipeline. This speedup is achieved without any loss in model quality, making vocabulary reduction through subword tokenization one of the most effective optimizations for language model training.</p>

<p><strong>Arithmetic Operations:</strong> The language model head performs $BndV$ multiply-accumulate operations, which scales linearly with vocabulary size. However, as discussed above, this operation is typically compute-bound only for large batch sizes, and the arithmetic cost is often dominated by memory bandwidth. For very large vocabularies ($V > 100{,}000$) and large batch sizes, the arithmetic can become significant, but for typical configurations with $V \approx 30{,}000$-$50{,}000$, memory bandwidth is the primary bottleneck.</p>

<p>The combined effect of these three mechanisms means that vocabulary size has a superlinear impact on training speed. Doubling the vocabulary size reduces the maximum batch size (due to memory capacity), increases the per-batch runtime (due to memory bandwidth and arithmetic), and may require more training steps (due to the smaller batch size). Empirically, doubling the vocabulary from 25,000 to 50,000 tokens typically increases total training time by 1.5-2√ó, making vocabulary selection a critical hyperparameter for efficient training.</p>

<h3>Optimization Techniques</h3>

<p>Several techniques mitigate the hardware costs of large vocabularies. <strong>Vocabulary pruning</strong> removes rare tokens that appear fewer than a threshold number of times in the training data, reducing vocabulary size without significantly impacting coverage. For example, removing tokens that appear fewer than 100 times in a large corpus might reduce vocabulary from 50,000 to 35,000 tokens while affecting less than 0.1\% of tokens in the data. This 30\% reduction in vocabulary size provides approximately 1.2√ó training speedup with negligible impact on model quality.</p>

<p><strong>Subword tokenization</strong> methods like BPE and WordPiece achieve smaller vocabularies by decomposing rare words into common subword units. This allows models to handle unlimited vocabulary with a fixed-size token set, typically 30,000-50,000 tokens. The trade-off is increased sequence length, as rare words are split into multiple tokens. However, for transformer models where attention has $O(n^2)$ complexity, the increased sequence length is often offset by the reduced per-token costs, resulting in net speedups of 1.3-1.5√ó compared to word-level tokenization.</p>

<p><strong>Adaptive softmax</strong> and <strong>sampled softmax</strong>, discussed in Section~[ref], provide algorithmic approaches to reducing the computational cost of large vocabularies. These techniques are particularly effective for very large vocabularies ($V > 100{,}000$) where the vocabulary size dominates training time. For typical transformer models with vocabularies of 30,000-50,000 tokens, the simpler approaches of vocabulary pruning and subword tokenization are often sufficient and easier to implement.</p>

<p><strong>Model parallelism</strong> distributes the vocabulary across multiple GPUs, allowing each GPU to compute softmax over only a subset of the vocabulary. For a vocabulary of 50,000 tokens distributed across 4 GPUs, each GPU computes softmax over 12,500 tokens, reducing the per-GPU memory and computation by 4√ó. However, this requires an all-reduce operation to compute the global normalization constant, which adds communication overhead. Model parallelism for the vocabulary is most effective for very large vocabularies and when training on high-bandwidth interconnects like NVLink or InfiniBand.</p>

<p>The choice of optimization technique depends on the specific model and training configuration. For most applications, subword tokenization with vocabularies of 30,000-50,000 tokens provides a good balance between vocabulary size and sequence length. For very large models or very large vocabularies, adaptive softmax or model parallelism may be necessary to achieve acceptable training speeds.</p>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> A neural network outputs $\hat{\mathbf{p}} = [0.15, 0.60, 0.20, 0.05]$ for 4 classes. Compute: (1) entropy $H(\hat{\mathbf{p}})$, (2) cross-entropy loss if true label is class 2, (3) optimal output distribution.
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Show that $H(p, q) = H(p) + D_{\text{KL}}(p \| q)$, proving cross-entropy minimization equals KL divergence minimization when $p$ is fixed.
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> For binary classifier with $\hat{p} = 0.8$ and true label class 1: (1) Compute binary cross-entropy loss, (2) Find $\frac{\partial L}{\partial \hat{p}}$, (3) Compare loss for $\hat{p} \in \{0.99, 0.2\}$.
</div>

<div class="exercise" id="exercise-4"><strong>Exercise 4:</strong> Calculate the memory requirements for storing logits in a GPT-2 model with vocabulary size $V = 50{,}257$, batch size $B = 16$, and sequence length $n = 1024$. How much memory is saved by using FP16 instead of FP32? If you have an NVIDIA A100 with 40 GB of memory, what is the maximum batch size you can use if logits consume at most 25\% of available memory?
</div>

<div class="exercise" id="exercise-5"><strong>Exercise 5:</strong> For sampled softmax with $K = 5{,}000$ negative samples and vocabulary size $V = 100{,}000$: (1) Calculate the speedup factor for the forward pass compared to full softmax, (2) Compute the memory reduction for a batch of 32 sequences with 512 tokens each, (3) Discuss why sampled softmax introduces bias in gradient estimates.
</div>

<div class="exercise" id="exercise-6"><strong>Exercise 6:</strong> An NVIDIA A100 GPU has memory bandwidth of 1.5 TB/s and FP16 compute throughput of 312 TFLOPS. For softmax over a vocabulary of $V = 30{,}000$ tokens: (1) Calculate the time to read and write the logits and probabilities (400 KB total), (2) Calculate the time to compute 30,000 exponentials and divisions at peak throughput, (3) Determine whether the operation is compute-bound or bandwidth-bound and by what factor.
</div>

<div class="exercise" id="exercise-7"><strong>Exercise 7:</strong> In knowledge distillation, the KL divergence loss is scaled by $T^2$ where $T$ is the temperature parameter. Explain why this scaling is necessary by: (1) Showing how temperature affects the magnitude of gradients, (2) Deriving the gradient of $D_{\text{KL}}(\text{softmax}(\mathbf{z}/T) \| \text{softmax}(\mathbf{z}'/T))$ with respect to $\mathbf{z}'$, (3) Demonstrating that without $T^2$ scaling, the distillation loss would vanish as $T \to \infty$.
</div>

<h2>Solutions</h2>

<div class="solution"><strong>Solution:</strong> For neural network output $\hat{\mathbf{p}} = [0.15, 0.60, 0.20, 0.05]$:

<p><strong>(1) Entropy:</strong>
<div class="equation">
$$\begin{align}
H(\hat{\mathbf{p}}) &= -\sum_{i=1}^4 \hat{p}_i \log_2 \hat{p}_i \\
&= -(0.15 \log_2 0.15 + 0.60 \log_2 0.60 + 0.20 \log_2 0.20 + 0.05 \log_2 0.05) \\
&= -(0.15(-2.737) + 0.60(-0.737) + 0.20(-2.322) + 0.05(-4.322)) \\
&= -(-0.411 - 0.442 - 0.464 - 0.216) \\
&= 1.533 \text{ bits}
\end{align}$$
</div>

<p><strong>(2) Cross-entropy loss for true label class 2:</strong>
<div class="equation">
$$
L = -\log \hat{p}_2 = -\log 0.60 \approx 0.511 \text{ nats} \quad \text{or} \quad -\log_2 0.60 \approx 0.737 \text{ bits}
$$
</div>

<p><strong>(3) Optimal output distribution:</strong>
The optimal distribution assigns probability 1 to the correct class:
<div class="equation">
$$
\mathbf{p}^* = [0, 1, 0, 0]
$$
</div>

<p>This gives entropy $H(\mathbf{p}^*) = 0$ (no uncertainty) and cross-entropy loss $L = -\log 1 = 0$ (perfect prediction).
</div>

<div class="solution"><strong>Solution:</strong> <strong>Proof that $H(p, q) = H(p) + D_{\text{KL}}(p \| q)$:</strong>

<p>Starting with the definition of cross-entropy:
<div class="equation">
$$\begin{align}
H(p, q) &= -\sum_x p(x) \log q(x) \\
&= -\sum_x p(x) \log q(x) + \sum_x p(x) \log p(x) - \sum_x p(x) \log p(x) \\
&= -\sum_x p(x) \log p(x) + \sum_x p(x) \log \frac{p(x)}{q(x)} \\
&= H(p) + D_{\text{KL}}(p \| q)
\end{align}$$
</div>

<p>Since $H(p)$ is constant with respect to $q$, minimizing $H(p, q)$ is equivalent to minimizing $D_{\text{KL}}(p \| q)$. This shows that training with cross-entropy loss is equivalent to minimizing the KL divergence between the true distribution and the predicted distribution.
</div>

<div class="solution"><strong>Solution:</strong> For binary classifier with $\hat{p} = 0.8$ and true label class 1:

<p><strong>(1) Binary cross-entropy loss:</strong>
<div class="equation">
$$
L = -[y \log \hat{p} + (1-y) \log(1-\hat{p})] = -[1 \cdot \log 0.8 + 0 \cdot \log 0.2] = -\log 0.8 \approx 0.223
$$
</div>

<p><strong>(2) Gradient:</strong>
<div class="equation">
$$\begin{align}
\frac{\partial L}{\partial \hat{p}} &= \frac{\partial}{\partial \hat{p}}[-y \log \hat{p} - (1-y) \log(1-\hat{p})] \\
&= -\frac{y}{\hat{p}} + \frac{1-y}{1-\hat{p}} \\
&= -\frac{1}{0.8} + \frac{0}{0.2} = -1.25
\end{align}$$
</div>

<p><strong>(3) Loss comparison:</strong>
<ul>
    <li>$\hat{p} = 0.99$: $L = -\log 0.99 \approx 0.010$ (very confident, correct)
    <li>$\hat{p} = 0.2$: $L = -\log 0.2 \approx 1.609$ (low confidence, incorrect)
</ul>

<p>The loss heavily penalizes confident wrong predictions, encouraging the model to be calibrated.
</div>

<div class="solution"><strong>Solution:</strong> For GPT-2 with $V = 50{,}257$, $B = 16$, $n = 1024$:

<p><strong>Memory for logits:</strong>
<div class="equation">
$$
B \times n \times V \times 4 \text{ bytes} = 16 \times 1024 \times 50{,}257 \times 4 = 3{,}280{,}838{,}144 \text{ bytes} \approx 3.06 \text{ GB}
$$
</div>

<p><strong>Memory with FP16:</strong>
<div class="equation">
$$
16 \times 1024 \times 50{,}257 \times 2 = 1{,}640{,}419{,}072 \text{ bytes} \approx 1.53 \text{ GB}
$$
</div>

<p>Savings: $3.06 - 1.53 = 1.53$ GB (50\% reduction)</p>

<p><strong>Maximum batch size with 25\% memory budget:</strong>
Available memory: $0.25 \times 40{,}000 \text{ MB} = 10{,}000$ MB</p>

<p>For FP16 logits:
<div class="equation">
$$
B = \frac{10{,}000 \text{ MB}}{n \times V \times 2 \text{ bytes}} = \frac{10{,}000 \times 10^6}{1024 \times 50{,}257 \times 2} \approx 97 \text{ sequences}
$$
</div>

<p>With FP32, maximum batch size would be only 48 sequences.
</div>

<div class="solution"><strong>Solution:</strong> For sampled softmax with $K = 5{,}000$ and $V = 100{,}000$:

<p><strong>(1) Speedup factor:</strong>
<ul>
    <li>Full softmax: $O(V) = 100{,}000$ operations per token
    <li>Sampled softmax: $O(K+1) = 5{,}001$ operations per token
    <li>Speedup: $\frac{100{,}000}{5{,}001} \approx 20\times$
</ul>

<p><strong>(2) Memory reduction:</strong>
For batch of 32 sequences with 512 tokens:
<ul>
    <li>Full softmax logits: $32 \times 512 \times 100{,}000 \times 4 = 6{,}553{,}600{,}000$ bytes $\approx 6.1$ GB
    <li>Sampled softmax logits: $32 \times 512 \times 5{,}001 \times 4 = 327{,}745{,}536$ bytes $\approx 312$ MB
    <li>Reduction: $\frac{6.1 \text{ GB}}{312 \text{ MB}} \approx 20\times$
</ul>

<p><strong>(3) Why sampled softmax introduces bias:</strong>
<ul>
    <li>The gradient estimate is unbiased only if we sample from the true distribution
    <li>In practice, we sample from a proposal distribution (e.g., unigram frequency)
    <li>This creates importance sampling bias in the gradient
    <li>The normalization constant is approximated, not exact
    <li>Bias decreases as $K$ increases, but never reaches zero
    <li>For large $K$ (e.g., 10,000), bias is negligible for most applications
</ul>
</div>

<div class="solution"><strong>Solution:</strong> For A100 GPU with 1.5 TB/s bandwidth and 312 TFLOPS FP16 throughput, softmax over $V = 30{,}000$:

<p><strong>(1) Memory transfer time:</strong>
<div class="equation">
$$
\text{Time} = \frac{400 \text{ KB}}{1{,}500{,}000{,}000 \text{ KB/s}} = \frac{400}{1{,}500{,}000{,}000} \approx 0.267 \text{ microseconds}
$$
</div>

<p><strong>(2) Compute time:</strong>
For 30,000 exponentials and 30,000 divisions:
<div class="equation">
$$
\text{Time} = \frac{60{,}000 \text{ ops}}{312 \times 10^{12} \text{ ops/s}} \approx 0.000192 \text{ microseconds}
$$
</div>

<p><strong>(3) Bottleneck analysis:</strong>
<ul>
    <li>Memory time: 0.267 microseconds
    <li>Compute time: 0.000192 microseconds
    <li>The operation is <strong>memory-bound</strong> by a factor of $\frac{0.267}{0.000192} \approx 1{,}390\times$
</ul>

<p>This extreme memory-bandwidth bottleneck explains why vocabulary size has such a direct impact on training speed, and why reducing precision from FP32 to FP16 provides nearly 2√ó speedup for softmax operations.
</div>

<div class="solution"><strong>Solution:</strong> For knowledge distillation with temperature $T$:

<p><strong>(1) Temperature effect on gradient magnitude:</strong>
The softmax with temperature is:
<div class="equation">
$$
p_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}
$$
</div>

<p>As $T$ increases, the distribution becomes more uniform (softer). The gradient magnitude scales as $O(1/T)$ because:
<div class="equation">
$$
\frac{\partial p_i}{\partial z_j} = \frac{1}{T} p_i(\delta_{ij} - p_j)
$$
</div>

<p><strong>(2) Gradient derivation:</strong>
For KL divergence $D_{\text{KL}}(p_{\text{teacher}} \| p_{\text{student}})$ where both use temperature $T$:
<div class="equation">
$$\begin{align}
\frac{\partial D_{\text{KL}}}{\partial z'_i} &= \frac{\partial}{\partial z'_i} \sum_j p_j^T \log \frac{p_j^T}{q_j^T} \\
&= -\sum_j p_j^T \frac{\partial \log q_j^T}{\partial z'_i} \\
&= -\sum_j p_j^T \frac{1}{q_j^T} \frac{\partial q_j^T}{\partial z'_i} \\
&= -\sum_j p_j^T \frac{1}{q_j^T} \cdot \frac{1}{T} q_j^T(\delta_{ij} - q_i^T) \\
&= -\frac{1}{T} \sum_j p_j^T(\delta_{ij} - q_i^T) \\
&= \frac{1}{T}(q_i^T - p_i^T)
\end{align}$$
</div>

<p><strong>(3) Why $T^2$ scaling is necessary:</strong>
Without $T^2$ scaling, the gradient is $O(1/T)$, which vanishes as $T \to \infty$:
<div class="equation">
$$
\lim_{T \to \infty} \frac{1}{T}(q_i^T - p_i^T) = 0
$$
</div>

<p>With $T^2$ scaling, the effective gradient becomes:
<div class="equation">
$$
T^2 \cdot \frac{1}{T}(q_i^T - p_i^T) = T(q_i^T - p_i^T)
$$
</div>

<p>This compensates for the $1/T$ factor from the softmax derivative, maintaining meaningful gradients even for large $T$. The $T^2$ factor ensures that the distillation loss has the same scale as the hard label loss, allowing proper balancing between the two objectives.
</div>
        
        <div class="chapter-nav">
  <a href="chapter02_calculus_optimization.html">‚Üê Chapter 2: Calculus and Optimization</a>
  <a href="../../deeptech.html">üìö Table of Contents</a>
  <a href="chapter04_feedforward_networks.html">Chapter 4: Feed-Forward Neural Networks ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
