<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 18: Multimodal Transformers - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../../deeptech.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Multimodal Transformers</h1>

<h2>Chapter Overview</h2>

<p>Multimodal transformers process multiple modalities (text, images, audio, video) in a unified framework. This chapter covers vision-language models (CLIP, DALL-E), audio-text models (Whisper), and unified architectures that handle arbitrary combinations of modalities.</p>

<h3>Learning Objectives</h3>

<ol>
    <li>Understand multimodal fusion strategies
    <li>Implement contrastive learning (CLIP)
    <li>Apply vision-language models to zero-shot classification
    <li>Generate images from text (DALL-E, Stable Diffusion)
    <li>Process audio with transformers (Whisper)
    <li>Build unified multimodal models
</ol>

<h2>Multimodal Learning Fundamentals</h2>

<h3>Fusion Strategies</h3>

<p>The choice of fusion strategy determines how modalities interact, with direct implications for computational cost and expressiveness. Three primary approaches have emerged:</p>

<figure>
<div class="architecture-diagram">
<h3>Multimodal Transformer Fusion</h3>
<pre class="mermaid">
graph LR
    TXT["Text tokens\n in N^n_t"] -->|"W_text in R^V x d"| TE["Text Emb\n in R^n_t x d"]
    IMG["Image patches\n in R^N x (P^2*C)"] -->|"W_img in R^(P^2*C) x d"| IE["Image Emb\n in R^N x d"]

<p>TE --> FUSE["Concatenate\n [text; image]\n in R^(n_t+N) x d"]
    IE --> FUSE
    FUSE --> ENC["Joint Transformer\n x L layers\n Cross-modal attention\n STORED per layer:\n all activations\n Memory: O(L*(n_t+N)^2)"]
    ENC --> TOUT["Text output\n in R^n_t x d"]
    ENC --> IOUT["Image output\n in R^N x d"]</p>

<p>style TXT fill:#e8f5e9,stroke:#4caf50,color:#000
    style IMG fill:#e3f2fd,stroke:#2196f3,color:#000
    style FUSE fill:#fff3e0,stroke:#ff9800,color:#000
    style ENC fill:#f3e5f5,stroke:#9c27b0,color:#000
</pre>
<p class="diagram-caption">Multimodal Transformer Fusion</p>
</div>

<div class="tikz-diagram"><img src="../diagrams/chapter18_multimodal_transformers_8d2834139e81.svg" alt="TikZ Diagram" /></div>
<figcaption>Three multimodal fusion strategies. <strong>Early fusion</strong> (left): concatenates modalities and processes with unified encoder, enabling rich interactions but with quadratic cost $O((N+M)^2)$. <strong>Late fusion</strong> (center): separate encoders with fusion only at output, efficient $O(N^2 + M^2)$ but limited cross-modal interaction. <strong>Cross-modal attention</strong> (right): separate encoders with explicit cross-attention, balancing efficiency $O(N^2 + M^2 + NM)$ with rich interactions.</figcaption>
</figure>

<div style="text-align: center;">

<table>
<tr><th><strong>Strategy</strong></th><th><strong>Description</strong></th><th><strong>Pros</strong></th><th><strong>Cons</strong></th></tr>
<tr><td>Early fusion</td><td>Concatenate modality tokens into one sequence; process with unified encoder</td><td>Rich cross-modal interaction at every layer; simple architecture</td><td>$O((N{+}M)^2 d)$ cost; adding patches dramatically increases compute</td></tr>
<tr><td>Late fusion</td><td>Separate encoders per modality; combine outputs at decision stage (CLIP)</td><td>Efficient $O(N^2 d {+} M^2 d)$; encoders parallelizable</td><td>No fine-grained cross-modal alignment; interaction only at output</td></tr>
<tr><td>Cross-modal attention</td><td>Separate encoders with cross-attention layers between modalities (BLIP, Flamingo)</td><td>$O(N^2 d {+} M^2 d {+} NMd)$; rich interactions with moderate cost</td><td>Additional parameters; more complex architecture</td></tr>
</table>

<p></div>

<p>Cross-modal attention offers the best trade-off for most applications: for 196 image patches and 128 text tokens, cross-attention requires $196 \times 128 = 25{,}088$ computations per head versus $324^2 = 104{,}976$ for early fusion---a 4$\times$ reduction while preserving fine-grained alignment between modalities.</p>

<h3>Alignment Objectives</h3>

<p><strong>Contrastive Learning:</strong>
<div class="equation">
$$
\mathcal{L}_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(v_i, t_i)/\tau)}{\sum_j \exp(\text{sim}(v_i, t_j)/\tau)}
$$
</div>
where $v_i$ = image embedding, $t_i$ = text embedding, $\tau$ = temperature</p>

<p><strong>Matching Loss:</strong>
<div class="equation">
$$
\mathcal{L}_{\text{match}} = -\mathbb{E}[\log P(\text{match}|v, t)]
$$
</div>

<p><strong>Reconstruction:</strong>
<div class="equation">
$$
\mathcal{L}_{\text{recon}} = \|f_{\text{dec}}(v) - t\|^2
$$
</div>

<h2>CLIP: Contrastive Language-Image Pre-training</h2>

<h3>CLIP Architecture</h3>

<p>CLIP (Contrastive Language-Image Pre-training) represents a breakthrough in vision-language learning by training image and text encoders jointly using a contrastive objective on 400 million image-text pairs collected from the internet. Unlike traditional supervised learning that requires manually labeled categories, CLIP learns to align images with their natural language descriptions, enabling zero-shot transfer to downstream tasks without any task-specific training data.</p>

<div class="definition"><strong>Definition:</strong> 
The CLIP architecture consists of three main components that work together to create a shared embedding space for images and text. The <strong>image encoder</strong> can be either a Vision Transformer (ViT) or a ResNet, which processes an input image and produces a fixed-dimensional embedding $\vv \in \R^{d}$. For the largest CLIP model (ViT-L/14), the image encoder is a ViT with patch size 14, hidden dimension 1024, 24 layers, and 16 attention heads, totaling approximately 304 million parameters. The <strong>text encoder</strong> is a transformer decoder (similar to GPT) with a context length of 77 tokens, hidden dimension 768, 12 layers, and 12 attention heads, containing roughly 63 million parameters. Both encoders are followed by learned linear <strong>projection layers</strong> that map their outputs to a shared embedding space of dimension $d = 512$, where cosine similarity can be computed directly.

<p>The training procedure processes batches of $(image, text)$ pairs simultaneously. For each batch of size $N$, all $N$ images are encoded to produce embeddings $\vv_1, \ldots, \vv_N \in \R^{512}$, and all $N$ text descriptions are encoded to produce $\vt_1, \ldots, \vt_N \in \R^{512}$. The model then computes an $N \times N$ similarity matrix where entry $(i,j)$ represents the cosine similarity between image $i$ and text $j$. The contrastive loss maximizes the similarity along the diagonal (correct image-text pairs) while minimizing off-diagonal similarities (incorrect pairings). This symmetric loss is computed in both directions‚Äîpredicting text from image and image from text‚Äîand averaged.
</div>

<p>The parameter count for CLIP varies significantly across model scales. CLIP ResNet-50 contains approximately 102 million parameters (38M for ResNet-50 image encoder, 63M for text encoder, 1M for projections), while CLIP ViT-L/14 totals around 428 million parameters (304M for ViT-L image encoder, 123M for a larger text encoder with 768 dimensions and 12 layers, 1M for projections). The largest variant, ViT-L/14@336px, processes higher-resolution images (336√ó336 instead of 224√ó224) with the same architecture, increasing computational cost but improving performance on fine-grained tasks.</p>

<div class="example"><strong>Example:</strong> 
Consider a training batch with size $N = 4$ and embedding dimension $d = 512$. The image encoder processes four images to produce embeddings arranged as rows in matrix $\mV = [\vv_1, \vv_2, \vv_3, \vv_4]\transpose \in \R^{4 \times 512}$, while the text encoder processes their corresponding captions to produce $\mT = [\vt_1, \vt_2, \vt_3, \vt_4]\transpose \in \R^{4 \times 512}$.

<p>The similarity matrix is computed as $\mS = \mV \mT\transpose \in \R^{4 \times 4}$, where each entry $S_{ij}$ represents the dot product between image embedding $i$ and text embedding $j$. To make this scale-invariant, CLIP uses cosine similarity: $S_{ij} = \frac{\vv_i \cdot \vt_j}{\|\vv_i\| \|\vt_j\|}$, which normalizes each embedding to unit length before computing the dot product. This ensures that the similarity is determined by the angle between embeddings rather than their magnitudes.</p>

<p>The contrastive loss for the image-to-text direction is computed as:
<div class="equation">
$$
\mathcal{L}_i^{\text{img}\to\text{txt}} = -\log \frac{\exp(S_{ii}/\tau)}{\sum_{j=1}^{N} \exp(S_{ij}/\tau)}
$$
</div>
where $\tau$ is a learned temperature parameter, initialized to $0.07$ and trained jointly with the model. The temperature controls the sharpness of the distribution: smaller values make the model more confident (sharper peaks), while larger values produce softer distributions. The symmetric text-to-image loss $\mathcal{L}_i^{\text{txt}\to\text{img}}$ is computed analogously by treating text as queries and images as candidates. The total loss averages both directions:
<div class="equation">
$$
\mathcal{L} = \frac{1}{2N} \sum_{i=1}^{N} (\mathcal{L}_i^{\text{img}\to\text{txt}} + \mathcal{L}_i^{\text{txt}\to\text{img}})
$$
</div>

<p>In practice, CLIP uses very large batch sizes to provide more negative examples for contrastive learning. The original CLIP was trained with batch size 32,768, requiring distributed training across multiple GPUs. With such large batches, each positive pair has 32,767 negative examples, providing a strong learning signal. However, this creates substantial memory requirements: storing the $32{,}768 \times 512$ embedding matrices for images and text requires $32{,}768 \times 512 \times 4 = 67$ MB per modality in FP32, and the $32{,}768 \times 32{,}768$ similarity matrix requires $4.3$ GB. To make this tractable, CLIP uses gradient checkpointing and distributes the batch across many GPUs, computing the similarity matrix in chunks.
</div>

<h3>Computational Analysis of CLIP Training</h3>

<p>Training CLIP at scale requires careful consideration of computational and memory costs across both the image and text encoding paths. For the ViT-L/14 image encoder processing 224√ó224 images, each image is divided into $16 \times 16 = 256$ patches of size $14 \times 14$. These patches are linearly projected to dimension 1024 and processed through 24 transformer layers. The computational cost per image is approximately $2 \times 24 \times 256^2 \times 1024 = 3.2$ GFLOPS for the attention operations (using the $2Ld^2n^2$ formula from Chapter 12) plus $2 \times 24 \times 256 \times 4 \times 1024^2 = 51.5$ GFLOPS for the feed-forward networks, totaling roughly 55 GFLOPS per image.</p>

<p>The text encoder processes sequences of up to 77 tokens through 12 transformer layers with dimension 768. The computational cost per text is approximately $2 \times 12 \times 77^2 \times 768 = 1.1$ GFLOPS for attention plus $2 \times 12 \times 77 \times 4 \times 768^2 = 4.4$ GFLOPS for feed-forward networks, totaling about 5.5 GFLOPS per text. This asymmetry‚Äîimages requiring 10√ó more compute than text‚Äîmeans that image encoding dominates the computational budget during training.</p>

<p>For a batch of 32,768 examples, the total forward pass requires approximately $32{,}768 \times (55 + 5.5) = 1{,}982{,}464$ GFLOPS or roughly 2 PFLOPS. On an NVIDIA A100 GPU with 312 TFLOPS of FP16 compute, this would take approximately 6.4 seconds per batch for the forward pass alone, not including backward propagation (which typically costs 2√ó the forward pass) or the contrastive loss computation. The full training of CLIP on 400 million image-text pairs with batch size 32,768 requires approximately $400{,}000{,}000 / 32{,}768 = 12{,}207$ batches. At roughly 20 seconds per batch (forward + backward + optimizer step), this amounts to 68 hours of continuous training on a single A100. In practice, OpenAI trained CLIP on 256 V100 GPUs for approximately 12 days, suggesting a total training cost of around 73,728 GPU-hours.</p>

<p>Memory requirements are equally demanding. Each image in the batch requires storing activations for 24 layers with 256 tokens and dimension 1024, totaling approximately $24 \times 256 \times 1024 \times 2 = 12.6$ MB per image in FP16 (the factor of 2 accounts for storing both pre- and post-activation values for backpropagation). For batch size 32,768, this amounts to 413 GB just for image activations. Text activations are smaller at approximately $12 \times 77 \times 768 \times 2 = 1.4$ MB per text, or 46 GB for the full batch. The similarity matrix requires $32{,}768 \times 32{,}768 \times 2 = 2.1$ GB in FP16. Combined with model parameters (428M parameters √ó 2 bytes = 856 MB) and optimizer states (typically 2√ó parameters for Adam), the total memory footprint exceeds 500 GB, necessitating distribution across many GPUs using techniques like ZeRO (Chapter 22) to partition optimizer states and activations.</p>

<h3>Zero-Shot Classification with CLIP</h3>

<p>One of CLIP's most remarkable capabilities is zero-shot classification: the ability to classify images into categories the model has never been explicitly trained on. This works by leveraging the natural language understanding of the text encoder to create classifiers on the fly from text descriptions. The procedure begins by creating text prompts for each class in the target classification task. For example, for a 10-class animal classification task, we might create prompts like "a photo of a dog", "a photo of a cat", "a photo of a bird", and so on. These prompts are encoded by the text encoder to produce class embeddings $\vt_1, \ldots, \vt_C \in \R^{512}$ where $C$ is the number of classes.</p>

<p>To classify a new image, we encode it with the image encoder to produce $\vv \in \R^{512}$, then compute the cosine similarity between the image embedding and each class embedding: $s_i = \frac{\vv \cdot \vt_i}{\|\vv\| \|\vt_i\|}$. The predicted class is simply $\arg\max_i s_i$, the class whose text description has the highest similarity to the image. This approach requires no training on the target dataset‚Äîthe model uses only its pre-trained knowledge of how images and text relate.</p>

<p>The performance of this zero-shot approach is surprisingly strong. CLIP ViT-L/14 achieves 76.2\% top-1 accuracy on ImageNet without ever seeing a single ImageNet training example, matching the performance of a ResNet-50 trained directly on ImageNet's 1.28 million labeled images. This demonstrates that CLIP has learned visual concepts that generalize far beyond its training distribution. Moreover, CLIP shows remarkable robustness to distribution shift: when evaluated on ImageNet variants with different image styles (sketches, cartoons, adversarial examples), CLIP's performance degrades much less than supervised models, suggesting it has learned more robust visual representations.</p>

<p>The prompt engineering aspect of zero-shot classification is crucial for performance. Simple prompts like "dog" perform worse than more descriptive prompts like "a photo of a dog". OpenAI found that using prompt ensembles‚Äîaveraging predictions across multiple prompt templates like "a photo of a \{class\}", "a picture of a \{class\}", "an image of a \{class\}"‚Äîimproves accuracy by 1-2\% by reducing sensitivity to prompt phrasing. For fine-grained classification tasks, more specific prompts help: "a photo of a \{species\}, a type of bird" outperforms "a photo of a \{species\}" for bird species classification.</p>

<h3>CLIP Variants and Training Requirements</h3>

<p>Following CLIP's success, several variants have been developed with different scales and training procedures. <strong>OpenCLIP</strong> is an open-source reproduction that has trained models ranging from small (ResNet-50 with 102M parameters) to very large (ViT-G/14 with 1.8B parameters) on datasets including LAION-400M and LAION-2B. The largest OpenCLIP models require training on clusters of 128-512 A100 GPUs for several weeks, with estimated costs exceeding \$100,000 for the full training run. The training uses mixed precision (FP16) to reduce memory consumption and enable larger batch sizes, typically 32,768 to 65,536 examples distributed across all GPUs.</p>

<p><strong>ALIGN</strong>, developed by Google, scales up the training data to 1.8 billion noisy image-text pairs collected from the web without extensive filtering. This demonstrates that contrastive learning is robust to noise in the training data‚Äîthe model learns to ignore mismatched pairs through the contrastive objective. ALIGN uses an EfficientNet-L2 image encoder (480M parameters) and a BERT-Large text encoder (340M parameters), totaling approximately 820M parameters. Training ALIGN required a cluster of 1024 Cloud TPU v3 cores for approximately 6 days, representing roughly 150,000 TPU-hours.</p>

<p><strong>Florence</strong>, Microsoft's unified vision foundation model, extends the CLIP approach to 900 million image-text pairs with a focus on creating a single model that can be adapted to diverse vision tasks. Florence uses a CoSwin transformer as the image encoder (637M parameters) and achieves state-of-the-art results on zero-shot classification, retrieval, and object detection after fine-tuning. The training infrastructure required 512 NVIDIA A100 GPUs for approximately 10 days, with an estimated cost of over \$200,000 in cloud compute.</p>

<p>The hardware requirements for training CLIP-scale models are substantial. A minimum viable setup might use 8-16 A100 GPUs (80GB each) to train a CLIP ResNet-50 model on a smaller dataset like Conceptual Captions (3M pairs) with batch size 2048-4096, requiring approximately 1-2 weeks. Scaling to the full CLIP ViT-L/14 with 400M training pairs and batch size 32,768 necessitates at least 64-128 A100 GPUs with high-bandwidth interconnects (NVLink or InfiniBand) to efficiently synchronize gradients across the distributed batch. The total training cost for reproducing CLIP ViT-L/14 is estimated at \$50,000-\$100,000 in cloud GPU costs, depending on the provider and optimization techniques employed.</p>

<h2>DALL-E and Stable Diffusion</h2>

<h3>DALL-E: Text-to-Image Generation</h3>

<div class="definition"><strong>Definition:</strong> 
<strong>DALL-E 1 (2021):</strong>
<ul>
    <li>Encoder: Compress images to discrete tokens (VQ-VAE)
    <li>Transformer: Autoregressive model over text + image tokens
    <li>Training: Next token prediction
</ul>

<p><strong>Sequence:</strong>
<div class="equation">
$$
[\text{BOS}, \text{text tokens}, \text{image tokens}, \text{EOS}]
$$
</div>

<p>Generate image by: (1) Encode text, (2) Sample image tokens autoregressively
</div>

<p><strong>DALL-E 2 (2022):</strong>
<ul>
    <li>Use CLIP embeddings
    <li>Prior: Text embedding $\to$ Image embedding
    <li>Decoder: Image embedding $\to$ Image (diffusion model)
    <li>Much higher quality than DALL-E 1
</ul>

<h3>Stable Diffusion</h3>

<p><strong>Latent Diffusion Model:</strong>
<ol>
    <li>Encode image to latent space (VAE)
    <li>Add noise iteratively (forward diffusion)
    <li>Learn to denoise (reverse diffusion)
    <li>Condition on text via cross-attention
</ol>

<p><strong>Text conditioning:</strong>
<ul>
    <li>Text encoder: CLIP or T5
    <li>Cross-attention: Latent queries attend to text keys/values
    <li>Enables text-guided image generation
</ul>

<div class="example"><strong>Example:</strong> 
<strong>Components:</strong>

<p><strong>1. Text Encoder:</strong> CLIP text encoder
<div class="equation">
$$
\text{prompt} \to \vt \in \R^{77 \times 768}
$$
</div>

<p><strong>2. VAE Encoder:</strong> Image $\to$ latent
<div class="equation">
$$
\mI \in \R^{512 \times 512 \times 3} \to \vz \in \R^{64 \times 64 \times 4}
$$
</div>

<p><strong>3. U-Net Denoiser:</strong> Diffusion model with cross-attention
<ul>
    <li>Input: Noisy latent $\vz_t$
    <li>Condition: Text embedding $\vt$
    <li>Output: Predicted noise $\epsilon_\theta(\vz_t, t, \vt)$
</ul>

<p><strong>4. VAE Decoder:</strong> Latent $\to$ image
<div class="equation">
$$
\vz \in \R^{64 \times 64 \times 4} \to \mI \in \R^{512 \times 512 \times 3}
$$
</div>

<p><strong>Parameters:</strong> $\approx 860$M total
</div>

<h2>Vision-Language Understanding</h2>

<h3>BLIP: Bootstrapped Language-Image Pre-training</h3>

<p><strong>Architecture:</strong>
<ul>
    <li>Image encoder (ViT)
    <li>Text encoder (BERT)
    <li>Multimodal encoder (cross-attention between vision and text)
</ul>

<p><strong>Training objectives:</strong>
<ol>
    <li><strong>ITC:</strong> Image-Text Contrastive (like CLIP)
    <li><strong>ITM:</strong> Image-Text Matching (binary: match or not)
    <li><strong>LM:</strong> Language Modeling on text
</ol>

<p><strong>Bootstrapping:</strong> Generate synthetic captions, filter with model, retrain</p>

<h3>Flamingo: Few-Shot Learning</h3>

<p>Flamingo represents a significant architectural innovation in multimodal transformers by enabling models to process arbitrarily interleaved sequences of images and text, supporting few-shot learning through in-context examples. Unlike CLIP, which processes single image-text pairs, Flamingo can handle inputs like "Here is an image of a cat: <code><image1></code>. Here is an image of a dog: <code><image2></code>. What animal is in this image: <code><image3></code>?" This capability enables few-shot visual learning where the model learns new tasks from just a few examples provided in the prompt, without any parameter updates.</p>

<p>The Flamingo architecture consists of three main components, carefully designed to leverage pre-trained models while adding minimal trainable parameters. The <strong>vision encoder</strong> is a frozen CLIP ViT-L/14 model that processes each image independently to produce a sequence of patch embeddings. For a 224√ó224 image with patch size 14, this yields 256 patch tokens of dimension 1024. The vision encoder's 304M parameters remain frozen throughout training, preserving the strong visual representations learned during CLIP pre-training.</p>

<p>The <strong>language model</strong> is a frozen Chinchilla 70B model, a large autoregressive transformer trained on text-only data. Chinchilla uses 70 billion parameters across 80 layers with hidden dimension 8192 and 64 attention heads. Keeping this massive language model frozen is crucial for computational tractability‚Äîtraining 70B parameters would require prohibitive memory and compute. Instead, Flamingo inserts new trainable layers that allow the frozen language model to attend to visual information without modifying its core text processing capabilities.</p>

<p>The key innovation is the <strong>Perceiver Resampler</strong>, a learned module that compresses the variable-length sequence of image patch embeddings into a fixed number of visual tokens that can be efficiently processed by the language model. The Perceiver Resampler uses cross-attention where a fixed set of learned queries $\mQ \in \R^{64 \times 2048}$ (64 visual tokens, dimension 2048) attends to the image patch embeddings $\mK, \mV \in \R^{256 \times 1024}$ from the vision encoder. This produces a fixed-size representation regardless of input image resolution or the number of images in the sequence. The Perceiver Resampler contains approximately 1.4B parameters (6 layers of cross-attention and feed-forward networks with dimension 2048), making it the primary trainable component of Flamingo.</p>

<p>Between every few layers of the frozen language model, Flamingo inserts new <strong>cross-attention layers</strong> that allow text tokens to attend to the visual tokens produced by the Perceiver Resampler. Specifically, for Flamingo-80B (built on Chinchilla-70B), cross-attention layers are inserted after every 7th transformer layer, resulting in approximately 11 cross-attention insertions across the 80 layers. Each cross-attention layer adds roughly 134M parameters (for dimension 8192), totaling about 1.5B parameters for all insertions. Combined with the Perceiver Resampler, Flamingo adds approximately 2.9B trainable parameters to the 70B frozen base model, representing just 4\% additional parameters while enabling full multimodal capabilities.</p>

<p>The memory requirements for Flamingo are dominated by the frozen language model. Storing 70B parameters in FP16 requires 140 GB, which exceeds the memory of any single GPU. Flamingo uses model parallelism to partition the language model across multiple GPUs‚Äîfor example, distributing across 8 A100 GPUs (80GB each) places roughly 8.75B parameters per GPU, consuming about 17.5 GB for parameters. Activations for a sequence of 2048 tokens (including both text and visual tokens) across 80 layers with dimension 8192 require approximately $2048 \times 80 \times 8192 \times 2 = 2.6$ GB per example in FP16. With batch size 8, activations consume 21 GB per GPU, leaving sufficient memory for gradients of the trainable parameters (2.9B parameters √ó 2 bytes √ó 2 for gradients = 11.6 GB) and optimizer states (23.2 GB for Adam).</p>

<p>Training Flamingo on a mixture of image-text pairs, interleaved image-text documents, and video-text pairs requires substantial computational resources. The training dataset consists of 2.3 billion image-text pairs (similar to CLIP), 43 million interleaved image-text web pages, and 27 million video clips. Training Flamingo-80B for 1 epoch through this data with batch size 256 distributed across 256 A100 GPUs takes approximately 15 days, representing roughly 92,000 GPU-hours. The estimated training cost exceeds \$300,000 in cloud compute. However, the key advantage is that only 2.9B parameters are trained while leveraging the capabilities of a 70B language model, making training far more efficient than training a 70B multimodal model from scratch.</p>

<p>For inference, Flamingo's few-shot learning capability means that users can provide 2-32 example image-text pairs in the prompt to demonstrate a new task, and the model adapts its predictions based on these examples without any fine-tuning. This in-context learning works because the cross-attention mechanism allows the model to attend to the example images when processing the query image. The computational cost of inference scales linearly with the number of examples in the context: each additional image adds 256 patch tokens (after vision encoding) compressed to 64 visual tokens (after Perceiver Resampler), increasing the sequence length and thus the attention cost. For a prompt with 4 example images and 1 query image (5 images total), the visual tokens contribute $5 \times 64 = 320$ tokens to the sequence, which combined with text tokens (typically 500-1000) results in sequences of 800-1300 tokens. On a single A100 GPU, Flamingo-80B can process approximately 2-3 such sequences per second, limited primarily by the memory bandwidth required to load the 70B parameter model.</p>

<h2>Computational Analysis of Multimodal Transformers</h2>

<p>Multimodal transformers follow the same FLOPs formulas derived in Chapter~12 for their individual encoders: each transformer layer costs $24Bnd_{\text{model}}^2 + 4Bn^2d_{\text{model}}$ FLOPs (attention plus feed-forward). The multimodal-specific addition is cross-modal attention, which costs $4mnd$ FLOPs per layer (where $m$ and $n$ are the sequence lengths of the two modalities). In practice, cross-modal attention is a small fraction of total cost---for BLIP with 128 text tokens and 196 image patches, cross-attention adds only 462~MFLOPs across 6 layers, negligible compared to the self-attention costs in each encoder.</p>

<p>The key computational asymmetry in multimodal models is between modalities: image encoding typically dominates. CLIP's ViT-L/14 requires $\sim$55~GFLOPs per image versus $\sim$5.5~GFLOPs per text, a 10$\times$ ratio. When a large language model serves as the text backbone (as in Flamingo with Chinchilla-70B), text processing dominates instead, requiring $\sim$110~TFLOPs per sequence.</p>

<p>Memory requirements follow the same principles as unimodal transformers (Chapter~12): parameters, gradients, optimizer states, and activations. The multimodal-specific concern is storing activations for both modalities simultaneously. For CLIP ViT-L/14, image activations consume $\sim$75~MB per image in FP16 while text activations require $\sim$1.4~MB per text. For large batch sizes (32,768 in CLIP), this necessitates distributed training with gradient checkpointing and mixed precision (see Chapter~11 for distributed training techniques).</p>

<h2>Training Challenges for Multimodal Transformers</h2>

<h3>Batch Size Requirements for Contrastive Learning</h3>

<p>Contrastive learning methods like CLIP require very large batch sizes to provide sufficient negative examples. CLIP's performance scales log-linearly with batch size: increasing from 256 to 32,768 improves ImageNet zero-shot accuracy from $\sim$58\% to 76\%. However, the $32{,}768 \times 32{,}768$ similarity matrix alone requires 4.3~GB in FP32. To make this tractable, CLIP distributes the batch across 256 GPUs using all-gather communication, so the full similarity matrix is never materialized on any single GPU.</p>

<h3>Distributed Training and Memory Optimization</h3>

<p>Multimodal transformers use the same distributed training techniques as unimodal models (see Chapter~11 for detailed coverage): data parallelism for CLIP-scale models that fit on a single GPU, tensor and pipeline parallelism for larger models like Flamingo-80B where the 70B parameter language model must be partitioned across multiple GPUs. Memory optimization techniques---gradient checkpointing, mixed precision training, and ZeRO optimizer state partitioning---are essential and apply identically to the multimodal setting.</p>

<p>The multimodal-specific challenge is the asymmetric memory profile: image activations ($\sim$75~MB per image for ViT-L) far exceed text activations ($\sim$1.4~MB per text for CLIP's encoder), so image encoding dominates the memory budget during training. For Flamingo-80B, the frozen 70B language model requires 140~GB in FP16, necessitating model parallelism across at least 2 A100 GPUs before accounting for activations or trainable parameters.</p>

<h2>Audio Transformers</h2>

<h3>Whisper: Speech Recognition</h3>

<div class="definition"><strong>Definition:</strong> 
Encoder-decoder transformer for speech:

<p><strong>Input:</strong> Audio waveform $\to$ Log-mel spectrogram</p>

<p><strong>Encoder:</strong>
<ul>
    <li>Input: Spectrogram (80 mel bins)
    <li>Convolution layers (downsample)
    <li>Transformer encoder layers
</ul>

<p><strong>Decoder:</strong>
<ul>
    <li>Autoregressive text generation
    <li>Special tokens for language, task, timestamps
</ul>
</div>

<p><strong>Training data:</strong> 680,000 hours of multilingual audio</p>

<p><strong>Tasks supported:</strong>
<ul>
    <li>Speech recognition (transcription)
    <li>Translation (to English)
    <li>Language identification
    <li>Voice activity detection
    <li>Timestamp prediction
</ul>

<div class="example"><strong>Example:</strong> 
<strong>Special tokens:</strong>
<pre><code>
<|startoftranscript|><|en|><|transcribe|><|notimestamps|>
</code></pre>

<p><strong>Spectrogram:</strong>
<ul>
    <li>80 mel bins
    <li>3000 frames (30 seconds audio at 100 Hz)
    <li>Input: $3000 \times 80$
</ul>

<p><strong>Encoder:</strong>
<ul>
    <li>Conv layers: $3000 \times 80 \to 1500 \times 768$
    <li>Transformer: Process 1500 tokens
</ul>

<p><strong>Decoder:</strong> Generate text tokens autoregressively
</div>

<h3>Audio-Text Pre-training</h3>

<p><strong>Contrastive learning:</strong> Like CLIP but audio-text</p>

<p><strong>AudioCLIP:</strong> Tri-modal (image, text, audio)</p>

<p><strong>Applications:</strong>
<ul>
    <li>Zero-shot audio classification
    <li>Audio captioning
    <li>Text-to-audio generation
</ul>

<h2>Unified Multimodal Models</h2>

<h3>Perceiver and Perceiver IO</h3>

<p><strong>Key idea:</strong> Map arbitrary modalities to latent space via cross-attention</p>

<div class="definition"><strong>Definition:</strong> 
<strong>Components:</strong>

<p><strong>1. Latent array:</strong> Fixed set of learned queries $\mZ \in \R^{M \times d}$</p>

<p><strong>2. Cross-attention:</strong> Latents attend to inputs
<div class="equation">
$$
\mZ_1 = \text{CrossAttn}(\mQ=\mZ, \mK=\mX, \mV=\mX)
$$
</div>

<p><strong>3. Transformer:</strong> Process latents
<div class="equation">
$$
\mZ_L = \text{Transformer}(\mZ_1)
$$
</div>

<p><strong>4. Output:</strong> Decode latents to task outputs
</div>

<p><strong>Benefits:</strong>
<ul>
    <li>Handles arbitrary input sizes
    <li>Computation independent of input size (fixed latents)
    <li>Unified architecture for images, video, audio, text
</ul>

<h3>GPT-4V and LLaVA</h3>

<p><strong>GPT-4V (Vision):</strong> GPT-4 with vision capabilities
<ul>
    <li>Interleaved image and text inputs
    <li>Strong vision-language understanding
    <li>Details not fully disclosed
</ul>

<p><strong>LLaVA (Open-source):</strong>
<ul>
    <li>CLIP vision encoder
    <li>LLaMA language model
    <li>Linear projection to align embeddings
    <li>Instruction tuning on visual conversations
</ul>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> Implement CLIP contrastive loss for batch size 8:
<ol>
    <li>Generate random image embeddings $(8, 512)$
    <li>Generate random text embeddings $(8, 512)$
    <li>Compute $8 \times 8$ similarity matrix
    <li>Calculate contrastive loss with $\tau = 0.07$
</ol>
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Use CLIP for zero-shot classification on CIFAR-10:
<ol>
    <li>Load pre-trained CLIP model
    <li>Create text prompts for 10 classes
    <li>Encode images and prompts
    <li>Compute accuracy
    <li>Compare to supervised baseline
</ol>
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> Analyze Whisper architecture:
<ol>
    <li>Calculate parameters for encoder (24 layers, $d=1024$)
    <li>Calculate parameters for decoder (24 layers)
    <li>Estimate memory for 30-second audio
    <li>Compare to text-only GPT-2
</ol>
</div>

<div class="exercise" id="exercise-4"><strong>Exercise 4:</strong> Design multimodal fusion strategy for video understanding (visual + audio + captions):
<ol>
    <li>Propose architecture
    <li>Define fusion mechanism
    <li>Specify training objective
    <li>Estimate parameter count
</ol>
</div>

<h2>Solutions</h2>

<p>Full solutions for all exercises are available at \url{https://deeplearning.hofkensvermeulen.be}.</p>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 1: CLIP Contrastive Loss Implementation</strong>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

def clip_contrastive_loss(image_embeddings, text_embeddings, temperature=0.07):
    """
    Compute CLIP contrastive loss
    Args:
        image_embeddings: (B, D) normalized image embeddings
        text_embeddings: (B, D) normalized text embeddings
        temperature: temperature parameter tau
    Returns:
        loss: scalar contrastive loss
    """
    # Normalize embeddings
    image_embeddings = F.normalize(image_embeddings, dim=-1)
    text_embeddings = F.normalize(text_embeddings, dim=-1)
    
    # Compute similarity matrix (B, B)
    logits = torch.matmul(image_embeddings, text_embeddings.t()) / temperature
    
    # Labels: diagonal elements are positive pairs
    batch_size = image_embeddings.shape[0]
    labels = torch.arange(batch_size, device=image_embeddings.device)
    
    # Symmetric loss: image-to-text + text-to-image
    loss_i2t = F.cross_entropy(logits, labels)
    loss_t2i = F.cross_entropy(logits.t(), labels)
    
    loss = (loss_i2t + loss_t2i) / 2
    
    return loss, logits

# Part (a): Generate random embeddings
batch_size = 8
embed_dim = 512

image_embeddings = torch.randn(batch_size, embed_dim)
text_embeddings = torch.randn(batch_size, embed_dim)

print(f"Image embeddings shape: {image_embeddings.shape}")
print(f"Text embeddings shape: {text_embeddings.shape}")

# Part (b): Normalize embeddings
image_embeddings = F.normalize(image_embeddings, dim=-1)
text_embeddings = F.normalize(text_embeddings, dim=-1)

print(f"\nAfter normalization:")
print(f"Image embedding norms: {torch.norm(image_embeddings, dim=-1)}")
print(f"Text embedding norms: {torch.norm(text_embeddings, dim=-1)}")

# Part (c): Compute similarity matrix
temperature = 0.07
similarity_matrix = torch.matmul(image_embeddings, text_embeddings.t()) / temperature

print(f"\nSimilarity matrix shape: {similarity_matrix.shape}")
print(f"Similarity matrix:\n{similarity_matrix}")

# Part (d): Calculate contrastive loss
loss, logits = clip_contrastive_loss(image_embeddings, text_embeddings, temperature)

print(f"\nContrastive loss: {loss.item():.4f}")
print(f"Logits shape: {logits.shape}")

# Analyze the loss
labels = torch.arange(batch_size)
predictions_i2t = logits.argmax(dim=1)
predictions_t2i = logits.t().argmax(dim=1)

accuracy_i2t = (predictions_i2t == labels).float().mean()
accuracy_t2i = (predictions_t2i == labels).float().mean()

print(f"\nImage-to-Text accuracy: {accuracy_i2t.item():.2
print(f"Text-to-Image accuracy: {accuracy_t2i.item():.2
</code></pre>

<p><strong>Mathematical Derivation:</strong></p>

<p><strong>Part (a) \& (b): Embeddings</strong></p>

<p>Image embeddings: $\vI = [\vi_1, \vi_2, \ldots, \vi_8] \in \mathbb{R}^{8 \times 512}$</p>

<p>Text embeddings: $\vT = [\vt_1, \vt_2, \ldots, \vt_8] \in \mathbb{R}^{8 \times 512}$</p>

<p>Normalize to unit sphere:
$\hat{\vi}_i = \frac{\vi_i}{\|\vi_i\|_2}, \quad \hat{\vt}_i = \frac{\vt_i}{\|\vt_i\|_2}$</p>

<p><strong>Part (c): Similarity Matrix</strong></p>

<p>Cosine similarity matrix:
$\vS_{ij} = \frac{\hat{\vi}_i \cdot \hat{\vt}_j}{\tau}$</p>

<p>where $\tau = 0.07$ is the temperature parameter.</p>

<p>Full matrix:
$\vS = \frac{1}{\tau} \hat{\vI} \hat{\vT}^T \in \mathbb{R}^{8 \times 8}$</p>

<p>Example:
\[
\vS = \begin{bmatrix}
s_{11} & s_{12} & \cdots & s_{18} \\
s_{21} & s_{22} & \cdots & s_{28} \\
\vdots & \vdots & \ddots & \vdots \\
s_{81} & s_{82} & \cdots & s_{88}
\end{bmatrix}
\]</p>

<p>Diagonal elements $s_{ii}$ are positive pairs (matched image-text).</p>

<p>Off-diagonal elements $s_{ij}$ ($i \neq j$) are negative pairs.</p>

<p><strong>Part (d): Contrastive Loss</strong></p>

<p><strong>Image-to-Text Loss:</strong></p>

<p>For each image $i$, predict its matching text from 8 candidates:</p>

<p>$\mathcal{L}_{i2t} = -\frac{1}{8} \sum_{i=1}^{8} \log \frac{\exp(s_{ii})}{\sum_{j=1}^{8} \exp(s_{ij})}$</p>

<p>This is cross-entropy with labels $y_i = i$ (diagonal).</p>

<p><strong>Text-to-Image Loss:</strong></p>

<p>For each text $j$, predict its matching image from 8 candidates:</p>

<p>$\mathcal{L}_{t2i} = -\frac{1}{8} \sum_{j=1}^{8} \log \frac{\exp(s_{jj})}{\sum_{i=1}^{8} \exp(s_{ij})}$</p>

<p><strong>Total CLIP Loss:</strong></p>

<p>$\mathcal{L}_{\text{CLIP}} = \frac{1}{2}(\mathcal{L}_{i2t} + \mathcal{L}_{t2i})$</p>

<p>Symmetric loss ensures both modalities learn aligned representations.</p>

<p><strong>Why Temperature $\tau = 0.07$?</strong></p>

<ul>
    <li><strong>Sharpens distribution:</strong> Small $\tau$ makes softmax more peaked
    <li><strong>Emphasizes hard negatives:</strong> Distinguishes similar but incorrect pairs
    <li><strong>Empirically optimal:</strong> Found through hyperparameter search
    <li><strong>Typical range:</strong> $\tau \in [0.01, 0.1]$
</ul>

<p>Effect of temperature:
<ul>
    <li>$\tau \to 0$: Approaches hard assignment (argmax)
    <li>$\tau \to \infty$: Uniform distribution (no learning)
    <li>$\tau = 0.07$: Good balance for contrastive learning
</ul>

<p><strong>Numerical Example:</strong></p>

<p>Suppose for image 1:
<ul>
    <li>$s_{11} = 0.9$ (correct text)
    <li>$s_{12} = 0.3, s_{13} = 0.2, \ldots, s_{18} = 0.1$ (incorrect texts)
</ul>

<p>Softmax probabilities:
$p_1 = \frac{\exp(0.9/0.07)}{\exp(0.9/0.07) + \sum_{j=2}^{8} \exp(s_{1j}/0.07)}$</p>

<p>Loss for image 1:
$\ell_1 = -\log p_1$</p>

<p>If $p_1 \approx 1$, then $\ell_1 \approx 0$ (good alignment).</p>

<p>If $p_1 \approx 0.125$ (uniform), then $\ell_1 \approx 2.08$ (poor alignment).</p>

<p><strong>Training Dynamics:</strong></p>

<ol>
    <li><strong>Initial:</strong> Random embeddings, $\mathcal{L} \approx \log(8) = 2.08$
    <li><strong>Training:</strong> Embeddings align, diagonal elements increase
    <li><strong>Converged:</strong> $s_{ii} \gg s_{ij}$ for $i \neq j$, $\mathcal{L} \to 0$
</ol>

<p><strong>Key Insights:</strong></p>

<ul>
    <li>Batch size acts as number of negative samples
    <li>Larger batches improve contrastive learning (more negatives)
    <li>CLIP uses batch sizes up to 32,768 in practice
    <li>Symmetric loss prevents modality collapse
    <li>Temperature is a critical hyperparameter
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 2: CLIP Zero-Shot Classification on CIFAR-10</strong>

<pre><code>import torch
import clip
from PIL import Image
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from tqdm import tqdm

# Part (a): Load pre-trained CLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

print(f"CLIP model loaded on {device}")
print(f"Model: ViT-B/32")

# Part (b): Create text prompts for 10 CIFAR-10 classes
cifar10_classes = [
    "airplane", "automobile", "bird", "cat", "deer",
    "dog", "frog", "horse", "ship", "truck"
]

# Template-based prompts (improves accuracy)
templates = [
    "a photo of a {}.",
    "a blurry photo of a {}.",
    "a photo of many {}.",
    "a photo of the small {}.",
    "a photo of the large {}.",
]

# Encode text prompts
def encode_text_prompts(model, classes, templates):
    """Encode text prompts with multiple templates"""
    text_features = []
    
    for classname in classes:
        # Create prompts from templates
        texts = [template.format(classname) for template in templates]
        texts = clip.tokenize(texts).to(device)
        
        # Encode texts
        with torch.no_grad():
            class_embeddings = model.encode_text(texts)
            class_embeddings = class_embeddings / class_embeddings.norm(dim=-1, keepdim=True)
            
            # Average over templates
            class_embedding = class_embeddings.mean(dim=0)
            class_embedding = class_embedding / class_embedding.norm()
            
            text_features.append(class_embedding)
    
    text_features = torch.stack(text_features, dim=0)
    return text_features

text_features = encode_text_prompts(model, cifar10_classes, templates)
print(f"\nText features shape: {text_features.shape}")  # (10, 512)

# Part (c): Load CIFAR-10 test set
test_dataset = torchvision.datasets.CIFAR10(
    root='./data', 
    train=False, 
    download=True,
    transform=preprocess
)

test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)

# Zero-shot classification
def zero_shot_classify(model, loader, text_features):
    """Perform zero-shot classification"""
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in tqdm(loader):
            images = images.to(device)
            labels = labels.to(device)
            
            # Encode images
            image_features = model.encode_image(images)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # Compute similarity with text features
            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
            
            # Predict
            predictions = similarity.argmax(dim=-1)
            
            correct += (predictions == labels).sum().item()
            total += labels.size(0)
    
    accuracy = 100.0 * correct / total
    return accuracy

# Part (d): Compute accuracy
zero_shot_accuracy = zero_shot_classify(model, test_loader, text_features)
print(f"\nZero-shot accuracy: {zero_shot_accuracy:.2f}



# Part (e): Compare to supervised baseline
# Train a simple supervised classifier
class SimpleCNN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = torch.nn.Conv2d(32, 64, 3, padding=1)
        self.pool = torch.nn.MaxPool2d(2, 2)
        self.fc1 = torch.nn.Linear(64 * 8 * 8, 128)
        self.fc2 = torch.nn.Linear(128, 10)
    
    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Train supervised model (simplified)
supervised_model = SimpleCNN().to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(supervised_model.parameters(), lr=0.001)

# Training loop (10 epochs for quick comparison)
train_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True,
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)

for epoch in range(10):
    supervised_model.train()
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = supervised_model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Evaluate supervised model
supervised_model.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = supervised_model(images)
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

supervised_accuracy = 100.0 * correct / total

print(f"\nComparison:")
print(f"CLIP Zero-shot: {zero_shot_accuracy:.2f}
print(f"Supervised CNN (10 epochs): {supervised_accuracy:.2f}
</code></pre>

<p><strong>Expected Results:</strong></p>

<table>
<tr><th><strong>Method</strong></th><th><strong>Accuracy</strong></th><th><strong>Training Data</strong></th></tr>
<tr><td>CLIP Zero-shot (ViT-B/32)</td><td>89-91\%</td><td>0 (CIFAR-10)</td></tr>
<tr><td>CLIP Zero-shot (ViT-L/14)</td><td>93-95\%</td><td>0 (CIFAR-10)</td></tr>
<tr><td>Supervised CNN (10 epochs)</td><td>70-75\%</td><td>50k (CIFAR-10)</td></tr>
<tr><td>Supervised ResNet-50 (200 epochs)</td><td>95-96\%</td><td>50k (CIFAR-10)</td></tr>
</table>

<p><strong>Analysis:</strong></p>

<p><strong>Part (a): Pre-trained CLIP Model</strong></p>

<p>CLIP models available:
<ul>
    <li><strong>RN50:</strong> ResNet-50 image encoder
    <li><strong>ViT-B/32:</strong> ViT-Base with patch size 32
    <li><strong>ViT-B/16:</strong> ViT-Base with patch size 16 (better)
    <li><strong>ViT-L/14:</strong> ViT-Large with patch size 14 (best)
</ul>

<p>Pre-training:
<ul>
    <li>Dataset: 400M image-text pairs from internet
    <li>Training: Contrastive learning for 32 epochs
    <li>Batch size: 32,768 (large-scale)
    <li>Compute: 256 V100 GPUs for 12 days
</ul>

<p><strong>Part (b): Text Prompts</strong></p>

<strong>Simple prompts:</strong>
<pre><code>
"airplane", "automobile", "bird", ...
</code></pre>

<strong>Template-based prompts (better):</strong>
<pre><code>
"a photo of a airplane."
"a blurry photo of a airplane."
"a photo of many airplanes."
</code></pre>

<p>Why templates help:
<ul>
    <li>Match training distribution (natural sentences)
    <li>Provide context for ambiguous classes
    <li>Ensemble over multiple descriptions
    <li>Improve robustness to variations
</ul>

<p>Prompt engineering tips:
<ul>
    <li>Use natural language sentences
    <li>Include domain-specific context
    <li>Try multiple templates and average
    <li>Avoid overly specific descriptions
</ul>

<p><strong>Part (c): Encoding Process</strong></p>

<p><strong>Image encoding:</strong>
<ol>
    <li>Preprocess: Resize to $224 \times 224$, normalize
    <li>ViT encoder: Extract features
    <li>Projection: Map to shared embedding space (512-dim)
    <li>Normalize: $\hat{\vi} = \vi / \|\vi\|_2$
</ol>

<p><strong>Text encoding:</strong>
<ol>
    <li>Tokenize: Convert text to token IDs
    <li>Text encoder: Transformer processes tokens
    <li>Projection: Map to shared embedding space (512-dim)
    <li>Normalize: $\hat{\vt} = \vt / \|\vt\|_2$
</ol>

<p><strong>Part (d): Zero-Shot Classification</strong></p>

<p><strong>Algorithm:</strong></p>

<p>For each test image $\vx$:
<ol>
    <li>Encode image: $\vi = \text{ImageEncoder}(\vx)$
    <li>Compute similarity with all class embeddings:
    $s_k = \vi \cdot \vt_k$ for $k = 1, \ldots, 10$
    <li>Apply softmax: $p_k = \frac{\exp(s_k / \tau)}{\sum_{j=1}^{10} \exp(s_j / \tau)}$
    <li>Predict: $\hat{y} = \argmax_k p_k$
</ol>

<p>Temperature $\tau = 0.01$ (learned during training).</p>

<p><strong>Mathematical Formulation:</strong></p>

<p>$P(y = k | \vx) = \frac{\exp(\text{sim}(\vi, \vt_k) / \tau)}{\sum_{j=1}^{10} \exp(\text{sim}(\vi, \vt_j) / \tau)}$</p>

<p>where $\text{sim}(\vi, \vt) = \vi \cdot \vt$ (cosine similarity after normalization).</p>

<p><strong>Part (e): Comparison with Supervised Baseline</strong></p>

<p><strong>Why CLIP Zero-Shot Outperforms Supervised CNN:</strong></p>

<ol>
    <li><strong>Pre-training scale:</strong> 400M image-text pairs vs 50k CIFAR-10 images
    <li><strong>Transfer learning:</strong> Leverages knowledge from diverse data
    <li><strong>Better architecture:</strong> ViT-B/32 vs simple CNN
    <li><strong>Semantic understanding:</strong> Learns concepts, not just patterns
    <li><strong>Robustness:</strong> Generalizes better to distribution shifts
</ol>

<p><strong>When Supervised Wins:</strong></p>

<ul>
    <li><strong>Sufficient training data:</strong> ResNet-50 with 200 epochs reaches 95-96\%
    <li><strong>Domain-specific:</strong> Fine-tuned models beat zero-shot on specialized tasks
    <li><strong>Computational constraints:</strong> Smaller models are faster
</ul>

<p><strong>CLIP Advantages:</strong></p>

<ol>
    <li><strong>No training required:</strong> Instant deployment
    <li><strong>Flexible:</strong> Change classes without retraining
    <li><strong>Interpretable:</strong> Natural language descriptions
    <li><strong>Robust:</strong> Handles distribution shifts better
    <li><strong>Multimodal:</strong> Can do image-text retrieval, captioning, etc.
</ol>

<p><strong>Practical Recommendations:</strong></p>

<table>
<tr><th><strong>Scenario</strong></th><th><strong>Recommendation</strong></th></tr>
<tr><td>Quick prototype</td><td>CLIP zero-shot</td></tr>
<tr><td>Fixed classes, lots of data</td><td>Supervised training</td></tr>
<tr><td>Changing classes frequently</td><td>CLIP zero-shot</td></tr>
<tr><td>Maximum accuracy</td><td>Fine-tune CLIP</td></tr>
<tr><td>Limited compute</td><td>Supervised small model</td></tr>
<tr><td>Interpretability needed</td><td>CLIP with prompts</td></tr>
</table>

<p><strong>Improving CLIP Zero-Shot:</strong></p>

<ol>
    <li><strong>Better prompts:</strong> Domain-specific templates
    <li><strong>Larger model:</strong> ViT-L/14 instead of ViT-B/32
    <li><strong>Ensemble:</strong> Average predictions from multiple prompts
    <li><strong>Few-shot:</strong> Add a few examples with linear probe
    <li><strong>Fine-tuning:</strong> Adapt to target domain
</ol>

<p><strong>Key Takeaways:</strong></p>

<ul>
    <li>CLIP achieves strong zero-shot performance through large-scale pre-training
    <li>Natural language prompts enable flexible classification
    <li>Zero-shot CLIP often matches or exceeds supervised baselines
    <li>Prompt engineering is crucial for optimal performance
    <li>CLIP's multimodal nature enables many downstream tasks
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 3: Whisper Architecture Analysis</strong>

<p><strong>Part (a): Encoder Parameters (24 layers, $d=1024$)</strong></p>

<p><strong>Whisper Encoder Configuration:</strong>
<ul>
    <li>Layers: $L = 24$
    <li>Hidden size: $d = 1024$
    <li>Attention heads: $h = 16$
    <li>MLP ratio: $4.0$ (MLP size = $4096$)
    <li>Audio features: 80-dimensional log-mel spectrogram
    <li>Sequence length: $T = 3000$ (30 seconds at 100 Hz)
</ul>

<p><strong>Parameter Breakdown:</strong></p>

<p><strong>1. Input Convolution Layers:</strong>
<ul>
    <li>Conv1: $80 \times 3 \times 1024 = 245{,}760$
    <li>Conv2: $1024 \times 3 \times 1024 = 3{,}145{,}728$
    <li>Total: $3{,}391{,}488$ parameters
</ul>

<p><strong>2. Position Embeddings:</strong>
<ul>
    <li>Sinusoidal (not learned): 0 parameters
</ul>

<p><strong>3. Per Transformer Layer:</strong></p>

<p><em>Multi-Head Attention:</em>
<ul>
    <li>$Q, K, V$ projections: $3 \times 1024^2 = 3{,}145{,}728$
    <li>Output projection: $1024^2 = 1{,}048{,}576$
    <li>Total attention: $4{,}194{,}304$
</ul>

<p><em>MLP:</em>
<ul>
    <li>First linear: $1024 \times 4096 = 4{,}194{,}304$
    <li>Second linear: $4096 \times 1024 = 4{,}194{,}304$
    <li>Total MLP: $8{,}388{,}608$
</ul>

<p><em>Layer Normalization:</em>
<ul>
    <li>2 LayerNorms: $2 \times 2 \times 1024 = 4{,}096$
</ul>

<p><strong>Total per layer: $12{,}587{,}008$ parameters</strong></p>

<p><strong>4. All 24 Encoder Layers:</strong>
$24 \times 12{,}587{,}008 = 302{,}088{,}192$ parameters</p>

<p><strong>Total Encoder: $\approx 305.5$M parameters</strong></p>

<p><strong>Part (b): Decoder Parameters (24 layers)</strong></p>

<p><strong>Whisper Decoder Configuration:</strong>
<ul>
    <li>Layers: $L = 24$
    <li>Hidden size: $d = 1024$
    <li>Attention heads: $h = 16$
    <li>Vocabulary size: $V = 51{,}865$
    <li>Max sequence length: $448$ tokens
</ul>

<p><strong>Parameter Breakdown:</strong></p>

<p><strong>1. Token Embedding:</strong>
<ul>
    <li>$51{,}865 \times 1024 = 53{,}109{,}760$ parameters
</ul>

<p><strong>2. Position Embeddings:</strong>
<ul>
    <li>$448 \times 1024 = 458{,}752$ parameters
</ul>

<p><strong>3. Per Decoder Layer:</strong></p>

<p><em>Masked Self-Attention:</em>
<ul>
    <li>Same as encoder: $4{,}194{,}304$ parameters
</ul>

<p><em>Cross-Attention:</em>
<ul>
    <li>$Q$ projection: $1024^2 = 1{,}048{,}576$
    <li>$K, V$ projections (from encoder): $2 \times 1024^2 = 2{,}097{,}152$
    <li>Output projection: $1024^2 = 1{,}048{,}576$
    <li>Total cross-attention: $4{,}194{,}304$
</ul>

<p><em>MLP:</em>
<ul>
    <li>Same as encoder: $8{,}388{,}608$ parameters
</ul>

<p><em>Layer Normalization:</em>
<ul>
    <li>3 LayerNorms: $3 \times 2 \times 1024 = 6{,}144$
</ul>

<p><strong>Total per decoder layer: $16{,}783{,}360$ parameters</strong></p>

<p><strong>4. All 24 Decoder Layers:</strong>
$24 \times 16{,}783{,}360 = 402{,}800{,}640$ parameters</p>

<p><strong>5. Output Projection:</strong>
<ul>
    <li>Shared with token embedding: 0 additional parameters
</ul>

<p><strong>Total Decoder: $\approx 456.4$M parameters</strong></p>

<p><strong>Total Whisper Model: $305.5 + 456.4 = 761.9$M parameters</strong></p>

<p>(Actual Whisper-large: $\approx 1.55$B parameters due to additional components)</p>

<p><strong>Part (c): Memory for 30-Second Audio</strong></p>

<p><strong>Input Processing:</strong></p>

<p><strong>1. Audio Preprocessing:</strong>
<ul>
    <li>Sample rate: 16 kHz
    <li>30 seconds: $30 \times 16{,}000 = 480{,}000$ samples
    <li>Raw audio: $480{,}000 \times 4$ bytes = 1.92 MB
</ul>

<p><strong>2. Log-Mel Spectrogram:</strong>
<ul>
    <li>Window size: 25 ms (400 samples)
    <li>Hop length: 10 ms (160 samples)
    <li>Number of frames: $\frac{480{,}000}{160} = 3{,}000$
    <li>Mel bins: 80
    <li>Features: $3{,}000 \times 80 = 240{,}000$ values
    <li>Memory: $240{,}000 \times 4$ bytes = 0.96 MB
</ul>

<p><strong>Encoder Memory (Inference):</strong></p>

<p><strong>1. Activations per layer:</strong>
<ul>
    <li>Input: $3{,}000 \times 1024 = 3{,}072{,}000$ values
    <li>Attention scores: $16 \times 3{,}000 \times 3{,}000 = 144{,}000{,}000$ values
    <li>MLP intermediate: $3{,}000 \times 4096 = 12{,}288{,}000$ values
</ul>

<p>Peak per layer: $\approx 159$M values $\times$ 4 bytes = 636 MB</p>

<p><strong>2. Total encoder activations:</strong>
$24 \times 636$ MB = 15.3 GB (if storing all layers)</p>

<p>With activation checkpointing: $\approx 1.3$ GB</p>

<p><strong>Decoder Memory (Inference):</strong></p>

<p>For generating 448 tokens:
<ul>
    <li>Decoder activations: $448 \times 1024 = 458{,}752$ values per layer
    <li>Cross-attention: $448 \times 3{,}000 = 1{,}344{,}000$ values per layer
    <li>KV cache: $2 \times 24 \times 448 \times 1024 = 22{,}020{,}096$ values
</ul>

<p>Decoder memory: $\approx 500$ MB</p>

<p><strong>Total Memory (Inference):</strong>
<ul>
    <li>Model parameters: $1.55$B $\times$ 4 bytes = 6.2 GB
    <li>Encoder activations: $\approx 1.3$ GB (with checkpointing)
    <li>Decoder activations: $\approx 0.5$ GB
    <li>KV cache: $\approx 0.1$ GB
    <li><strong>Total: $\approx 8.1$ GB</strong>
</ul>

<p>For FP16: $\approx 4.1$ GB</p>

<p>For INT8 quantization: $\approx 2.1$ GB</p>

<p><strong>Part (d): Compare to Text-Only GPT-2</strong></p>

<p><strong>GPT-2 (1.5B parameters):</strong>
<ul>
    <li>Layers: 48
    <li>Hidden size: 1600
    <li>Attention heads: 25
    <li>Vocabulary: 50,257
    <li>Context length: 1024 tokens
</ul>

<p><strong>Comparison Table:</strong></p>

<table>
<tr><th><strong>Metric</strong></th><th><strong>Whisper-large</strong></th><th><strong>GPT-2 (1.5B)</strong></th></tr>
<tr><td>Total Parameters</td><td>1.55B</td><td>1.5B</td></tr>
<tr><td>Encoder Layers</td><td>24</td><td>N/A</td></tr>
<tr><td>Decoder Layers</td><td>24</td><td>48</td></tr>
<tr><td>Hidden Size</td><td>1024</td><td>1600</td></tr>
<tr><td>Attention Heads</td><td>16</td><td>25</td></tr>
<tr><td>Input Modality</td><td>Audio</td><td>Text</td></tr>
<tr><td>Output Modality</td><td>Text</td><td>Text</td></tr>
<tr><td>Context Length</td><td>3000 (audio) + 448 (text)</td><td>1024 (text)</td></tr>
<tr><td>Memory (FP32)</td><td>8.1 GB</td><td>6.5 GB</td></tr>
<tr><td>Inference Speed</td><td>Slower (audio encoding)</td><td>Faster</td></tr>
</table>

<p><strong>Key Differences:</strong></p>

<ol>
    <li><strong>Architecture:</strong>
    <ul>
        <li>Whisper: Encoder-decoder (like T5)
        <li>GPT-2: Decoder-only
    </ul>
    
    <li><strong>Input Processing:</strong>
    <ul>
        <li>Whisper: Audio $\to$ Log-mel $\to$ Encoder
        <li>GPT-2: Text $\to$ Tokens $\to$ Decoder
    </ul>
    
    <li><strong>Computational Cost:</strong>
    <ul>
        <li>Whisper encoder: $O(T^2 d)$ where $T = 3000$
        <li>GPT-2: $O(n^2 d)$ where $n = 1024$
        <li>Whisper is $\approx 9\times$ more expensive for encoder
    </ul>
    
    <li><strong>Memory Footprint:</strong>
    <ul>
        <li>Whisper: Larger due to long audio sequences
        <li>GPT-2: Smaller, text-only
    </ul>
    
    <li><strong>Use Cases:</strong>
    <ul>
        <li>Whisper: Speech recognition, translation, transcription
        <li>GPT-2: Text generation, completion, summarization
    </ul>
</ol>

<p><strong>Why Whisper Needs Encoder-Decoder:</strong></p>

<ul>
    <li><strong>Cross-modal:</strong> Audio input, text output
    <li><strong>Compression:</strong> Encoder compresses 3000 audio frames
    <li><strong>Attention:</strong> Decoder attends to compressed audio
    <li><strong>Efficiency:</strong> Encoder processes audio once, decoder generates text autoregressively
</ul>

<p><strong>Performance Comparison:</strong></p>

<table>
<tr><th><strong>Task</strong></th><th><strong>Whisper</strong></th><th><strong>GPT-2</strong></th></tr>
<tr><td>Speech Recognition</td><td>Excellent</td><td>N/A</td></tr>
<tr><td>Text Generation</td><td>N/A</td><td>Excellent</td></tr>
<tr><td>Multilingual</td><td>99 languages</td><td>Limited</td></tr>
<tr><td>Robustness</td><td>High (noisy audio)</td><td>N/A</td></tr>
<tr><td>Zero-shot</td><td>Strong</td><td>Strong</td></tr>
</table>

<p><strong>Practical Considerations:</strong></p>

<ol>
    <li><strong>Deployment:</strong>
    <ul>
        <li>Whisper: Requires audio preprocessing
        <li>GPT-2: Simple tokenization
    </ul>
    
    <li><strong>Latency:</strong>
    <ul>
        <li>Whisper: Higher (audio encoding + decoding)
        <li>GPT-2: Lower (text-only)
    </ul>
    
    <li><strong>Hardware:</strong>
    <ul>
        <li>Whisper: Needs GPU for real-time (8+ GB VRAM)
        <li>GPT-2: Can run on CPU for small batches
    </ul>
</ol>

<p><strong>Key Insights:</strong></p>

<ul>
    <li>Whisper and GPT-2 have similar parameter counts but different architectures
    <li>Encoder-decoder is essential for cross-modal tasks
    <li>Audio sequences are much longer than text, requiring more memory
    <li>Both models benefit from large-scale pre-training
    <li>Whisper's multimodal nature enables speech-to-text applications
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 4: Multimodal Fusion for Video Understanding</strong>

<p><strong>Part (a): Proposed Architecture</strong></p>

<pre><code>import torch
import torch.nn as nn

class MultimodalVideoTransformer(nn.Module):
    def __init__(self, 
                 visual_dim=768,      # ViT features
                 audio_dim=512,       # Audio features
                 text_dim=768,        # BERT features
                 hidden_dim=1024,     # Fusion dimension
                 num_layers=12,       # Fusion transformer layers
                 num_heads=16,
                 num_classes=400):    # Action recognition classes
        super().__init__()
        
        # Modality-specific encoders
        self.visual_encoder = VisualEncoder(visual_dim, hidden_dim)
        self.audio_encoder = AudioEncoder(audio_dim, hidden_dim)
        self.text_encoder = TextEncoder(text_dim, hidden_dim)
        
        # Modality-specific tokens
        self.visual_token = nn.Parameter(torch.randn(1, 1, hidden_dim))
        self.audio_token = nn.Parameter(torch.randn(1, 1, hidden_dim))
        self.text_token = nn.Parameter(torch.randn(1, 1, hidden_dim))
        
        # Fusion transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=0.1,
            batch_first=True
        )
        self.fusion_transformer = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=num_layers
        )
        
        # Classification head
        self.classifier = nn.Sequential(
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, num_classes)
        )
    
    def forward(self, visual_features, audio_features, text_features):
        """
        Args:
            visual_features: (B, T_v, D_v) - video frames
            audio_features: (B, T_a, D_a) - audio segments
            text_features: (B, T_t, D_t) - caption tokens
        Returns:
            logits: (B, num_classes)
        """
        B = visual_features.shape[0]
        
        # Encode each modality
        visual_emb = self.visual_encoder(visual_features)  # (B, T_v, H)
        audio_emb = self.audio_encoder(audio_features)     # (B, T_a, H)
        text_emb = self.text_encoder(text_features)        # (B, T_t, H)
        
        # Add modality tokens
        visual_token = self.visual_token.expand(B, -1, -1)
        audio_token = self.audio_token.expand(B, -1, -1)
        text_token = self.text_token.expand(B, -1, -1)
        
        visual_emb = torch.cat([visual_token, visual_emb], dim=1)
        audio_emb = torch.cat([audio_token, audio_emb], dim=1)
        text_emb = torch.cat([text_token, text_emb], dim=1)
        
        # Concatenate all modalities
        multimodal_emb = torch.cat([visual_emb, audio_emb, text_emb], dim=1)
        # Shape: (B, 1+T_v + 1+T_a + 1+T_t, H)
        
        # Fusion transformer
        fused = self.fusion_transformer(multimodal_emb)
        
        # Aggregate: average modality tokens
        visual_rep = fused[:, 0, :]
        audio_rep = fused[:, 1+visual_features.shape[1], :]
        text_rep = fused[:, 1+visual_features.shape[1]+1+audio_features.shape[1], :]
        
        # Combine representations
        combined = (visual_rep + audio_rep + text_rep) / 3
        
        # Classification
        logits = self.classifier(combined)
        
        return logits

class VisualEncoder(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.proj = nn.Linear(input_dim, output_dim)
        self.norm = nn.LayerNorm(output_dim)
    
    def forward(self, x):
        return self.norm(self.proj(x))

class AudioEncoder(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.proj = nn.Linear(input_dim, output_dim)
        self.norm = nn.LayerNorm(output_dim)
    
    def forward(self, x):
        return self.norm(self.proj(x))

class TextEncoder(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.proj = nn.Linear(input_dim, output_dim)
        self.norm = nn.LayerNorm(output_dim)
    
    def forward(self, x):
        return self.norm(self.proj(x))

# Example usage
model = MultimodalVideoTransformer()

# Simulate inputs
batch_size = 4
visual = torch.randn(batch_size, 16, 768)   # 16 frames
audio = torch.randn(batch_size, 32, 512)    # 32 audio segments
text = torch.randn(batch_size, 20, 768)     # 20 caption tokens

logits = model(visual, audio, text)
print(f"Output shape: {logits.shape}")  # (4, 400)
</code></pre>

<p><strong>Part (b): Fusion Mechanism</strong></p>

<p><strong>Architecture Overview:</strong></p>

<pre><code>
Input:
  Visual: (B, 16, 768)  - 16 video frames from ViT
  Audio:  (B, 32, 512)  - 32 audio segments from audio encoder
  Text:   (B, 20, 768)  - 20 caption tokens from BERT

Step 1: Modality-Specific Projection
  Visual -> (B, 16, 1024)
  Audio  -> (B, 32, 1024)
  Text   -> (B, 20, 1024)

Step 2: Add Modality Tokens
  Visual: [V_token, v1, v2, ..., v16]  -> (B, 17, 1024)
  Audio:  [A_token, a1, a2, ..., a32]  -> (B, 33, 1024)
  Text:   [T_token, t1, t2, ..., t20]  -> (B, 21, 1024)

Step 3: Concatenate
  Multimodal: [V_token, v1, ..., v16, A_token, a1, ..., a32, T_token, t1, ..., t20]
  Shape: (B, 71, 1024)

Step 4: Fusion Transformer (12 layers)
  Cross-modal attention enables interaction
  Output: (B, 71, 1024)

Step 5: Aggregate
  Extract modality tokens: V_token, A_token, T_token
  Average: (V_token + A_token + T_token) / 3
  Shape: (B, 1024)

Step 6: Classification
  MLP: (B, 1024) -> (B, 400)
</code></pre>

<p><strong>Fusion Strategies Comparison:</strong></p>

<ol>
    <li><strong>Early Fusion (Concatenation):</strong>
    <ul>
        <li>Concatenate features before transformer
        <li>Simple but limited cross-modal interaction
        <li>Used in this design
    </ul>
    
    <li><strong>Late Fusion (Ensemble):</strong>
    <ul>
        <li>Process modalities separately
        <li>Combine predictions at the end
        <li>No cross-modal learning
    </ul>
    
    <li><strong>Cross-Modal Attention:</strong>
    <ul>
        <li>Visual attends to audio and text
        <li>Audio attends to visual and text
        <li>More complex but better interaction
    </ul>
    
    <li><strong>Bottleneck Fusion:</strong>
    <ul>
        <li>Compress each modality to bottleneck tokens
        <li>Fuse bottlenecks
        <li>More efficient for long sequences
    </ul>
</ol>

<p><strong>Why This Design:</strong></p>

<ul>
    <li><strong>Modality tokens:</strong> Aggregate information from each modality
    <li><strong>Shared transformer:</strong> Enables cross-modal attention
    <li><strong>Flexible:</strong> Can handle missing modalities
    <li><strong>Scalable:</strong> Easy to add more modalities
</ul>

<p><strong>Part (c): Training Objective</strong></p>

<p><strong>Primary Objective: Action Recognition</strong></p>

<p>$\mathcal{L}_{\text{action}} = -\frac{1}{B} \sum_{i=1}^{B} \log P(y_i | \vv_i, \va_i, \vt_i)$</p>

<p>where:
<ul>
    <li>$\vv_i$: visual features for sample $i$
    <li>$\va_i$: audio features for sample $i$
    <li>$\vt_i$: text features for sample $i$
    <li>$y_i$: ground truth action class
</ul>

<p><strong>Auxiliary Objectives (Multi-Task Learning):</strong></p>

<p><strong>1. Contrastive Loss (Cross-Modal Alignment):</strong></p>

<p>Align visual-audio, visual-text, audio-text pairs:</p>

<p>$\mathcal{L}_{\text{contrast}} = \mathcal{L}_{\text{VA}} + \mathcal{L}_{\text{VT}} + \mathcal{L}_{\text{AT}}$</p>

<p>where each term is CLIP-style contrastive loss:</p>

<p>$\mathcal{L}_{\text{VA}} = -\frac{1}{B} \sum_{i=1}^{B} \log \frac{\exp(\text{sim}(\vv_i, \va_i) / \tau)}{\sum_{j=1}^{B} \exp(\text{sim}(\vv_i, \va_j) / \tau)}$</p>

<p><strong>2. Masked Modality Modeling:</strong></p>

<p>Randomly mask one modality and predict it from others:</p>

<p>$\mathcal{L}_{\text{mask}} = \mathcal{L}_{\text{mask-V}} + \mathcal{L}_{\text{mask-A}} + \mathcal{L}_{\text{mask-T}}$</p>

<p>Example (mask visual):
$\mathcal{L}_{\text{mask-V}} = \|\hat{\vv} - \vv\|_2^2$</p>

<p>where $\hat{\vv} = f(\va, \vt)$ is predicted visual features.</p>

<p><strong>3. Temporal Ordering:</strong></p>

<p>Predict correct temporal order of video segments:</p>

<p>$\mathcal{L}_{\text{temporal}} = -\log P(\text{order} | \vv, \va, \vt)$</p>

<p><strong>Total Training Objective:</strong></p>

<p>$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{action}} + \lambda_1 \mathcal{L}_{\text{contrast}} + \lambda_2 \mathcal{L}_{\text{mask}} + \lambda_3 \mathcal{L}_{\text{temporal}}$</p>

<p>Typical weights: $\lambda_1 = 0.1$, $\lambda_2 = 0.05$, $\lambda_3 = 0.05$</p>

<p><strong>Training Recipe:</strong></p>

<pre><code># Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)

# Learning rate schedule
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

# Training loop
for epoch in range(100):
    for batch in dataloader:
        visual, audio, text, labels = batch
        
        # Forward pass
        logits = model(visual, audio, text)
        
        # Action recognition loss
        loss_action = F.cross_entropy(logits, labels)
        
        # Contrastive loss (optional)
        visual_rep = model.get_visual_rep(visual)
        audio_rep = model.get_audio_rep(audio)
        loss_contrast = contrastive_loss(visual_rep, audio_rep)
        
        # Total loss
        loss = loss_action + 0.1 * loss_contrast
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    scheduler.step()
</code></pre>

<p><strong>Data Augmentation:</strong></p>

<ul>
    <li><strong>Visual:</strong> Random crop, color jitter, temporal sampling
    <li><strong>Audio:</strong> Time stretching, pitch shifting, noise injection
    <li><strong>Text:</strong> Synonym replacement, back-translation
    <li><strong>Multimodal:</strong> Random modality dropout (robustness)
</ul>

<p><strong>Part (d): Parameter Count Estimation</strong></p>

<p><strong>Component Breakdown:</strong></p>

<p><strong>1. Modality-Specific Encoders:</strong></p>

<p><em>Visual Encoder:</em>
<ul>
    <li>Projection: $768 \times 1024 = 786{,}432$
    <li>LayerNorm: $2 \times 1024 = 2{,}048$
    <li>Total: $788{,}480$
</ul>

<p><em>Audio Encoder:</em>
<ul>
    <li>Projection: $512 \times 1024 = 524{,}288$
    <li>LayerNorm: $2 \times 1024 = 2{,}048$
    <li>Total: $526{,}336$
</ul>

<p><em>Text Encoder:</em>
<ul>
    <li>Projection: $768 \times 1024 = 786{,}432$
    <li>LayerNorm: $2 \times 1024 = 2{,}048$
    <li>Total: $788{,}480$
</ul>

<p><strong>Encoder total: $2{,}103{,}296$ parameters</strong></p>

<p><strong>2. Modality Tokens:</strong>
<ul>
    <li>3 tokens $\times$ 1024 = $3{,}072$ parameters
</ul>

<p><strong>3. Fusion Transformer (12 layers):</strong></p>

<p>Per layer:
<ul>
    <li>Self-attention: $4 \times 1024^2 = 4{,}194{,}304$
    <li>MLP: $2 \times 1024 \times 4096 = 8{,}388{,}608$
    <li>LayerNorm: $2 \times 2 \times 1024 = 4{,}096$
    <li>Total per layer: $12{,}587{,}008$
</ul>

<p>12 layers: $12 \times 12{,}587{,}008 = 151{,}044{,}096$ parameters</p>

<p><strong>4. Classification Head:</strong>
<ul>
    <li>LayerNorm: $2 \times 1024 = 2{,}048$
    <li>Linear 1: $1024 \times 1024 = 1{,}048{,}576$
    <li>Linear 2: $1024 \times 400 = 409{,}600$
    <li>Total: $1{,}460{,}224$
</ul>

<p><strong>Total Model Parameters:</strong></p>

<p>$2{,}103{,}296 + 3{,}072 + 151{,}044{,}096 + 1{,}460{,}224 = 154{,}610{,}688$</p>

<p><strong>Total: $\approx 155$M parameters</strong></p>

<p><strong>Memory Footprint (FP32):</strong></p>

<ul>
    <li>Parameters: $155$M $\times$ 4 bytes = 620 MB
    <li>Activations (batch size 4):
    <ul>
        <li>Input: $4 \times 71 \times 1024 = 290{,}816$ values
        <li>Per layer: $\approx 2$M values
        <li>Total: $\approx 24$M values $\times$ 4 bytes = 96 MB
    </ul>
    <li>Gradients: 620 MB (same as parameters)
    <li>Optimizer states (AdamW): $2 \times 620$ MB = 1.24 GB
</ul>

<p><strong>Total training memory: $\approx 2.6$ GB</strong></p>

<p><strong>Comparison with Baselines:</strong></p>

<table>
<tr><th><strong>Model</strong></th><th><strong>Parameters</strong></th><th><strong>Modalities</strong></th></tr>
<tr><td>Single-modal (visual only)</td><td>86M</td><td>1</td></tr>
<tr><td>Two-modal (visual + audio)</td><td>120M</td><td>2</td></tr>
<tr><td>Our three-modal</td><td>155M</td><td>3</td></tr>
<tr><td>CLIP (ViT-B/32)</td><td>151M</td><td>2</td></tr>
<tr><td>Whisper-large</td><td>1.55B</td><td>2</td></tr>
</table>

<p><strong>Design Trade-offs:</strong></p>

<ol>
    <li><strong>Parameter efficiency:</strong>
    <ul>
        <li>Shared fusion transformer reduces parameters
        <li>Modality-specific encoders are lightweight
        <li>Could use pre-trained encoders (ViT, BERT, etc.)
    </ul>
    
    <li><strong>Computational cost:</strong>
    <ul>
        <li>Sequence length: 71 tokens (manageable)
        <li>Attention complexity: $O(71^2 \times 1024) \approx 5$M operations
        <li>Inference time: $\approx 50$ ms on GPU
    </ul>
    
    <li><strong>Scalability:</strong>
    <ul>
        <li>Easy to add more modalities (depth, optical flow, etc.)
        <li>Can increase fusion layers for better interaction
        <li>Bottleneck fusion for longer sequences
    </ul>
</ol>

<p><strong>Practical Recommendations:</strong></p>

<ol>
    <li><strong>Use pre-trained encoders:</strong> ViT for visual, Wav2Vec for audio, BERT for text
    <li><strong>Freeze encoders initially:</strong> Train fusion transformer first
    <li><strong>Fine-tune end-to-end:</strong> Unfreeze all parameters later
    <li><strong>Modality dropout:</strong> Randomly drop modalities during training for robustness
    <li><strong>Temporal modeling:</strong> Add temporal attention for video sequences
</ol>

<p><strong>Key Insights:</strong></p>

<ul>
    <li>Multimodal fusion requires careful architecture design
    <li>Modality tokens enable flexible aggregation
    <li>Shared transformer enables cross-modal learning
    <li>Multi-task learning improves representation quality
    <li>Parameter count is reasonable for modern GPUs
    <li>Pre-trained encoders significantly improve performance
</ul>
</div>
        
        <div class="chapter-nav">
  <a href="chapter17_vision_transformers.html">‚Üê Chapter 17: Vision Transformers</a>
  <a href="../../deeptech.html">üìö Table of Contents</a>
  <a href="chapter19_long_context.html">Chapter 19: Long Context Handling ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
