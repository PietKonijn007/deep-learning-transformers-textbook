<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 29: Recommendation Systems - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../../deeptech.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Recommendation Systems and Personalization</h1>

<h2>Chapter Overview</h2>

<p>Recommendation systems are the economic engines of the modern internet. They drive 80\% of content consumed on Netflix, 70\% of watch time on YouTube, and 35\% of purchases on Amazon. For these platforms, recommendation quality directly translates to revenue: a 1\% improvement in recommendation accuracy can generate tens to hundreds of millions of dollars in additional revenue annually. Poor recommendations, conversely, lead to user churn, reduced engagement, and lost revenue opportunities.</p>

<p>The business challenge is substantial. Platforms must serve personalized recommendations to billions of users in real-time (under 200ms latency), processing trillions of user-item interactions to learn preferences, while balancing competing objectives: immediate engagement (clicks, watch time) versus long-term retention, personalization versus diversity, and business goals (revenue, growth) versus societal concerns (fairness, filter bubbles).</p>

<p>This chapter examines how transformers and sequence models have revolutionized recommendation systems by capturing temporal dynamics and complex user-item interactions that traditional methods miss. A user who watches action movies followed by documentaries has different preferences than one who watches in reverse order‚Äîsequence matters. Transformer-based recommenders capture these patterns, improving recommendation quality by 10-30\% over traditional collaborative filtering.</p>

<p>However, these improvements come with challenges. Training requires processing billions of user interactions. Serving demands sub-second latency at massive scale. Model drift is severe‚Äîuser preferences change daily, new items arrive constantly, and seasonal patterns shift. Fairness concerns are paramount‚Äîbiased recommendations can amplify inequality and create filter bubbles that harm users and society. This chapter provides the technical foundation and business context to build recommendation systems that balance these competing demands effectively.</p>

<h2>Learning Objectives</h2>

<ol>
<li>Understand sequence-based recommendation architectures using transformers
<li>Design and optimize ranking systems for accuracy and diversity
<li>Implement multi-task learning for recommendations (CTR, conversion, long-term engagement)
<li>Build real-time serving systems with latency constraints
<li>Address fairness and filter-bubble concerns in personalization
<li>Conduct online experiments (A/B tests) to validate recommendation improvements
<li>Optimize for business metrics beyond accuracy
</ol>

<h2>Sequence-Aware Recommenders</h2>

<p>Traditional recommendation systems treat user preferences as static. Matrix factorization, the workhorse of early recommender systems, decomposes a user-item interaction matrix $M \in \mathbb{R}^{U \times I}$ into low-rank factors: user $u$ is represented by a latent vector $\mathbf{p}_u \in \mathbb{R}^k$, and item $i$ by $\mathbf{q}_i \in \mathbb{R}^k$. The predicted rating is simply $\hat{M}_{u,i} = \mathbf{p}_u^T \mathbf{q}_i$. This approach powered early Netflix and Amazon recommendations and remains computationally efficient.</p>

<p>However, this static view ignores a fundamental aspect of human behavior: preferences evolve over time and depend on context. A user watching action movies in January followed by documentaries in February has different current preferences than one who watched in reverse order. A user browsing products on Monday morning (work-related) has different intent than the same user browsing Saturday evening (leisure). Matrix factorization treats these scenarios identically, missing critical temporal and contextual signals.</p>

<p>The business impact of this limitation is substantial. Static recommendations become stale quickly, especially for platforms with frequent user activity. A video platform user who watches 10 videos daily has preferences that shift hour-by-hour based on mood, time of day, and recent viewing. Static recommendations might suggest content from last week's interests, reducing engagement. One major streaming platform found that incorporating sequence information improved watch time by 12\% and reduced churn by 8\%‚Äîtranslating to hundreds of millions in annual revenue.</p>

<p>Sequence-aware models address this by treating recommendations as a language modeling problem: given a user's historical sequence of interactions, predict the next item. This framing is powerful because it leverages decades of NLP research on sequence modeling. Just as language models predict the next word given previous words, recommendation models predict the next item given previous items. The key insight is that user behavior follows patterns‚Äîwatching a superhero movie increases the probability of watching another superhero movie, just as the word "New" increases the probability of "York."</p>

<div class="definition"><strong>Definition:</strong> 
Given a user's interaction sequence $(i_1, i_2, \ldots, i_t)$, a sequence model predicts the probability distribution over next items:
<div class="equation">
$$\begin{align}
P(i_{t+1} \mid i_1, \ldots, i_t) &= \text{softmax}(W \text{encoder}(i_1, \ldots, i_t) + b)
\end{align}$$
</div>
where <code>encoder</code> is a transformer, RNN, or other sequential model that processes the interaction history. The vocabulary is the set of all items $|I|$ (potentially millions), and the logits correspond to scores for each item. Items with higher scores are more likely to be the next interaction.
</div>

<p>This formulation enables the model to capture rich temporal patterns. If a user watches three action movies in a row, the model learns that action movies have high probability for the next interaction. If a user alternates between genres, the model learns that pattern too. The model can even capture long-range dependencies‚Äîa user who watched a TV show's first episode two weeks ago is likely interested in episode two, even if they watched other content in between.</p>

<p>Historically, RNNs (LSTM, GRU) were the standard for sequence modeling in recommendations. However, transformers with self-attention provide several critical advantages that translate directly to business value:</p>

<p><strong>Parallelization during training.</strong> RNNs process sequences sequentially, making training slow on long user histories. Transformers process entire sequences in parallel, reducing training time by 5-10x. For platforms retraining models daily on billions of interactions, this means the difference between 8-hour and 2-hour training jobs‚Äîenabling faster iteration and more frequent model updates.</p>

<p><strong>Long-range dependency modeling.</strong> RNNs struggle with dependencies spanning hundreds of steps due to vanishing gradients. Transformers' attention mechanism directly connects any two positions in the sequence, capturing long-range patterns. For users with thousands of historical interactions, this means better recommendations based on preferences from weeks or months ago, not just recent activity.</p>

<p><strong>Interpretability through attention weights.</strong> Attention weights show which past items influence the current recommendation. This interpretability helps debug model behavior, explain recommendations to users ("because you watched X"), and identify biases. For regulated industries or platforms facing scrutiny over algorithmic recommendations, this transparency is valuable.</p>

<p><strong>Multi-head attention for diverse patterns.</strong> Different attention heads can learn different temporal patterns‚Äîone head might focus on recent items (short-term preferences), another on items from the same genre (topical consistency), and another on seasonal patterns (holiday content). This diversity improves recommendation quality by capturing multiple aspects of user behavior simultaneously.</p>

<h3>SASRec: Self-Attentive Sequential Recommendation</h3>

<p>SASRec is a transformer-based recommender that achieves state-of-the-art performance on benchmark datasets. The architecture:</p>

<ol>
<li><strong>Embedding:</strong> Each item is embedded as $\mathbf{e}_i \in \mathbb{R}^d$. Optionally, add positional encodings to capture temporal positions.
<li><strong>Transformer layers:</strong> Stack L transformer encoder layers, each with multi-head self-attention and feed-forward networks.
<li><strong>Causal masking:</strong> Use causal attention mask to prevent the model from attending to future items (maintaining prediction task structure).
<li><strong>Output:</strong> The representation at position $t$ predicts the next item: 
<div class="equation">
$$\begin{align}
\text{logits}_i = \mathbf{h}_t^T \mathbf{e}_i + b_i
\end{align}$$
</div>
where $\mathbf{h}_t$ is the output of the transformer at step $t$.
<li><strong>Loss:</strong> Cross-entropy loss on the correct next item, computed at each position.
</ol>

<p>SASRec significantly outperforms RNN-based recommenders and matrix factorization on benchmark datasets (MovieLens, Amazon reviews, e-commerce), especially for longer user histories.</p>

<h3>Cold-Start Recommendations with Transformer-Capsule Networks (2024-2025)</h3>

<p>Cold-start recommendation‚Äîproviding quality recommendations for new users or new items with limited interaction history‚Äîhas been a persistent challenge in recommendation systems. Traditional approaches struggle because collaborative filtering requires sufficient interaction data to learn meaningful embeddings. Recent advances in 2024-2025 using transformer-capsule networks have achieved breakthrough performance on cold-start scenarios.</p>

<p><strong>Transformer-Capsule Graph (TCG-CS):</strong> The TCG-CS architecture combines transformers' sequence modeling capabilities with capsule networks' ability to capture hierarchical relationships and graph neural networks' structural reasoning. This hybrid approach achieves 94.2\% accuracy on cold-start recommendation tasks, significantly outperforming previous methods.</p>

<p><strong>Key innovations:</strong> Several breakthrough techniques enable TCG-CS to achieve superior cold-start performance.</p>

<p>Capsule-based user representation replaces single embedding vectors with capsules, which are groups of neurons that encode different aspects of preferences such as genre preferences, temporal patterns, and quality sensitivity. This structured representation enables learning from limited data by capturing the multifaceted nature of user preferences. Unlike flat embeddings that compress all preference information into a single vector, capsules maintain separate representations for different preference dimensions, making it easier to generalize from sparse interactions.</p>

<p>Graph-based item relationships model items as a graph where edges represent similarities such as same genre, same director, or co-purchased patterns. Graph neural networks propagate information from items with rich interaction history to cold-start items, enabling better initial embeddings for new items. This structural approach leverages the principle that similar items should have similar embeddings, allowing new items to benefit from the interaction history of related items even before receiving their own interactions.</p>

<p>Meta-learning for rapid adaptation uses techniques like Model-Agnostic Meta-Learning (MAML) to train models that can quickly adapt to new users with just 3-5 interactions. The model learns to learn, discovering which features and patterns are most informative for rapid personalization. This meta-learning approach is fundamentally different from standard training: instead of learning fixed parameters, the model learns initialization parameters that can be quickly fine-tuned to individual users with minimal data.</p>

<p>Content-collaborative hybrid approaches combine content features such as item metadata, descriptions, and images with collaborative signals from user-item interactions. For cold-start items, the system relies more heavily on content features to make initial recommendations. As interactions accumulate, the system gradually shifts weight to collaborative signals, which typically provide stronger personalization. This adaptive weighting ensures that recommendations remain relevant throughout an item's lifecycle, from launch through maturity.</p>

<p><strong>Business Impact:</strong> Cold-start is particularly critical for platforms with high user or item churn. E-commerce platforms add thousands of new products daily. Streaming platforms onboard millions of new users monthly. Poor cold-start recommendations cause early churn‚Äîusers who don't find relevant content in their first session are 3-5x more likely to abandon the platform.</p>

<p>Platforms implementing TCG-CS report substantial improvements across multiple metrics that directly impact business outcomes.</p>

<p>New user retention improves by 25-35\% when measured at day 7, meaning significantly more users continue using the platform after their first week. This improvement is critical because early retention is highly predictive of long-term user value. Users who remain engaged through the first week are 5-10x more likely to become long-term active users.</p>

<p>New item discovery improves by 40-50\%, measured as the fraction of new items that receive engagement within their first week. This metric is crucial for platforms with high content velocity, as it determines how quickly new content can find its audience. Better cold-start recommendations accelerate the discovery process, benefiting both users (who find relevant new content faster) and creators (whose content gains traction more quickly).</p>

<p>Time-to-personalization reduces by 15-20\%, meaning the system delivers personalized recommendations sooner in a user's lifecycle. Traditional systems might require weeks of interaction data before recommendations become truly personalized. TCG-CS achieves meaningful personalization within hours or days, dramatically improving the new user experience and reducing early churn.</p>

<p>Example: E-commerce platform with 1M new users monthly and 30\% day-7 retention. Improving retention by 5 percentage points (to 35\%) retains 50,000 additional users. At \$50 customer lifetime value, this generates \$2.5M monthly = \$30M annually. Development cost: \$500K. ROI: 60x.</p>

<p><strong>Implementation Considerations:</strong> TCG-CS is computationally more expensive than standard transformers‚Äîtraining requires 2-3x compute due to capsule routing and graph convolutions. However, the improved cold-start performance often justifies the cost for platforms where cold-start is a critical bottleneck. Open-source implementations are emerging in PyTorch and TensorFlow as of 2025.</p>

<h2>Feature Engineering and Behavior Language</h2>

<p>Recommendations depend on more than just item history. User demographics, item metadata, temporal context, and behavioral signals all influence next-item preferences.</p>

<h3>DSL for Behavior Data</h3>

<p>Recommendation systems process event streams: each user interaction is an event with timestamp, user ID, item ID, and contextual features. The ``language'' of recommendation is this event schema:</p>

<div class="definition"><strong>Definition:</strong> Each event in a user session is:
<div class="equation">
$$\begin{align}
\text{Event} = \{\text{user\_id}, \text{item\_id}, \text{timestamp}, \text{event\_type}, \text{context}\}
\end{align}$$
</div>
where several components define the structure of each interaction event. The <code>event\_type</code> field categorizes the interaction, taking values from the set \{view, click, purchase, add-to-cart, share, rate\}, with each type carrying different signals about user intent and engagement level. The <code>context</code> field captures situational information including device type, geographic location, search query if applicable, and other environmental factors that influence user behavior and preferences.
</div>

The sequence of events becomes the user's ``behavior language.'' A user's history might be:
<pre><code>
[{item: 42, type: view, time: 10:00},
 {item: 42, type: click, time: 10:05},
 {item: 87, type: view, time: 10:15},
 {item: 87, type: purchase, time: 10:25}]
</code></pre>

<p>The model learns that viewing+clicking an item increases the likelihood of purchase; views alone do not. Different event types carry different signals.</p>

<h3>Dense and Sparse Features</h3>

<p>Feature engineering bridges item history with demographic and contextual signals. Dense features provide continuous numerical representations that capture various aspects of users, items, and context.</p>

<p>User embeddings are learned during pre-training on user similarity tasks, capturing latent user characteristics and preferences in a continuous vector space. These embeddings encode patterns like which types of users tend to have similar preferences, enabling the model to generalize across similar users.</p>

<p>Item embeddings derive from product taxonomy or content embeddings, representing items in a semantic space where similar items are positioned nearby. These embeddings can be learned from item metadata, collaborative filtering patterns, or content features like text descriptions and images.</p>

<p>Temporal signals capture time-dependent patterns including time-of-day effects (users browse differently in morning vs. evening), day-of-week patterns (weekend vs. weekday behavior), and seasonality (holiday shopping, summer content preferences). These temporal features help the model adapt recommendations to the current context.</p>

<p>Historical aggregates summarize past behavior through statistics like the average rating a user gives (indicating rating strictness), the popularity of items the user likes (indicating mainstream vs. niche preferences), and engagement patterns (binge-watching vs. casual browsing). These aggregates provide stable signals about long-term user characteristics.</p>

<p>Sparse categorical features represent discrete attributes that take values from large vocabularies, requiring special handling through embedding layers.</p>

<p>User demographics include categorical attributes like age range, geographic location, and language preference. These features help the model understand how different user segments have different preferences and behaviors.</p>

<p>Item metadata captures categorical properties including category, subcategory, brand, and author. These attributes provide content-based signals that complement collaborative filtering, especially for cold-start items with limited interaction history.</p>

<p>Context features describe the interaction environment including device type (mobile, tablet, desktop), platform (app vs. web), and time zone. These contextual factors significantly influence user behavior and should be incorporated into recommendations.</p>

<p>Embedding these sparse features increases model capacity substantially. A categorical feature with 10,000 categories, such as user's home country, is embedded into a 16--32 dimensional vector. While this increases model parameters significantly, it enables the model to learn complex feature interactions and capture nuanced patterns that would be impossible with one-hot encoding.</p>

<h3>Multi-Task Learning for Recommendations</h3>

<p>Real recommendation systems optimize multiple objectives simultaneously, each capturing different aspects of user engagement and business value.</p>

<p>CTR (click-through rate) predicts whether the user will click on a recommended item, serving as a primary signal of immediate interest. High CTR indicates that recommendations are relevant and appealing enough to capture user attention. This metric is easy to measure and provides abundant training signal since every impression generates a click or no-click label.</p>

<p>Conversion predicts whether the user will complete a desired action such as making a purchase, subscribing, or completing a form. Conversion is a stronger signal than clicks because it represents actual business value, but it's also sparser‚Äîonly a small fraction of clicks convert. Optimizing for conversion ensures recommendations drive revenue, not just engagement.</p>

<p>Engagement time predicts how long the user will engage with the recommended item, measured as video watch time, article reading time, or time spent on a product page. This metric captures content quality and user satisfaction better than binary click signals. A user who watches an entire video is more satisfied than one who clicks but immediately leaves.</p>

<p>Long-term value predicts whether the recommendation will lead to sustained engagement and user retention over weeks or months. This forward-looking metric is crucial because optimizing only for immediate engagement can harm long-term retention through clickbait, filter bubbles, or content that satisfies curiosity but doesn't build lasting interest. Long-term value is difficult to measure (requires long observation windows) but essential for platform health.</p>

<p>Multi-task learning trains a shared backbone with task-specific heads. The losses are combined with weights:
<div class="equation">
$$\begin{align}
\text{Loss}_{\text{total}} = \lambda_{\text{ctr}} \text{Loss}_{\text{ctr}} + \lambda_{\text{conv}} \text{Loss}_{\text{conversion}} + \lambda_{\text{engagement}} \text{Loss}_{\text{engagement}}
\end{align}$$
</div>

<p>Task weights $\lambda$ are often tuned based on business priorities. A subscription platform may weight long-term engagement more heavily than short-term CTR.</p>

<h2>Real-Time Serving and Ranking</h2>

<p>Training a model is just the first step. Serving recommendations to billions of users in real-time is a massive engineering challenge.</p>

<h3>Two-Stage Architecture</h3>

<p>Most recommendation systems use a two-stage pipeline:</p>

<ol>
<li><strong>Candidate generation (retrieval):</strong> From millions of items, retrieve a small set of candidates (100--1,000) that are relevant to the user. This stage is fast and approximate; exact ranking over all items is infeasible.
<li><strong>Ranking:</strong> Score the candidate set with a more complex model. Return the top-k items to the user.
</ol>

<h3>Candidate Generation Strategies</h3>

<h3>Candidate Generation Strategies</h3>

<p>Candidate generation uses simple, fast methods to narrow the search space from millions of items to a manageable set for detailed ranking.</p>

<p>Embedding-based retrieval embeds both the user and all items into a shared vector space, then retrieves items nearest to the user embedding using efficient similarity search. This approach is fast, typically taking only milliseconds for MIPS (maximum inner product search) on GPU-accelerated indices like Faiss. The quality depends on the embedding space‚Äîwell-trained embeddings that capture semantic similarity produce relevant candidates.</p>

<p>Collaborative filtering identifies similar users based on their interaction patterns, then recommends items that those similar users liked. User similarity can be computed offline using techniques like cosine similarity on interaction vectors or learned embeddings. At serving time, the system retrieves candidates by finding similar users and aggregating their preferences. This approach is effective because users with similar past behavior often have similar future preferences.</p>

<p>Content-based filtering retrieves items similar to those the user has previously interacted with, using item features like category, tags, or content embeddings. This approach works well for users with consistent preferences and provides good coverage for new items with rich metadata. However, it can create filter bubbles by only recommending items similar to past interactions.</p>

<p>Hybrid approaches combine multiple candidate sources to improve both relevance and diversity. A typical hybrid strategy might retrieve top-k trending items (ensuring popular content is considered), personalized candidates from collaborative filtering (ensuring personalization), and content-based candidates (ensuring coverage of user interests). The diversity of sources improves coverage and serendipity, exposing users to a broader range of relevant content than any single method would provide.</p>

<h3>Ranking Model and Latency Budget</h3>

<p>The ranking model scores candidates. With a 200 ms latency budget for the entire recommendation request and 50 ms allocated to candidate generation, the ranker has 150 ms. This is enough for a small neural network (2--3 layers) but not a 24-layer transformer.</p>

<p>Practical rankers are often gradient-boosted trees (e.g., XGBoost, LightGBM) or shallow neural networks. They consume hundreds of features (user features, item features, candidate-specific features like co-occurrence with items the user has rated) and output a score. The top-k candidates by score are returned.</p>

<h3>Real-Time Updates and Freshness</h3>

<h3>Real-Time Updates and Freshness</h3>

<p>Recommendation scores must be updated as new items arrive and user preferences change. Naive approaches that recompute scores for all users at each step are prohibitively expensive. Practical approaches balance freshness with computational feasibility.</p>

<p>Batch serving precomputes recommendations for all users nightly, storing results in a cache for fast retrieval. The system serves recommendations from the cache with occasional refreshes for highly active users who might have significantly changed preferences since the last batch update. This approach minimizes serving latency and computational cost but sacrifices freshness‚Äîrecommendations can be up to 24 hours stale.</p>

<p>Online serving with feature caching computes expensive features like item embeddings and candidate sets offline, then fetches these precomputed features at request time to score with a fast model. This hybrid approach maintains reasonable freshness for features that change slowly (item embeddings) while enabling real-time scoring based on current user state. The fast scoring model can incorporate recent user actions without recomputing all features.</p>

<p>Streaming updates track user behavior in real-time using stream processing frameworks, updating embeddings and candidate sets incrementally as new interactions arrive. This approach provides the freshest recommendations but requires sophisticated infrastructure to handle high-throughput event streams and maintain consistency. Streaming updates are most valuable for highly active users whose preferences change rapidly within a session.</p>

<p>Freshness vs. latency is a trade-off. Highly personalized real-time recommendations are better but slower. Pre-computed recommendations are faster but stale.</p>

<h2>Fairness, Diversity, and Filter Bubbles</h2>

<p>Recommendation systems can amplify biases and trap users in filter bubbles---showing only content that aligns with past preferences, limiting exposure to diverse views.</p>

<h3>Filter Bubble Problem</h3>

<p>If a user watches many political videos from one perspective, a system optimizing for watch time may recommend only that perspective, reinforcing views and reducing exposure to alternative viewpoints. While maximizing engagement, this harms user growth and societal polarization.</p>

<p>Solutions include several complementary approaches to increase recommendation diversity and reduce filter bubble effects.</p>

<p>Diversity metrics measure the variety of recommendations using measures like entropy of recommended categories. By including diversity as an explicit term in the ranking objective, the system can balance relevance with variety. For example, the ranking score might be a weighted combination of predicted engagement and category diversity, ensuring that highly relevant but homogeneous recommendations don't dominate.</p>

<p>Exploration allocates a portion of recommendations (typically 10\%) to items outside the user's typical preferences, enabling discovery of new interests. This exploration can be random or guided by uncertainty estimates‚Äîrecommending items where the model is uncertain about user preferences. Exploration serves dual purposes: it helps users discover new content they might enjoy, and it generates training data for underexplored items and user segments.</p>

<p>Fairness constraints ensure that underrepresented creators and items receive exposure proportional to their quality, not just their historical popularity. Dynamic allocation algorithms balance personalization with fairness by setting minimum exposure thresholds for different creator demographics or content categories. This prevents winner-take-all dynamics where popular content dominates recommendations at the expense of quality niche content.</p>

<p>User control empowers users to adjust recommendation diversity or opt for curated feeds rather than purely algorithmic recommendations. Providing transparency about why items are recommended ("because you watched X") and controls to adjust recommendation behavior ("show me more diverse content") increases user trust and satisfaction. Users who feel in control of their experience are more likely to remain engaged long-term.</p>

<h3>Handling Demographic Bias</h3>

<h3>Handling Demographic Bias</h3>

<p>Models trained on interaction data inherit biases present in historical patterns. If women historically receive fewer views for technical content due to societal biases, the model may learn to downrank women creators in technical categories, perpetuating and amplifying the bias. Mitigation strategies address these biases at multiple stages of the recommendation pipeline.</p>

<p>Balanced datasets oversample interactions from underrepresented groups during training, ensuring the model sees sufficient examples of diverse creator-user interactions. For example, if women creators represent 20\% of technical content but receive only 10\% of views, oversampling their interactions to 20\% of training data helps the model learn unbiased quality signals. This rebalancing corrects for historical underexposure without requiring the model to explicitly reason about fairness.</p>

<p>Fairness-aware loss functions add a penalty term that discourages disparate performance across demographic groups. The loss function might include a term measuring the difference in average recommendation scores between demographic groups, encouraging the model to provide similar quality recommendations regardless of creator demographics. This approach directly optimizes for fairness as a training objective.</p>

<p>Allocation fairness ensures that a minimum fraction of recommendations go to creators from all demographics, regardless of predicted engagement. Post-processing algorithms can enforce these constraints by adjusting recommendation rankings to meet fairness targets while minimizing impact on overall engagement. For example, if the top-10 recommendations contain no creators from underrepresented groups, the system might replace the lowest-ranked item with the highest-ranked item from an underrepresented group.</p>

<h3>A/B Testing for Recommendation Changes</h3>

<p>Before deploying recommendation model improvements, validate with A/B tests that randomly split users into control and treatment groups.</p>

<p>The control group continues using the existing recommendation algorithm, providing a baseline for comparison. This group represents the status quo performance and helps account for external factors like seasonality or platform-wide changes that affect all users.</p>

<p>The treatment group receives recommendations from the new model, enabling direct measurement of the model's impact. The random assignment ensures that differences between groups are attributable to the model change rather than pre-existing user differences.</p>

<p>Measure metrics over 2--4 weeks to account for short-term novelty effects and capture longer-term impacts on user behavior.</p>

<p>Engagement metrics including watch time, clicks, and session length measure immediate user response to recommendations. These metrics are sensitive and provide quick feedback on whether the new model improves user experience. However, they can be gamed by clickbait or sensational content that attracts clicks but doesn't satisfy users.</p>

<p>Retention metrics measure the return rate, calculating the fraction of users who return the next day or week. Retention is a stronger signal of recommendation quality than engagement because it reflects sustained satisfaction. Users who receive consistently good recommendations are more likely to return, while poor recommendations drive churn.</p>

<p>Diversity metrics measure the entropy of categories recommended, ensuring that improvements in engagement don't come at the cost of reduced variety. A model that only recommends popular mainstream content might achieve high engagement but create filter bubbles and reduce long-term user satisfaction.</p>

<p>Fairness metrics track recommendation volume for different creator demographics, ensuring that model improvements don't disproportionately benefit or harm specific groups. Measuring fairness explicitly prevents unintended bias amplification and ensures the platform serves all creators equitably.</p>

<p>If the treatment significantly improves key metrics without degrading others, roll out to all users. This empirical validation is critical; models that perform well on offline metrics may hurt business KPIs in practice.</p>

<h2>Case Study: Video Recommendation for a Streaming Platform</h2>

<p>A video streaming platform with 100 million users and 10 million videos seeks to improve watch time and retention through better recommendations.</p>

<h3>System Architecture</h3>

<p><strong>Candidate Generation:</strong> The first stage rapidly narrows millions of videos to a manageable candidate set.</p>

<p>User embedding is pretrained using a siamese network on user-user similarity, where users who watch similar videos receive similar embeddings. This embedding captures latent user preferences and enables fast similarity-based retrieval.</p>

<p>Item embedding combines video embeddings from a collaborative filtering model (capturing which videos are watched together) with content embeddings from video metadata (title, description, tags, thumbnails). This hybrid embedding represents both collaborative patterns and content semantics.</p>

<p>Retrieval uses MIPS (maximum inner product search) with the Faiss library to find the top-500 candidates most similar to the user embedding in just 20 milliseconds. This fast approximate search makes real-time candidate generation feasible at scale.</p>

<p><strong>Ranking:</strong> The second stage scores candidates with a more sophisticated model to produce the final top-10 recommendations.</p>

<p>The model uses XGBoost, a gradient-boosted decision tree algorithm, with 500 features capturing user characteristics, item properties, and user-item interactions. XGBoost provides excellent performance with reasonable latency, making it suitable for real-time serving.</p>

<p>Features include user watch time patterns, historical CTR for similar content, video popularity metrics, recency (how recently uploaded), user-item co-occurrence (how often users like this user watch this video), and genre match between user preferences and video category. These diverse features enable the model to capture multiple signals of relevance.</p>

<p>Latency is constrained to 80 milliseconds to score all 500 candidates and rank the top-10, fitting within the overall 200ms latency budget for the recommendation request. This tight latency constraint requires careful model optimization and efficient feature computation.</p>

<p><strong>Post-processing:</strong> The final stage applies business rules and fairness constraints to the ranked list.</p>

<p>A diversity filter ensures variety by checking if the top-10 contains too many videos from the same genre (e.g., 5 action movies). If so, the system reranks to ensure a maximum of 3 videos per genre, promoting exploration of different content types.</p>

<p>A freshness boost increases the score of videos uploaded less than 1 week ago, ensuring that new content receives exposure even if it hasn't accumulated many interactions yet. This helps new creators gain traction and keeps the platform feeling current.</p>

<p>Creator fairness constraints ensure the top-10 includes creators from different regions and demographics, preventing the recommendations from being dominated by a small set of popular creators. This promotes platform diversity and gives all creators fair opportunity for exposure.</p>

<h3>Training and Offline Evaluation</h3>

<p><strong>Data:</strong> 100 billion historical events (views, clicks, shares) over 3 months</p>

<p><strong>Metrics:</strong> The system is evaluated using standard recommendation metrics that measure both coverage and ranking quality.</p>

<p>Recall@10 measures what fraction of videos the user actually watched next appear in the top-10 recommendations. This metric captures whether the system successfully identifies relevant content, with a target of at least 8\%. Higher recall means users are more likely to find content they want in the recommendations.</p>

<p>NDCG@10 (Normalized Discounted Cumulative Gain) measures ranking quality by assigning higher value to relevant items that appear earlier in the list. Videos that users clicked or watched receive higher scores, and the metric penalizes relevant items that appear lower in the ranking. The target is at least 0.45, indicating that relevant content consistently appears near the top of recommendations.</p>

<p><strong>Results:</strong> Progressive model improvements demonstrate clear gains in recommendation quality.</p>

<p>The collaborative filtering baseline achieves NDCG@10 of 0.38, providing a strong foundation but missing temporal patterns in user behavior. This traditional approach serves as the performance floor that newer methods must exceed.</p>

<p>The sequence model (SASRec) improves to NDCG@10 of 0.43, a 13\% improvement over the baseline. By modeling user interaction sequences with transformers, the system captures temporal dynamics and session-level patterns that collaborative filtering misses.</p>

<p>SASRec with multi-task learning reaches NDCG@10 of 0.46, adding another 7\% improvement. By jointly optimizing for multiple objectives (CTR, watch time, retention), the model learns richer representations that better predict user satisfaction across different dimensions.</p>

<h3>Online A/B Test</h3>

<p>Deploy the improved recommender to 10\% of users.</p>

<p><strong>Results over 4 weeks:</strong> The online test validates that offline improvements translate to real business impact.</p>

<p>Watch time increases by 4.2\%, a statistically significant improvement indicating that users engage more deeply with the improved recommendations. This translates to millions of additional hours of content consumption, directly impacting platform value and advertising revenue.</p>

<p>CTR on recommendations improves by 3.8\%, showing that users find the recommendations more appealing and relevant. Higher CTR indicates better matching between recommended content and user interests.</p>

<p>Session length increases by 2.1\%, meaning users explore more content per visit. This suggests that good recommendations create positive momentum, encouraging users to continue browsing and watching rather than leaving the platform.</p>

<p>Retention (30-day) improves by 1.5\%, with more users returning to the platform over the following month. This long-term metric is crucial because it indicates sustained satisfaction rather than just short-term engagement spikes. Improved retention has compounding effects on platform growth and user lifetime value.</p>

<p>Diversity increases by 8\%, measured as entropy of recommended categories. Users explore new genres and content types rather than staying in narrow filter bubbles. This diversity benefits both users (who discover new interests) and creators (who reach broader audiences).</p>

<p>Creator fairness improves with a 12\% increase in watch share for underrepresented creators, rising from 5\% to 5.6\% of total watch time. While seemingly small in absolute terms, this represents thousands of additional creators receiving meaningful exposure and represents progress toward equitable content distribution.</p>

<p>No negative impact on fairness or diversity metrics. The model improved both engagement and societal goals. Deployment proceeds to all users, expected to recover millions of hours of additional user engagement annually.</p>

<h2>Model Maintenance and Drift in Recommendation Systems</h2>

<p>Recommendation systems face some of the most severe drift challenges of any machine learning application. User preferences evolve constantly‚Äîdaily, weekly, and seasonally. New items arrive continuously, creating cold-start problems. Content trends shift rapidly, especially on social platforms. External events (holidays, news, cultural moments) dramatically change consumption patterns. A recommendation model trained on last month's data can become obsolete within weeks, causing measurable degradation in engagement and revenue.</p>

<p>The business stakes are enormous. A 1\% drop in recommendation quality can cost large platforms millions of dollars monthly in lost engagement and advertising revenue. One major video platform observed a 5\% decline in watch time over three months due to undetected model drift, costing an estimated \$30 million in revenue before the issue was identified and corrected. Effective drift management is not optional‚Äîit's essential for maintaining competitive advantage and business performance.</p>

<h3>Domain-Specific Drift Patterns in Recommendations</h3>

<p>Recommendation drift manifests in several distinct ways, each requiring different detection and mitigation strategies:</p>

<p><strong>User preference drift.</strong> Individual users' tastes evolve over time. A user interested in action movies in January may shift to documentaries in March. A user who primarily shopped for electronics may start shopping for baby products (life event). A user's music preferences may broaden as they discover new genres. This drift is gradual but pervasive‚Äîstudies show 20-40\% of users exhibit significant preference changes over 3-6 months.</p>

<p>The business impact is direct: recommendations based on outdated preferences reduce engagement. A user who has moved on from action movies but continues receiving action recommendations will have lower click-through rates and watch time. At scale, even small per-user impacts compound to significant revenue losses.</p>

<p><strong>Item cold-start and freshness drift.</strong> New items arrive constantly‚Äînew videos uploaded, new products listed, new songs released. These items have no interaction history, making them difficult to recommend (cold-start problem). However, users often prefer fresh content over older content, even if older content has better historical engagement. A recommendation system that doesn't adapt to new items will feel stale, reducing user satisfaction.</p>

<p>The challenge is particularly acute for platforms with high content velocity. A short-form video platform might receive millions of new videos daily. A news platform's content becomes stale within hours. Recommendation systems must balance exploiting known good content (high engagement) with exploring new content (freshness, discovery).</p>

<p><strong>Seasonal and event-driven drift.</strong> Consumption patterns exhibit strong seasonal patterns. Holiday shopping peaks in November-December. Sports content surges during major events. Back-to-school shopping spikes in August. Music preferences shift with seasons (upbeat in summer, mellow in winter). These patterns are predictable but must be incorporated into models.</p>

<p>External events create unpredictable drift. A viral trend, breaking news, or cultural moment can shift consumption patterns overnight. During the COVID-19 pandemic, content consumption patterns changed dramatically‚Äîmore home workout videos, cooking content, and educational material. Models trained on pre-pandemic data performed poorly without rapid adaptation.</p>

<p><strong>Popularity and trend drift.</strong> Item popularity is highly dynamic. A video that goes viral sees 1000x increase in views within days. A product featured in a celebrity post sees massive demand spikes. A song that becomes a meme dominates listening. Recommendation systems must adapt to these popularity shifts to remain relevant.</p>

<p>However, over-emphasizing popularity creates problems. Recommending only trending content reduces personalization and creates winner-take-all dynamics that harm content diversity and creator fairness. Balancing popularity signals with personalization is a key challenge.</p>

<p><strong>Behavioral pattern drift.</strong> How users interact with platforms evolves. Mobile usage patterns differ from desktop. Short-form video platforms train users to expect rapid content switching. Binge-watching behavior on streaming platforms creates different engagement patterns than episodic viewing. As platform features evolve (new UI, new content formats), user behavior adapts, and recommendation models must follow.</p>

<p><strong>Cross-platform and cross-device drift.</strong> Users increasingly interact with platforms across multiple devices (phone, tablet, desktop, TV) and contexts (commute, home, work). Preferences and engagement patterns differ by device and context. A user might watch short clips on mobile during commute but long-form content on TV at home. Models must adapt to these context-dependent patterns.</p>

<div class="keypoint">
For the generic drift detection and continuous learning framework, see Chapter~[ref], Section~[ref]. Recommendation systems typically require the most aggressive retraining cadence (daily to weekly) due to the rapid pace of preference and content evolution.
</div>

<p>Key recommendation-specific strategies beyond the generic framework include:
<ul>
<li><strong>Separate short-term and long-term models:</strong> Use a transformer on recent interactions for ``what the user wants right now'' and a collaborative filter on full history for ``what the user generally likes,'' combining with learned weights.
<li><strong>Contextual bandits for exploration:</strong> Allocate 5--10\% of recommendations to exploration via Thompson sampling or UCB, enabling continuous learning about new items and shifting preferences.
<li><strong>Seasonal and event-aware modeling:</strong> Explicitly include temporal features (day-of-week, holiday indicators, event flags) and train separate model components for predictable seasonal shifts.
<li><strong>Online user embedding updates:</strong> Update user embeddings hourly based on recent interactions while keeping item embeddings on a daily cycle, balancing freshness with stability.
</ul>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> Implement a simple sequence-based recommender using a 2-layer transformer encoder. Train on MovieLens-1M. Evaluate using Recall@20 and NDCG@20 metrics. How does performance compare to a baseline RNN-based recommender?
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Design a multi-task recommendation system that predicts both click-through rate (CTR) and conversion rate (CVR). What is the relationship between the two tasks? Should they share weights or have separate heads? How would you weight the two losses?
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> Analyze the filter bubble effect in a recommendation system. Given historical user interactions, recommend items and measure recommendation diversity. Propose modifications to increase diversity while maintaining engagement.
</div>

<h2>Solutions</h2>

<p>Full solutions for all exercises are available at \url{https://deeplearning.hofkensvermeulen.be}.</p>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 1: Sequence-Based Recommender</strong>

<p><em>Architecture:</em>
<ul>
<li>Item embedding dimension: 64
<li>Transformer: 2 layers, 4 attention heads, FFN hidden dim = 256
<li>Causal masking: prevent attending to future items
<li>Loss: Cross-entropy on next item (max sequence length = 20)
</ul>

<p><em>Training (MovieLens-1M):</em>
<ul>
<li>Prepare sequences of movie ratings. Threshold rating > 3 as positive interactions.
<li>Filter users with $\geq$ 5 interactions; obtain 650,000 user sequences.
<li>Train/val split: 80/20
<li>Batch size: 128, learning rate: $1 \times 10^{-3}$, epochs: 20
</ul>

<p><em>Results:</em>
<ul>
<li>Transformer Recall@20: 0.52, NDCG@20: 0.38
<li>RNN baseline (GRU-128): Recall@20: 0.48, NDCG@20: 0.35
<li>Transformer is 8\% better on recall, 9\% better on NDCG
<li>Inference latency: 15 ms per user (transformer) vs. 8 ms (RNN); trade-off acceptable
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 2: Multi-Task Learning for CTR and CVR</strong>

<p><em>Task Relationship:</em>
CTR (click) and CVR (conversion/purchase) are sequential: a user must click before converting. Correlation is high (users who click are more likely to convert), but causality is clear: click enables conversion.</p>

<p><em>Architecture:</em>
<ul>
<li>Shared backbone: embedding layer + 2 fully-connected hidden layers (256 -> 128)
<li>Task-specific heads:
  <ul>
  <li>CTR head: 1 dense layer -> sigmoid -> CTR probability
  <li>CVR head: 1 dense layer -> sigmoid -> CVR probability
  </ul>
<li>Note: CVR is computed on clicked items only (conditional probability)
</ul>

<p><em>Loss weighting:</em>
<div class="equation">
$$\begin{align}
\text{Loss} = \lambda_{\text{ctr}} \text{BCE}(\hat{y}_{\text{ctr}}, y_{\text{ctr}}) + \lambda_{\text{cvr}} \text{BCE}(\hat{y}_{\text{cvr}}, y_{\text{cvr}})
\end{align}$$
</div>

<p>Suggest $\lambda_{\text{ctr}} = 0.7, \lambda_{\text{cvr}} = 0.3$ since CTR is more frequent and diverse.</p>

<p><em>Alternative: CTR as auxiliary task:</em>
Some systems use CTR as an auxiliary task to regularize the CVR model, since more training signal is available for CTR. This improves CVR generalization.</p>

<p>\itshape Results (on dataset with 10\% CTR, 2\% CVR):
<ul>
<li>Standalone CVR model: AUC = 0.72
<li>Multi-task (CTR + CVR): AUC = 0.78 (8\% improvement)
<li>Improvement from auxiliary task signal and shared representation learning
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 3: Filter Bubble Analysis</strong>

<p><em>Diversity Measurement:</em>
For a user with recommendation sequence, compute entropy of recommended categories:
<div class="equation">
$$\begin{align}
\text{Diversity} = -\sum_c p_c \log p_c
\end{align}$$
</div>
where $p_c$ is the fraction of recommendations in category $c$.</p>

<p><em>Analysis:</em>
<ul>
<li>Baseline recommender: Average diversity = 1.2 bits (low; most recommendations in user's primary interests)
<li>Improvement proposal: Allocate 10\% of top-10 recommendations for exploration (diverse categories)
<li>Result: Diversity increases to 1.8 bits (+50\%)
<li>Engagement impact: Watch time on exploration recommendations is 30\% lower but leads to user growth (10\% increase in diversity preference)
</ul>

<p>\itshape Implementation:
Re-rank top-20 candidates to ensure diversity:
<ol>
<li>Sort by engagement score
<li>Greedily select top-10 while maintaining max 3 items per category
<li>For each selected item, remove other items from the same category to increase diversity
</ol>

<p>This increases diversity with minimal engagement loss ($< 0.5\%$) and aligns system with user long-term interests.
</div>
        
        <div class="chapter-nav">
  <a href="chapter28_knowledge_graphs.html">‚Üê Chapter 28: Knowledge Graphs and Reasoning</a>
  <a href="../../deeptech.html">üìö Table of Contents</a>
  <a href="chapter30_healthcare.html">Chapter 30: Healthcare Applications ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
