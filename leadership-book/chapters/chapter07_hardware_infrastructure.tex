\chapter{Hardware and Infrastructure}

\section*{Why This Matters}

Infrastructure decisions—GPU selection, cloud versus on-premise deployment, memory configuration—directly determine project feasibility, operational costs, and system performance. A single architectural choice can shift project costs by 10× or make certain capabilities entirely infeasible. Understanding hardware characteristics and their relationship to workload requirements is essential for accurate cost forecasting, infrastructure planning, and vendor evaluation.

GPU architecture fundamentally shapes what's possible and at what cost. Memory bandwidth limits determine whether operations run at peak efficiency or waste computational capacity. The distinction between compute-bound and memory-bound operations explains why some models achieve 80\% of theoretical performance while others reach only 15\%. These relationships aren't abstract—they translate directly to infrastructure costs, training times, and operational expenses.

This chapter examines hardware from an engineering and economic perspective, focusing on the characteristics that determine system performance, the trade-offs between deployment options, and frameworks for making informed infrastructure decisions.

\section{GPU Architecture}

\subsection{Memory Hierarchy}

GPU memory operates in a hierarchy, with each level offering different capacity, bandwidth, and latency characteristics. Understanding this hierarchy is essential for evaluating performance claims and identifying optimization opportunities.

\textbf{Registers}: Fastest storage, located directly in compute units. A100 provides 256 KB per streaming multiprocessor (SM), accessible in less than one cycle. Compilers automatically manage register allocation—developers rarely interact with this level directly, but register pressure can limit parallelism.

\textbf{L1 Cache and Shared Memory}: 128 KB per SM on A100, accessible in approximately 28 cycles. Shared memory is explicitly managed by developers for performance-critical operations. Effective use of shared memory can improve performance by 2-5× for memory-intensive operations.

\textbf{L2 Cache}: 40 MB shared across all SMs on A100, approximately 200 cycles latency. Automatically managed by hardware. Effective for data reuse across different parts of the computation.

\textbf{HBM2 (High Bandwidth Memory)}: Main GPU memory, 40-80 GB capacity on A100. Latency approximately 350 cycles, but high bandwidth (1.5-2 TB/s) enables parallel access. This is where model parameters, activations, and gradients reside during training.

The performance implication: operations that fit in faster memory levels achieve higher throughput. A computation requiring 10 GB of working memory cannot leverage L2 cache (40 MB) and must access HBM2, limiting performance to memory bandwidth rather than compute capacity.

\subsection{Computational Units}

Modern GPUs contain multiple types of computational units, each optimized for different operations.

\textbf{CUDA Cores}: General-purpose floating-point units. A100 contains 6,912 CUDA cores delivering 19.5 TFLOPS at FP32 precision. These handle general computation but are not optimized for the matrix operations central to deep learning.

\textbf{Tensor Cores}: Specialized units for matrix multiply-accumulate operations—the fundamental operation in neural networks. A100 contains 432 Tensor Cores delivering 312 TFLOPS at FP16 precision and 156 TFLOPS at TF32 (TensorFloat-32) precision. This 16-20× advantage over CUDA cores makes Tensor Cores essential for efficient training.

\textbf{Streaming Multiprocessors (SMs)}: Organizational units containing CUDA cores, Tensor Cores, memory, and scheduling logic. A100 has 108 SMs. Effective GPU utilization requires keeping all SMs busy—underutilization directly reduces performance.

The practical implication: transformer training primarily uses Tensor Cores for matrix operations (attention, feed-forward layers) and CUDA cores for element-wise operations (activation functions, normalization). Peak performance requires operations that fully utilize Tensor Cores.

\subsection{Memory Bandwidth}

Bandwidth—the rate at which data moves between memory levels—often determines actual performance more than peak computational capacity.

\textbf{HBM2 Bandwidth}: 1,555 GB/s (40 GB A100) or 2,039 GB/s (80 GB A100). This represents the maximum rate for moving data between main memory and compute units.

\textbf{NVLink}: 600 GB/s for GPU-to-GPU communication within a node. Essential for multi-GPU training where gradients and activations must be shared across GPUs.

\textbf{PCIe 4.0}: 64 GB/s for CPU-GPU communication. Sufficient for loading model weights and data but too slow for frequent CPU-GPU data movement during training.

The critical relationship: many operations are bandwidth-limited rather than compute-limited. An operation requiring 1 TB of data movement but only 100 TFLOPS of computation cannot exceed 1 TB ÷ 2 TB/s = 0.5 seconds, regardless of computational capacity. This explains why some operations achieve only 15-20\% of peak TFLOPS.

\section{Compute-Bound versus Memory-Bound Operations}

\subsection{Roofline Model}

The roofline model provides a framework for understanding performance limits. Achieved performance is constrained by either peak compute capacity or memory bandwidth:

\begin{center}
\fbox{\parbox{0.85\textwidth}{
\centering
Achieved Performance = min(Peak Compute, Bandwidth × Arithmetic Intensity)
}}
\end{center}

Arithmetic Intensity (AI) = FLOPs / Bytes Transferred

Operations with high arithmetic intensity (many computations per byte of data) are compute-bound and can approach peak TFLOPS. Operations with low arithmetic intensity are memory-bound and achieve only a fraction of peak performance.

\subsection{Transformer Operations Analysis}

Different transformer operations exhibit different arithmetic intensities and performance characteristics:

\textbf{Large Linear Projections}: Matrix multiplication with dimensions like 4096×4096 achieves AI $\approx$ 256 FLOP/byte. This is compute-bound on A100, reaching approximately 280 TFLOPS (90\% of peak FP16 Tensor Core performance).

\textbf{Small Linear Projections}: Smaller matrices (512×512) achieve AI $\approx$ 32 FLOP/byte. Memory-bound, reaching only approximately 50 TFLOPS (16\% of peak).

\textbf{Attention Computation}: $QK^T$ multiplication for sequence length 512 achieves AI $\approx$ 200 FLOP/byte—borderline compute-bound, reaching approximately 220 TFLOPS. For sequence length 2048, AI drops to approximately 50 FLOP/byte—memory-bound, reaching only approximately 78 TFLOPS.

\textbf{Softmax and Normalization}: AI $\approx$ 1-2.5 FLOP/byte. Heavily memory-bound, achieving only 2-4 TFLOPS (1-2\% of peak).

The key insight: long-context attention becomes memory-bound because memory requirements grow as O(n²) while computation grows as O(n²d). For large n, memory bandwidth limits performance regardless of computational capacity.

\subsection{Optimization Implications}

Understanding compute versus memory bounds informs optimization strategies:

\textbf{Compute-Bound Operations}: Benefit from higher precision (FP32 vs FP16) with minimal performance impact. Optimization focuses on maximizing Tensor Core utilization through proper matrix dimensions and batch sizes.

\textbf{Memory-Bound Operations}: Precision reduction (FP16 vs FP32) provides 2× speedup by halving data movement. Optimization focuses on data reuse, kernel fusion (combining multiple operations to reduce memory traffic), and efficient memory access patterns.

For transformer training, approximately 60-70\% of time is spent in compute-bound operations (large matrix multiplications) and 30-40\% in memory-bound operations (attention, normalization, element-wise operations). This explains why mixed-precision training (FP16 for compute-bound, FP32 for stability) provides substantial benefits.

\section{Infrastructure Deployment Considerations}

\subsection{Deployment Model Trade-offs}

Infrastructure deployment decisions involve complex trade-offs between cost, flexibility, control, and operational complexity. The choice between cloud, on-premise, or hybrid deployment depends on workload characteristics, organizational capabilities, and strategic priorities rather than simple cost calculations.

\textbf{Cloud Deployment}:
\begin{itemize}
    \item Elastic scaling: Add or remove capacity based on demand
    \item Access to latest hardware: New GPU generations available immediately
    \item Operational simplicity: No hardware management, maintenance, or facilities
    \item Variable costs: Pay for actual usage, no capital expenditure
    \item Geographic distribution: Deploy close to users or data sources
    \item Trade-off: Higher per-hour costs, less control over infrastructure
\end{itemize}

\textbf{On-Premise Deployment}:
\begin{itemize}
    \item Capital investment: Significant upfront hardware costs
    \item Operational overhead: Requires infrastructure team, facilities, power, cooling
    \item Hardware lifecycle: 3-5 year useful life before obsolescence
    \item Data sovereignty: Complete control over data location and access
    \item Network performance: Low-latency access to internal data sources
    \item Trade-off: Fixed capacity, slower to scale, operational complexity
\end{itemize}

\textbf{Hybrid Deployment}:
\begin{itemize}
    \item Base capacity on-premise for steady workloads
    \item Cloud burst capacity for variable or peak demands
    \item Data-sensitive workloads on-premise, others in cloud
    \item Complexity: Requires managing both environments
\end{itemize}

\subsection{Decision Factors}

\textbf{Workload Characteristics}:
\begin{itemize}
    \item Steady, predictable training workloads may justify on-premise investment
    \item Highly variable research workloads benefit from cloud elasticity
    \item Inference workloads with strict latency requirements may require specific deployment
    \item Batch processing can leverage spot instances for cost optimization
\end{itemize}

\textbf{Organizational Capabilities}:
\begin{itemize}
    \item Infrastructure expertise: On-premise requires dedicated team
    \item Capital availability: Cloud avoids large upfront investments
    \item Operational maturity: Managing GPU clusters requires specialized skills
    \item Scale: Small teams benefit from cloud's operational simplicity
\end{itemize}

\textbf{Data and Compliance}:
\begin{itemize}
    \item Data sensitivity and regulatory requirements may mandate on-premise
    \item Data transfer costs and bandwidth affect cloud economics
    \item Geographic restrictions may limit cloud provider options
    \item Audit and compliance requirements vary by deployment model
\end{itemize}

\textbf{Technology Evolution}:
\begin{itemize}
    \item GPU performance improves 2-3× every 2 years
    \item On-premise hardware depreciates over 3-5 year lifecycle
    \item Cloud provides immediate access to new generations
    \item Consider opportunity cost of locked capital in depreciating hardware
\end{itemize}

The deployment decision is strategic rather than purely financial. Organizations should evaluate based on their specific workload patterns, capabilities, and requirements rather than generic cost models.

\section{GPU Selection and Configuration}

\subsection{GPU Comparison}

Different GPU models offer different performance, memory, and cost characteristics that determine their suitability for specific workloads.

The NVIDIA A100 with 80GB memory provides 80 GB HBM2, 312 TFLOPS at FP16 precision (156 TFLOPS at TF32), and 2,039 GB/s memory bandwidth. At approximately \$15,000-20,000 per unit, it serves as the workhorse for large model training and production inference where memory capacity is critical.

The 40GB variant of the A100 offers identical compute performance—312 TFLOPS FP16 and 156 TFLOPS TF32—but with 40 GB HBM2 and 1,555 GB/s bandwidth. Priced at approximately \$10,000-12,000, it provides a cost-effective option for medium model training and deployments where memory requirements are less demanding.

The NVIDIA H100 represents the current generation flagship, delivering 80 GB HBM3, 989 TFLOPS at FP16 (495 TFLOPS at TF32), and 3,350 GB/s bandwidth. At \$30,000-40,000, it targets large-scale training workloads and applications where maximum performance justifies the premium cost.

\textbf{2026 GPU Landscape:} H100 remains the standard for large-scale training in 2026, with B200/GB200 (Blackwell architecture) emerging for the largest frontier model training. A100 continues to be widely deployed for inference and medium-scale training, now available at significantly reduced prices (\$8,000-12,000 for 40GB, \$12,000-16,000 for 80GB) as organizations upgrade to H100. Cloud spot pricing has decreased: A100 now \$1.80-2.20/hour, H100 \$3.50-4.50/hour.

For inference-focused deployments, the NVIDIA L4 offers 24 GB GDDR6, 242 TFLOPS at FP16, and 300 GB/s bandwidth at approximately \$3,000-5,000. This cost-optimized configuration suits inference workloads, fine-tuning, and training scenarios where memory and compute requirements are moderate.

\subsection{Memory Requirements}

GPU memory determines maximum model size and batch size, making it a hard constraint that can render training infeasible regardless of computational capacity. Understanding memory requirements is essential for hardware selection and capacity planning.

For training, memory requirements accumulate across multiple components. Model parameters stored in FP16 precision require 2 bytes per parameter. Gradients, computed during backpropagation, require another 2 bytes per parameter. Optimizer states for Adam consume 8-12 bytes per parameter, maintaining momentum and variance estimates. Activations vary by batch size and sequence length, often dominating memory consumption. The total reaches approximately 16-20 bytes per parameter plus activations.

Consider BERT-base with 110 million parameters, batch size 32, and sequence length 512. Parameters and gradients consume 0.44 GB. Optimizer states require 1.32 GB. Activations consume approximately 8 GB. The total reaches approximately 10 GB—comfortably fitting on any modern GPU.

Scale this to GPT-3 with 175 billion parameters, and the picture changes dramatically. Parameters and gradients require 700 GB. Optimizer states demand 2,100 GB. Activations add 100+ GB per GPU. The total exceeds 3 TB, making distributed training across many GPUs not just beneficial but absolutely necessary.

\subsection{Multi-GPU Configuration}

Large models require multiple GPUs. Configuration choices affect performance and cost:

\textbf{Single-Node Multi-GPU} (8 GPUs per server):
\begin{itemize}
    \item Communication: NVLink (600 GB/s)
    \item Latency: Low (microseconds)
    \item Cost: \$150,000-300,000 per node
    \item Use case: Models up to approximately 50B parameters
\end{itemize}

\textbf{Multi-Node} (multiple servers):
\begin{itemize}
    \item Communication: InfiniBand (200-400 Gb/s) or Ethernet (100-400 Gb/s)
    \item Latency: Higher (milliseconds)
    \item Cost: Scales linearly with nodes plus networking
    \item Use case: Models exceeding 50B parameters
\end{itemize}

Network bandwidth becomes critical for multi-node training. Insufficient bandwidth creates communication bottlenecks, reducing GPU utilization and increasing training time. For GPT-3 scale training, network costs can exceed \$1M for high-performance InfiniBand fabric.

\section{Key Insights}

\textbf{Memory Hierarchy Determines Performance}: Understanding the GPU memory hierarchy—registers, caches, shared memory, HBM—is essential for evaluating performance claims and identifying optimization opportunities. Operations that fit in faster memory achieve higher throughput.

\textbf{Compute versus Memory Bounds}: Many operations are memory-bound rather than compute-bound. Arithmetic intensity (FLOPs per byte) determines whether an operation can approach peak performance. Long-context attention is memory-bound, explaining why it doesn't benefit from more powerful GPUs.

\textbf{Deployment Trade-offs}: Infrastructure deployment involves complex trade-offs between cost, flexibility, control, and operational complexity. The choice depends on workload characteristics, organizational capabilities, and strategic priorities rather than simple cost calculations.

\textbf{Memory Limits Feasibility}: GPU memory determines maximum model size and batch size. Insufficient memory makes training infeasible regardless of computational capacity. Memory requirements grow as 16-20 bytes per parameter plus activations.

\textbf{Network Bandwidth for Scale}: Multi-node training requires high-bandwidth networking (InfiniBand or high-speed Ethernet). Network costs can exceed hardware costs for large-scale deployments. Insufficient bandwidth creates communication bottlenecks.

\textbf{Technology Evolution}: GPU performance improves 2-3× every two years. On-premise hardware becomes obsolete faster than its useful life. Cloud provides access to latest hardware without capital investment, but at higher operational cost.

This completes Part III's hardware foundation. The next chapter examines data pipelines and training systems—the software infrastructure that determines training efficiency, data quality, and operational reliability.
