<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Linear Algebra for Deep Learning - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../css/style.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                vh: '{\\mathbf{h}}',
                vw: '{\\mathbf{w}}',
                vb: '{\\mathbf{b}}',
                vq: '{\\mathbf{q}}',
                vk: '{\\mathbf{k}}',
                vv: '{\\mathbf{v}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mQ: '{\\mathbf{Q}}',
                mK: '{\\mathbf{K}}',
                mV: '{\\mathbf{V}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mU: '{\\mathbf{U}}',
                mM: '{\\mathbf{M}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
    </nav>

    <main>
        <h1>Linear Algebra for Deep Learning</h1>

<h2>Chapter Overview</h2>

<p>Linear algebra forms the mathematical foundation of deep learning. Neural networks perform sequences of linear transformations interspersed with nonlinear operations, making matrices and vectors the fundamental objects of study. This chapter develops the linear algebra concepts essential for understanding how deep learning models transform data, how information flows through neural architectures, and how we can interpret the geometric operations these models perform.</p>

<p>Unlike a pure mathematics course, our treatment emphasizes the specific linear algebra operations that appear repeatedly in deep learning: matrix multiplication for transforming representations, dot products for measuring similarity, and matrix decompositions for understanding structure. We pay particular attention to dimensions and shapes, as tracking how tensor dimensions transform through operations is crucial for implementing and debugging deep learning systems.</p>

<h3>Learning Objectives</h3>

<p>After completing this chapter, you will be able to:</p>

<ol>
    <li>Represent data as vectors and transformations as matrices with clear understanding of dimensions
    <li>Perform matrix operations and understand their geometric interpretations
    <li>Calculate and interpret dot products as similarity measures
    <li>Understand eigendecompositions and singular value decompositions and their applications
    <li>Apply matrix norms and use them in regularization
    <li>Recognize how linear algebra operations map to neural network computations
</ol>

<h2>Vector Spaces and Transformations</h2>

<h3>Vectors as Data Representations</h3>

<p>In deep learning, we represent data as vectors in high-dimensional spaces. A vector $\vx \in \R^n$ is an ordered collection of $n$ real numbers, which we can interpret geometrically as a point in $n$-dimensional space or as an arrow from the origin to that point.</p>

<div class="definition"><strong>Definition:</strong> 
A vector $\vx \in \R^n$ is an $n$-tuple of real numbers:
<div class="equation">
$$
\vx = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
$$
</div>
where each $x_i \in \R$ is called a component or element of the vector.
</div>

<p>The dimension $n$ is the number of components in the vector. We write vectors as column vectors by default.</p>

<div class="example"><strong>Example:</strong> 
Consider a grayscale image of size $28 \times 28$ pixels, such as an image from the MNIST handwritten digit dataset. Each pixel has an intensity value between 0 (black) and 255 (white). We can represent this image as a vector $\vx \in \R^{784}$ by concatenating all pixel values:
<div class="equation">
$$
\vx = \begin{bmatrix} x_{1,1} \\ x_{1,2} \\ \vdots \\ x_{28,28} \end{bmatrix} \in \R^{784}
$$
</div>

<p>For color images with three channels (red, green, blue), a $224 \times 224$ RGB image becomes a vector in $\R^{150528}$ ($224 \times 224 \times 3 = 150{,}528$). The enormous dimensionality of image data motivates the need for powerful models that can find meaningful patterns in such high-dimensional spaces.
</div>

<div class="example"><strong>Example:</strong> 
In natural language processing, we represent words as vectors called <em>word embeddings</em>. A common choice is to represent each word as a vector in $\R^{300}$ or $\R^{768}$. For instance, the word ``king'' might be represented as:
<div class="equation">
$$
\vw_{\text{king}} = \begin{bmatrix} 0.23 \\ -0.45 \\ 0.87 \\ \vdots \\ 0.12 \end{bmatrix} \in \R^{300}
$$
</div>
These embeddings are learned such that semantically similar words have similar vector representations. The famous example is that $\vw_{\text{king}} - \vw_{\text{man}} + \vw_{\text{woman}} \approx \vw_{\text{queen}}$, suggesting that vector arithmetic can capture semantic relationships.
</div>

<h3>Linear Transformations</h3>

<div class="definition"><strong>Definition:</strong> 
A function $T: \R^n \to \R^m$ is a <strong>linear transformation</strong> if for all vectors $\vx, \vy \in \R^n$ and all scalars $a, b \in \R$:
<div class="equation">
$$
T(a\vx + b\vy) = aT(\vx) + bT(\vy)
$$
</div>
</div>

<p>Linear transformations preserve vector space structure: they map lines to lines and preserve the origin ($T(\mathbf{0}) = \mathbf{0}$).</p>

<h3>Matrices as Linear Transformations</h3>

<p>Every linear transformation from $\R^n$ to $\R^m$ can be represented by an $m \times n$ matrix.</p>

<div class="definition"><strong>Definition:</strong> 
An $m \times n$ matrix $\mA$ is a rectangular array of numbers with $m$ rows and $n$ columns:
<div class="equation">
$$
\mA = \begin{bmatrix} 
a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m,1} & a_{m,2} & \cdots & a_{m,n}
\end{bmatrix} \in \R^{m \times n}
$$
</div>
The notation $\mA \in \R^{m \times n}$ specifies the dimensions explicitly: $m$ rows and $n$ columns.
</div>

<div class="keypoint">
<strong>Dimension Tracking:</strong> For matrix-vector multiplication $\mA\vx = \vy$:
<div class="equation">
$$
\underbrace{\mA}_{\R^{m \times n}} \underbrace{\vx}_{\R^{n}} = \underbrace{\vy}_{\R^{m}}
$$
</div>
The inner dimensions must match ($n$), and the result has the outer dimensions ($m$).
</div>

<div class="example"><strong>Example:</strong> 
A single fully-connected neural network layer performs:
<div class="equation">
$$
\vh = \mW\vx + \vb
$$
</div>
where $\vx \in \R^{n_{\text{in}}}$, $\mW \in \R^{n_{\text{out}} \times n_{\text{in}}}$, $\vb \in \R^{n_{\text{out}}}$, $\vh \in \R^{n_{\text{out}}}$.

<p>For transforming a 784-dimensional input to 256-dimensional hidden representation:
<div class="equation">
$$
\underbrace{\vh}_{\R^{256}} = \underbrace{\mW}_{\R^{256 \times 784}} \underbrace{\vx}_{\R^{784}} + \underbrace{\vb}_{\R^{256}}
$$
</div>

<p>This layer has $256 \times 784 = 200{,}704$ weights plus 256 biases, totaling <strong>200,960 trainable parameters</strong>.</p>

<p><strong>Concrete Numerical Example:</strong> With $n_{\text{in}} = 3$, $n_{\text{out}} = 2$:
<div class="equation">
$$\begin{align}
\mW &= \begin{bmatrix} 0.5 & -0.3 & 0.8 \\ 0.2 & 0.6 & -0.4 \end{bmatrix}, \quad \vb = \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix}, \quad \vx = \begin{bmatrix} 1.0 \\ 2.0 \\ -0.5 \end{bmatrix}
\end{align}$$
</div>

<p>Computing:
<div class="equation">
$$\begin{align}
\mW\vx &= \begin{bmatrix} 0.5(1.0) - 0.3(2.0) + 0.8(-0.5) \\ 0.2(1.0) + 0.6(2.0) - 0.4(-0.5) \end{bmatrix} = \begin{bmatrix} -0.5 \\ 1.6 \end{bmatrix}\\
\vh &= \begin{bmatrix} -0.5 \\ 1.6 \end{bmatrix} + \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix} = \begin{bmatrix} -0.4 \\ 1.4 \end{bmatrix}
\end{align}$$
</div>
</div>

<h2>Matrix Operations</h2>

<h3>Matrix Multiplication</h3>

<div class="definition"><strong>Definition:</strong> 
For $\mA \in \R^{m \times n}$ and $\mB \in \R^{n \times p}$, their product $\mC = \mA\mB \in \R^{m \times p}$ is:
<div class="equation">
$$
c_{i,k} = \sum_{j=1}^{n} a_{i,j} b_{j,k}
$$
</div>
</div>

<div class="example"><strong>Example:</strong> 
Compute $\mC = \mA\mB$ where:
<div class="equation">
$$
\mA = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \in \R^{2 \times 2}, \quad \mB = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} \in \R^{2 \times 2}
$$
</div>

<p>Computing each entry:
<div class="equation">
$$\begin{align}
c_{1,1} &= 1(5) + 2(7) = 19 \\
c_{1,2} &= 1(6) + 2(8) = 22 \\
c_{2,1} &= 3(5) + 4(7) = 43 \\
c_{2,2} &= 3(6) + 4(8) = 50
\end{align}$$
</div>

<p>Therefore: $\mC = \begin{bmatrix} 19 & 22 \\ 43 & 50 \end{bmatrix}$
</div>

<h3>Transpose</h3>

<div class="definition"><strong>Definition:</strong>

<div class="definition"><strong>Definition:</strong> 
For vectors $\vx, \vy \in \R^n$, the <strong>dot product</strong> is:
<div class="equation">
$$
\vx\transpose \vy = \sum_{i=1}^{n} x_i y_i
$$
</div>
</div>

<div class="theorem"><strong>Theorem:</strong> 
For non-zero vectors $\vx, \vy \in \R^n$:
<div class="equation">
$$
\vx\transpose \vy = \norm{\vx}_2 \norm{\vy}_2 \cos(\theta)
$$
</div>
where $\theta$ is the angle between vectors and $\norm{\vx}_2 = \sqrt{\vx\transpose \vx}$ is the Euclidean norm.
</div>

<p>\begin{corollary}[Cosine Similarity]</p>

<p>The <strong>cosine similarity</strong> between two non-zero vectors is:
<div class="equation">
$$
\text{sim}(\vx, \vy) = \frac{\vx\transpose \vy}{\norm{\vx}_2 \norm{\vy}_2} = \cos(\theta) \in [-1, 1]
$$
</div>
\end{corollary}</p>

<div class="example"><strong>Example:</strong> 
In transformer attention, we compute similarity between query and key vectors using dot products:
<div class="equation">
$$
\vq = \begin{bmatrix} 0.5 \\ 0.8 \\ 0.3 \end{bmatrix}, \quad 
\vk_1 = \begin{bmatrix} 0.6 \\ 0.7 \\ 0.2 \end{bmatrix}, \quad
\vk_2 = \begin{bmatrix} -0.3 \\ 0.1 \\ 0.9 \end{bmatrix}
$$
</div>

<p>Computing similarities:
<div class="equation">
$$\begin{align}
\vq\transpose \vk_1 &= 0.5(0.6) + 0.8(0.7) + 0.3(0.2) = 0.92 \\
\vq\transpose \vk_2 &= 0.5(-0.3) + 0.8(0.1) + 0.3(0.9) = 0.20
\end{align}$$
</div>

<p>The query $\vq$ is more similar to $\vk_1$ (score 0.92) than to $\vk_2$ (score 0.20). These scores determine attention weights.
</div>

<h2>Matrix Decompositions</h2>

<h3>Eigenvalues and Eigenvectors</h3>

<div class="definition"><strong>Definition:</strong> 
For a square matrix $\mA \in \R^{n \times n}$, a non-zero vector $\vv \in \R^n$ is an <strong>eigenvector</strong> with corresponding <strong>eigenvalue</strong> $\lambda \in \R$ if:
<div class="equation">
$$
\mA \vv = \lambda \vv
$$
</div>
</div>

<p>Geometrically, an eigenvector is only scaled (not rotated) when $\mA$ is applied. The eigenvalue $\lambda$ is the scaling factor.</p>

<div class="example"><strong>Example:</strong> 
Find eigenvalues of:
<div class="equation">
$$
\mA = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix}
$$
</div>

<p>Solving $\det(\mA - \lambda \mI) = 0$:
<div class="equation">
$$\begin{align}
\det\begin{bmatrix} 3-\lambda & 1 \\ 1 & 3-\lambda \end{bmatrix} &= (3-\lambda)^2 - 1 = \lambda^2 - 6\lambda + 8 = 0\\
&= (\lambda - 4)(\lambda - 2) = 0
\end{align}$$
</div>

<p>Eigenvalues: $\lambda_1 = 4$, $\lambda_2 = 2$</p>

<p>For $\lambda_1 = 4$, eigenvector: $\vv_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$</p>

<p>For $\lambda_2 = 2$, eigenvector: $\vv_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$
</div>

<h3>Singular Value Decomposition</h3>

<div class="theorem"><strong>Theorem:</strong> 
Any matrix $\mA \in \R^{m \times n}$ can be decomposed as:
<div class="equation">
$$
\mA = \mU \boldsymbol{\Sigma} \mV\transpose
$$
</div>
where:
<ul>
    <li>$\mU \in \R^{m \times m}$ is orthogonal (left singular vectors)
    <li>$\boldsymbol{\Sigma} \in \R^{m \times n}$ is diagonal with singular values $\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$
    <li>$\mV \in \R^{n \times n}$ is orthogonal (right singular vectors)
</ul>
</div>

<div class="keypoint">
SVD always exists for any matrix, unlike eigendecomposition which requires special conditions.
</div>

<div class="example"><strong>Example:</strong> 
Consider weight matrix $\mW \in \R^{512 \times 2048}$ containing $1{,}048{,}576$ parameters.

<p>Using rank-$k=64$ SVD approximation:
<div class="equation">
$$
\mW \approx \mW_1 \mW_2
$$
</div>
where $\mW_1 \in \R^{512 \times 64}$ (32,768 parameters) and $\mW_2 \in \R^{64 \times 2048}$ (131,072 parameters).</p>

<p>Total: 163,840 parameters $\Rightarrow$ <strong>84\% compression!</strong>
</div>

<h2>Norms and Distance Metrics</h2>

<div class="definition"><strong>Definition:</strong> 
For vector $\vx \in \R^n$:
<div class="equation">
$$\begin{align}
\text{L1 norm (Manhattan):} \quad &\norm{\vx}_1 = \sum_{i=1}^n |x_i| \\
\text{L2 norm (Euclidean):} \quad &\norm{\vx}_2 = \sqrt{\sum_{i=1}^n x_i^2} \\
\text{L}\infty \text{ norm (Max):} \quad &\norm{\vx}_\infty = \max_i |x_i|
\end{align}$$
</div>
</div>

<div class="definition"><strong>Definition:</strong> 
For matrix $\mA \in \R^{m \times n}$:
<div class="equation">
$$
\text{Frobenius norm:} \quad \norm{\mA}_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n a_{i,j}^2} = \sqrt{\text{tr}(\mA\transpose \mA)}
$$
</div>
</div>

<p>Norms are used in regularization to prevent overfitting by penalizing large weights.</p>

<div class="implementation">
In PyTorch:
<pre><code>import torch

<p># Vector norms
x = torch.tensor([3.0, 4.0])
l2_norm = torch.norm(x, p=2)  # 5.0
l1_norm = torch.norm(x, p=1)  # 7.0</p>

<p># Matrix Frobenius norm
W = torch.randn(256, 784)
frob_norm = torch.norm(W, p='fro')
</code></pre>
</div>

<h2>Exercises</h2>

<div class="exercise"><strong>Exercise:</strong> 
Given $\vx = [2, -1, 3]\transpose$ and $\vy = [1, 4, -2]\transpose$, compute:
<ol>
    <li>The dot product $\vx\transpose \vy$
    <li>The L2 norms $\norm{\vx}_2$ and $\norm{\vy}_2$
    <li>The cosine similarity between $\vx$ and $\vy$
</ol>
</div>

<div class="exercise"><strong>Exercise:</strong> 
For a transformer layer with $d_{\text{model}} = 768$ and feed-forward dimension $d_{ff} = 3072$:
<ol>
    <li>Calculate the number of parameters in the two linear transformations
    <li>If processing a batch of $B = 32$ sequences of length $n = 512$, what are the dimensions of the input tensor?
    <li>How many floating-point operations (FLOPs) are required for one forward pass through this layer?
</ol>
</div>

<div class="exercise"><strong>Exercise:</strong> 
Prove that for symmetric matrix $\mA = \mA\transpose$, eigenvectors corresponding to distinct eigenvalues are orthogonal.
</div>

<div class="exercise"><strong>Exercise:</strong> 
A weight matrix $\mW \in \R^{1024 \times 4096}$ is approximated using SVD with rank $r$.
<ol>
    <li>Express the number of parameters as a function of $r$
    <li>What value of $r$ achieves 75\% compression?
    <li>What is the memory savings in MB (assuming 32-bit floats)?
</ol>
</div>
        
        <div class="chapter-nav">
  <a href="notation.html">‚Üê Notation and Conventions</a>
  <a href="../index.html">üìö Table of Contents</a>
  <a href="chapter02_calculus_optimization.html">Chapter 2: Calculus and Optimization ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
