<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning and Transformers - A Graduate-Level Course</title>
    <link rel="stylesheet" href="css/style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>Deep Learning and Transformers</h1>
        <p style="font-size: 1.2em; text-align: center; color: #666;">
            Theory, Mathematics, and Implementation<br>
            <em>A Graduate-Level Course</em>
        </p>
    </header>

    <main>
        <div class="toc">
            <h2>Table of Contents</h2>
            
            <h3>Front Matter</h3>
            <ul>
                <li><a href="chapters/preface.html">Preface</a></li>
                <li><a href="chapters/notation.html">Notation and Conventions</a></li>
            </ul>

            <h3>Part I: Mathematical Foundations</h3>
            <ul>
                <li><a href="chapters/chapter01_linear_algebra.html">Chapter 1: Linear Algebra for Deep Learning</a></li>
                <li><a href="chapters/chapter02_calculus_optimization.html">Chapter 2: Calculus and Optimization</a></li>
                <li><a href="chapters/chapter03_probability_information.html">Chapter 3: Probability and Information Theory</a></li>
            </ul>

            <h3>Part II: Neural Network Fundamentals</h3>
            <ul>
                <li><a href="chapters/chapter04_feedforward_networks.html">Chapter 4: Feed-Forward Neural Networks</a></li>
                <li><a href="chapters/chapter05_convolutional_networks.html">Chapter 5: Convolutional Neural Networks</a></li>
                <li><a href="chapters/chapter06_recurrent_networks.html">Chapter 6: Recurrent Neural Networks</a></li>
            </ul>

            <h3>Part III: Attention Mechanisms</h3>
            <ul>
                <li><a href="chapters/chapter07_attention_fundamentals.html">Chapter 7: Attention Mechanisms: Fundamentals</a></li>
                <li><a href="chapters/chapter08_self_attention.html">Chapter 8: Self-Attention and Multi-Head Attention</a></li>
                <li><a href="chapters/chapter09_attention_variants.html">Chapter 9: Attention Variants and Mechanisms</a></li>
            </ul>

            <h3>Part IV-VII: Coming Soon</h3>
            <ul>
                <li>Part IV: Transformer Architecture</li>
                <li>Part V: Modern Transformer Variants</li>
                <li>Part VI: Advanced Topics</li>
                <li>Part VII: Practical Implementation</li>
            </ul>
        </div>

        <div style="margin-top: 3em; padding: 2em; background-color: #f8f9fa; border-radius: 5px;">
            <h2>About This Book</h2>
            <p>
                This textbook provides a comprehensive, graduate-level treatment of deep learning and transformer architectures.
                It emphasizes mathematical rigor while maintaining practical relevance, with complete derivations,
                concrete numerical examples, and implementation guidance.
            </p>
            <p>
                <strong>Key Features:</strong>
            </p>
            <ul>
                <li>Complete mathematical derivations with geometric intuition</li>
                <li>Explicit dimension tracking for all operations</li>
                <li>Concrete numerical examples from real models (BERT, GPT, etc.)</li>
                <li>Implementation notes and code examples</li>
                <li>Progressive complexity building from foundations</li>
            </ul>
        </div>
    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
        <p>
            <a href="https://github.com/PietKonijn007/deep-learning-transformers-textbook">View on GitHub</a>
        </p>
    </footer>

    <script src="js/main.js"></script>
</body>
</html>
