\chapter{Hardware Optimization and Deployment}
\label{chap:hardware_optimization}

\section*{Chapter Overview}

Deploying transformers efficiently requires understanding hardware architectures, optimization techniques, and deployment strategies. This chapter covers GPUs, TPUs, model quantization, pruning, distillation, and production deployment best practices.

\subsection*{Learning Objectives}

\begin{enumerate}
    \item Understand GPU/TPU architectures for transformers
    \item Apply model quantization (INT8, FP16)
    \item Implement pruning and sparsity
    \item Use knowledge distillation for compression
    \item Optimize inference latency and throughput
    \item Deploy models in production environments
\end{enumerate}

\section{Hardware Architectures}
\label{sec:hardware_architectures}

\subsection{GPU Architecture for Deep Learning}

Modern GPUs contain two primary types of compute units that are critical for transformer training and inference. CUDA cores are general-purpose floating-point units that can execute arbitrary arithmetic operations, while Tensor Cores are specialized matrix multiplication units designed specifically for deep learning workloads. Understanding the distinction between these units is essential for achieving optimal performance.

\textbf{CUDA Cores vs Tensor Cores.} CUDA cores provide flexibility for general computation but operate at lower throughput for matrix operations. A single NVIDIA A100 GPU contains 6912 CUDA cores capable of 19.5 TFLOPS at FP32 precision. In contrast, Tensor Cores are specialized hardware units that perform fused multiply-add operations on small matrix tiles. The same A100 GPU contains 432 third-generation Tensor Cores that deliver 312 TFLOPS for FP16 matrix multiplication, representing a 16× advantage over CUDA cores for this specific operation. This dramatic performance difference makes Tensor Cores essential for transformer workloads, which are dominated by matrix multiplications in attention mechanisms and feed-forward layers.

\textbf{Memory Hierarchy.} GPU memory is organized in a hierarchy that trades capacity for access speed. At the top of the hierarchy, each streaming multiprocessor (SM) contains registers that provide the fastest access with approximately 256KB of storage per SM. These registers are private to individual threads and have single-cycle latency. The next level is shared memory, a software-managed cache that allows threads within a block to communicate efficiently. The A100 provides 164KB of shared memory per SM with latency of approximately 20-30 cycles. Below this sits the L2 cache, a 40MB hardware-managed cache shared across all SMs with latency around 200 cycles. Finally, high-bandwidth memory (HBM) provides the largest capacity at 40-80GB but with the highest latency of 300-400 cycles. This hierarchy means that keeping data in faster memory levels is critical for performance.

The memory bandwidth available at each level determines how quickly data can be moved. The A100's HBM provides 1.6 TB/s of bandwidth, while the V100 provides 900 GB/s. Although these numbers seem large, they are often the bottleneck for transformer operations. Consider that the A100's Tensor Cores can consume data at a rate of 312 TFLOPS × 2 bytes (FP16) = 624 TB/s if fully utilized, far exceeding the 1.6 TB/s that HBM can supply. This mismatch between compute capability and memory bandwidth is why memory optimization is crucial for transformers.

\textbf{Streaming Multiprocessors.} The A100 contains 108 streaming multiprocessors, each capable of executing multiple thread blocks concurrently. Each SM has its own register file, shared memory, and L1 cache, along with 4 Tensor Cores. The SM scheduler can switch between thread warps (groups of 32 threads) with zero overhead, hiding memory latency by executing other warps while some wait for data. Achieving high occupancy, defined as the ratio of active warps to maximum possible warps, is essential for hiding latency and maximizing throughput.

\subsection{Computational Intensity}

\begin{definition}[Arithmetic Intensity]
\label{def:arithmetic_intensity}
\begin{equation}
\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes transferred}}
\end{equation}
\end{definition}

\begin{mermaid}[GPU Memory Hierarchy and Optimization]
graph LR
    PARAM["Model Params\n W in R^P\n FP16: 2P bytes"] --> HBM["HBM (GPU RAM)\n 40-80 GB\n Bandwidth: 2 TB/s\n Stores: params, grads,\n optimizer states, activations"]
    HBM --> SRAM["SRAM (On-chip)\n ~20 MB per SM\n Bandwidth: 19 TB/s\n Stores: tiles of Q,K,V\n during FlashAttention"]

    subgraph Optim["Memory Optimizations"]
        MP["Mixed Precision\n FP32 master weights\n FP16 forward/backward\n Saves ~50% memory"]
        GC["Gradient Checkpointing\n Discard activations\n Recompute in backward\n Saves O(L) memory\n Costs ~33% extra compute"]
        FSDP["FSDP/ZeRO\n Shard params+grads+opt\n across N GPUs\n Memory: O(P/N) per GPU"]
    end

    HBM --> Optim

    style PARAM fill:#e8f5e9,stroke:#4caf50,color:#000
    style HBM fill:#fff3e0,stroke:#ff9800,color:#000
    style SRAM fill:#e3f2fd,stroke:#2196f3,color:#000
    style GC fill:#fce4ec,stroke:#e91e63,color:#000
\end{mermaid}


Arithmetic intensity measures the ratio of computation to memory access for a given operation, and it is a critical metric for understanding whether an operation will be compute-bound or memory-bound on modern hardware. For attention mechanisms, the arithmetic intensity varies significantly with sequence length, which has profound implications for optimization strategies.

Consider the computation of $\mQ \mK\transpose$ in the attention mechanism, where both $\mQ$ and $\mK$ have shape $[n, d]$ with $n$ being the sequence length and $d$ the head dimension. This matrix multiplication requires $2n^2d$ floating-point operations (FLOPs), as we compute $n^2$ dot products, each involving $d$ multiply-add operations. The memory traffic consists of loading $\mQ$ (requiring $nd$ elements) and $\mK$ (another $nd$ elements), plus writing the output matrix of size $n^2$. For large $n$, the output dominates, giving approximately $2nd$ bytes transferred. The arithmetic intensity is therefore approximately $\frac{2n^2d}{2nd} = \frac{nd}{d} = n$.

This analysis reveals a crucial insight: for small sequence lengths (such as $n < 1024$), the arithmetic intensity is low, meaning the operation transfers nearly as much data as it performs computation. On an A100 GPU with 1.6 TB/s memory bandwidth and 312 TFLOPS compute capability, operations with intensity below 195 FLOPs/byte will be memory-bound. Since attention with $n = 512$ has intensity of only 512 FLOPs/byte when considering all memory traffic, it operates well below the compute-bound regime. For large sequence lengths ($n > 4096$), the arithmetic intensity increases proportionally, and the operation becomes increasingly compute-bound, allowing better utilization of Tensor Cores. This is why techniques like FlashAttention, which restructure attention to increase data reuse and reduce memory traffic, provide the most dramatic speedups for moderate sequence lengths where memory bandwidth is the primary bottleneck.

\subsection{Tensor Core Optimization}

Tensor Cores achieve their peak performance only when specific conditions are met regarding data types, matrix dimensions, and memory layout. Understanding these requirements is essential for extracting maximum performance from modern GPUs.

\textbf{Precision Requirements.} Tensor Cores support several precision modes, each with different performance characteristics. FP16 (half precision) provides 312 TFLOPS on the A100, making it the standard choice for training. BF16 (bfloat16) offers the same throughput but with a larger dynamic range that better matches FP32, reducing the need for loss scaling during mixed-precision training. For inference, INT8 provides 624 TOPS, doubling throughput at the cost of reduced precision. The choice of precision involves trading off between speed, memory usage, and numerical accuracy.

\textbf{Dimension Requirements.} Tensor Cores operate on small matrix tiles and achieve peak performance when matrix dimensions are multiples of specific values. For FP16 operations, dimensions should be multiples of 8, while BF16 and INT8 operations prefer multiples of 16. When dimensions are not multiples of these values, the hardware must pad matrices or fall back to slower execution paths. For example, a matrix multiplication with dimensions 1023×1023 will perform significantly worse than 1024×1024 because the former requires padding or partial tile operations.

\begin{example}[BERT-base Tensor Core Utilization]
\label{ex:bert_tensor_core}
Consider BERT-base with hidden size $d=768$ and 12 attention heads. The per-head dimension is $d_k = 768/12 = 64$, which is a multiple of 8, allowing efficient Tensor Core usage. The query, key, and value projections have shape $[b \times n, 768] \times [768, 768]$ where $b$ is batch size and $n$ is sequence length.

\textbf{Baseline configuration:} Batch size 16, sequence length 128, FP32 precision
\begin{itemize}
    \item Throughput: 145 sequences/second
    \item GPU utilization: 42\%
    \item Memory bandwidth: 890 GB/s (56\% of peak)
\end{itemize}

\textbf{Optimized configuration:} Batch size 32, sequence length 128, FP16 with Tensor Cores
\begin{itemize}
    \item Throughput: 520 sequences/second (3.6× improvement)
    \item GPU utilization: 78\%
    \item Memory bandwidth: 1.45 TB/s (91\% of peak)
    \item Tensor Core utilization: 85\%
\end{itemize}

The optimization involved: (1) switching to FP16 to enable Tensor Cores, (2) increasing batch size to improve arithmetic intensity, and (3) ensuring all matrix dimensions are multiples of 8. The result is a 3.6× speedup with negligible accuracy loss when using mixed-precision training with loss scaling.
\end{example}

\textbf{Achieving Peak Performance.} Reaching 90\% of theoretical peak TFLOPS requires careful attention to several factors. First, ensure sufficient work is available by using large batch sizes or long sequences to keep all Tensor Cores busy. Second, minimize memory transfers by fusing operations and reusing data in shared memory. Third, maintain high occupancy by using appropriate thread block sizes and avoiding resource limitations. Finally, profile the application to identify bottlenecks using tools like NVIDIA Nsight Compute, which can show Tensor Core utilization and identify whether operations are compute-bound or memory-bound.

\subsection{TPU Architecture}

Tensor Processing Units (TPUs) represent Google's specialized hardware for machine learning workloads, built around a fundamentally different architectural philosophy than GPUs. At the core of each TPU is a systolic array, a grid of processing elements where data flows rhythmically through the array in a wave-like pattern. This architecture is specifically optimized for the matrix multiplications that dominate transformer computations, achieving high efficiency by minimizing data movement and maximizing computational density.

The TPU v4, Google's latest generation at the time of writing, delivers 275 TFLOPS of compute throughput using bfloat16 precision, a 16-bit floating-point format that maintains the dynamic range of FP32 while halving memory requirements. The systolic array architecture enables this performance with 900 GB/s of memory bandwidth, comparable to high-end GPUs. Unlike GPUs, which evolved from graphics rendering and retain general-purpose programmability, TPUs are purpose-built for machine learning and sacrifice some flexibility for higher efficiency on matrix operations.

The choice between TPUs and GPUs involves several trade-offs. GPUs offer greater flexibility through CUDA programming and support for arbitrary computational patterns, making them well-suited for research environments where novel architectures and operations are frequently explored. TPUs excel at large-scale training of standard transformer architectures, particularly when very large batch sizes can be used to fully utilize the systolic array. In terms of raw performance, the A100 GPU delivers 312 TFLOPS for FP16 operations compared to the TPU v4's 275 TFLOPS for bfloat16, though direct comparisons are complicated by differences in precision formats and memory hierarchies.

Memory capacity also differs between the platforms. Modern GPUs like the A100 offer 40-80 GB of high-bandwidth memory per device, while TPU v4 chips provide 32 GB per chip but are typically deployed in pods of multiple chips with fast interconnects. For training, GPUs are often preferred for their flexibility and mature software ecosystem, while TPUs can be more cost-effective for large-scale production training of well-established architectures. The optimal choice depends on the specific workload, scale, and whether the application requires custom operations or can work within the constraints of TPU-optimized frameworks like JAX and TensorFlow.

\begin{table}[htbp]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{GPU} & \textbf{TPU} \\
\midrule
Flexibility & High (general purpose) & Medium (ML-specific) \\
Peak FLOPS & 312 (A100 FP16) & 275 (v4 bf16) \\
Memory & 40-80 GB & 32 GB (per chip) \\
Batch size & Medium-Large & Very Large \\
Best for & Flexibility, research & Large-scale training \\
\bottomrule
\end{tabular}
\end{table}

\section{Memory Optimization Techniques}
\label{sec:memory_optimization}

Memory access patterns have a profound impact on GPU performance, often determining whether an operation runs at 10\% or 90\% of peak throughput. This section explores the key memory optimization techniques that are essential for efficient transformer implementations.

\subsection{Coalesced Memory Access}

When threads in a warp access global memory, the hardware attempts to combine these accesses into a single transaction. Coalesced access occurs when consecutive threads access consecutive memory locations, allowing the hardware to issue one memory transaction instead of 32 separate ones. For example, if thread 0 accesses address 0, thread 1 accesses address 4, thread 2 accesses address 8, and so on (assuming 4-byte elements), the hardware can coalesce these into a single 128-byte transaction. In contrast, if threads access random or strided locations, each access may require a separate transaction, reducing effective bandwidth by up to 32×.

For transformer operations, coalesced access is particularly important in matrix multiplications and attention computations. When loading a row of the query matrix, ensuring that consecutive threads load consecutive elements allows full utilization of memory bandwidth. This often requires careful consideration of matrix layout (row-major vs column-major) and access patterns in custom CUDA kernels.

\subsection{Shared Memory and Bank Conflicts}

Shared memory is divided into 32 banks that can be accessed simultaneously. When multiple threads in a warp access the same bank but different addresses, a bank conflict occurs, serializing the accesses and reducing throughput. The A100's shared memory has 32 banks with 4-byte bank width, meaning addresses that differ by 128 bytes map to the same bank.

In attention implementations, shared memory is commonly used to cache tiles of the query, key, and value matrices. Careful padding of these tiles can eliminate bank conflicts. For example, if each thread block loads a 64×64 tile of FP16 values, adding 8 elements of padding to each row ensures that consecutive rows start at different banks, eliminating conflicts when threads access columns.

\begin{example}[Shared Memory Optimization in Attention]
\label{ex:shared_memory_attention}
Consider computing attention scores $\mA = \mQ\mK\transpose$ where $\mQ, \mK \in \mathbb{R}^{n \times d}$. A naive implementation loads tiles of $\mQ$ and $\mK$ into shared memory and computes partial results.

\textbf{Unoptimized approach:}
\begin{itemize}
    \item Tile size: 64×64 FP16 values
    \item Shared memory per tile: $64 \times 64 \times 2 = 8192$ bytes
    \item Bank conflicts when accessing columns: 32-way conflicts
    \item Effective bandwidth: 50 GB/s (3\% of peak)
\end{itemize}

\textbf{Optimized approach with padding:}
\begin{itemize}
    \item Tile size: 64×72 FP16 values (8 elements padding per row)
    \item Shared memory per tile: $64 \times 72 \times 2 = 9216$ bytes
    \item No bank conflicts: consecutive rows in different banks
    \item Effective bandwidth: 1.4 TB/s (88\% of peak)
\end{itemize}

The 12.5\% increase in shared memory usage eliminates bank conflicts and increases bandwidth by 28×, demonstrating the critical importance of memory access patterns.
\end{example}

\subsection{Memory Bandwidth Utilization}

Maximizing memory bandwidth requires ensuring that memory operations are large enough to amortize transaction overhead and that the GPU has sufficient concurrent operations to hide latency. Small transfers are inefficient because they cannot fully utilize the 32-byte or 128-byte cache line sizes. Additionally, launching enough thread blocks to keep all memory controllers busy is essential for achieving peak bandwidth.

For transformers, memory bandwidth is often the limiting factor during attention computation with short sequences. When sequence length $n < 1024$, the arithmetic intensity of attention is low, meaning each floating-point operation requires loading relatively more data from memory. Techniques like Flash Attention address this by restructuring computations to maximize data reuse in shared memory, reducing the number of global memory accesses.

\section{Kernel Fusion and Operation Optimization}
\label{sec:kernel_fusion}

Kernel fusion combines multiple operations into a single GPU kernel, reducing memory traffic and kernel launch overhead. This technique is particularly effective for transformers, where many operations are memory-bound and benefit from data reuse.

\subsection{Fusion Opportunities in Transformers}

Standard transformer implementations launch separate kernels for each operation, requiring intermediate results to be written to and read from global memory. For example, computing layer normalization followed by dropout requires writing the normalized values to memory, then reading them back for the dropout operation. Fusing these operations allows the normalized values to remain in registers or shared memory, eliminating the round-trip to global memory.

\textbf{Common fusion patterns:}
\begin{enumerate}
    \item \textbf{Layer norm + dropout:} Normalize activations and apply dropout in a single pass, keeping intermediate values in registers.
    \item \textbf{GELU + bias:} Compute the GELU activation and add bias without storing intermediate results.
    \item \textbf{Attention score + softmax + dropout:} Compute $\text{softmax}(\mQ\mK\transpose/\sqrt{d_k})$ and apply dropout in one kernel.
    \item \textbf{Residual + layer norm:} Add residual connection and normalize in a single operation.
\end{enumerate}

\begin{example}[Layer Norm + Dropout Fusion]
\label{ex:layernorm_dropout_fusion}
Consider layer normalization followed by dropout on a tensor of shape $[32, 128, 768]$ (batch size 32, sequence length 128, hidden size 768).

\textbf{Unfused implementation:}
\begin{itemize}
    \item Layer norm kernel: Read 3.1M elements, write 3.1M elements
    \item Dropout kernel: Read 3.1M elements, write 3.1M elements
    \item Total memory traffic: 12.4M elements = 24.8 MB (FP16)
    \item Execution time: 0.18 ms
\end{itemize}

\textbf{Fused implementation:}
\begin{itemize}
    \item Single kernel: Read 3.1M elements, write 3.1M elements
    \item Total memory traffic: 6.2M elements = 12.4 MB (FP16)
    \item Execution time: 0.10 ms
\end{itemize}

The fused kernel achieves 1.8× speedup by halving memory traffic and eliminating kernel launch overhead. For a 12-layer transformer, this fusion appears 24 times per forward pass (twice per layer), providing substantial cumulative savings.
\end{example}

\subsection{Flash Attention: Fused Attention Implementation}

Flash Attention represents a sophisticated application of kernel fusion to the attention mechanism. Standard attention implementations compute $\mA = \mQ\mK\transpose$, write $\mA$ to memory, read it back for softmax, write the result, read it again for multiplication with $\mV$, and finally write the output. This results in $O(n^2)$ memory reads and writes where $n$ is sequence length.

Flash Attention restructures the computation to work on tiles that fit in shared memory. It computes attention for one tile at a time, keeping intermediate results in fast memory and only writing the final output. This reduces memory traffic from $O(n^2)$ to $O(n)$, providing dramatic speedups for long sequences.

\begin{example}[Flash Attention Performance]
\label{ex:flash_attention_perf}
BERT-base with sequence length 512, batch size 16, on A100 GPU:

\textbf{Standard attention:}
\begin{itemize}
    \item Memory traffic: 48 GB per forward pass
    \item Execution time: 8.2 ms
    \item Memory bandwidth utilization: 45\%
\end{itemize}

\textbf{Flash Attention:}
\begin{itemize}
    \item Memory traffic: 12 GB per forward pass (4× reduction)
    \item Execution time: 3.8 ms (2.2× speedup)
    \item Memory bandwidth utilization: 82\%
\end{itemize}

For longer sequences, the benefits are even more pronounced. At sequence length 2048, Flash Attention provides 3.5× speedup, and at 8192, it provides 5.2× speedup while also enabling sequences that would otherwise exceed memory capacity.
\end{example}

\subsection{Implementing Fused Kernels}

Creating fused kernels requires careful consideration of register usage, shared memory capacity, and thread block dimensions. The goal is to maximize data reuse while maintaining high occupancy. Modern deep learning frameworks provide tools for kernel fusion, including PyTorch's JIT compiler and TensorRT's graph optimizer, which can automatically fuse compatible operations. For custom fusion patterns, libraries like CUTLASS provide templates for efficient CUDA implementations.

\section{Model Quantization}
\label{sec:quantization}

\subsection{Quantization Fundamentals}

\begin{definition}[Quantization]
\label{def:quantization}
Map FP32 weights to lower precision (INT8, FP16):
\begin{equation}
w_{\text{quant}} = \text{round}\left(\frac{w_{\text{float}}}{s}\right) + z
\end{equation}
where $s$ is scale factor, $z$ is zero-point.
\end{definition}

The choice of numerical precision fundamentally affects both the performance and accuracy of transformer models. For PyTorch quantization implementation (dynamic, static, and QAT), see Chapter~\ref{chap:pytorchimplementation}. Modern hardware supports several precision formats, each offering different trade-offs between memory usage, computational throughput, and numerical fidelity.

FP32 (32-bit floating point) serves as the baseline precision format, providing full numerical precision with a wide dynamic range. This format uses 1 bit for sign, 8 bits for exponent, and 23 bits for mantissa, allowing representation of values from approximately $10^{-38}$ to $10^{38}$ with about 7 decimal digits of precision. While FP32 ensures numerical stability and is the standard for training, it consumes significant memory and cannot leverage specialized hardware acceleration like Tensor Cores.

FP16 (16-bit floating point) reduces memory usage by half compared to FP32, using 1 sign bit, 5 exponent bits, and 10 mantissa bits. This format enables 2× compression and, more importantly, allows Tensor Cores to deliver their peak throughput. However, FP16's limited dynamic range (approximately $10^{-8}$ to $65504$) can cause numerical issues during training, particularly with gradient underflow. Mixed-precision training addresses this by maintaining FP32 master weights while using FP16 for forward and backward passes, combined with loss scaling to prevent gradient underflow.

BF16 (bfloat16) offers an alternative 16-bit format that maintains FP32's 8-bit exponent while reducing the mantissa to 7 bits. This design preserves FP32's dynamic range, eliminating the need for loss scaling in most cases, though at the cost of reduced precision. BF16 has become increasingly popular for training large language models, as it provides the memory and compute benefits of FP16 without the numerical stability challenges. Modern GPUs like the A100 support BF16 with the same throughput as FP16.

INT8 (8-bit integer) provides 4× compression compared to FP32 and is primarily used for inference. Tensor Cores can process INT8 operations at twice the throughput of FP16, making it attractive for deployment. However, mapping continuous floating-point values to 256 discrete integer levels requires careful calibration to minimize accuracy loss. The quantization process determines appropriate scale factors and zero-points based on the distribution of weights and activations.

INT4 (4-bit integer) represents the extreme end of quantization, offering 8× compression but with significant accuracy challenges. This format is typically reserved for specialized applications where model size is the dominant constraint, such as edge deployment on severely memory-constrained devices. Recent research has explored mixed-precision approaches where different layers use different precisions based on their sensitivity to quantization.

\subsection{Post-Training Quantization (PTQ)}

Post-training quantization offers a straightforward path to model compression without requiring access to the original training pipeline or large amounts of training data. This approach takes a fully trained FP32 model and converts it to lower precision, typically INT8, through a calibration process that determines appropriate scale factors for weights and activations.

The PTQ procedure begins with the trained FP32 model and a small calibration dataset, typically consisting of a few hundred to a few thousand representative examples. During calibration, the model processes these examples in FP32 mode while collecting statistics about the distribution of activations in each layer. These statistics, particularly the minimum and maximum values observed for each activation tensor, are used to determine the scale factors that map the continuous FP32 range to the discrete INT8 range.

Once scale factors are determined, the conversion process applies the quantization formula to both weights and activations. Weights can be quantized offline since they are fixed after training, while activation quantization must be performed dynamically during inference. The scale factor $s$ is chosen to map the observed range of values to the INT8 range of $[-128, 127]$, and the zero-point $z$ is selected to ensure that the value zero in floating-point maps exactly to an integer value, which is important for operations like ReLU and padding.

The primary advantage of PTQ is its simplicity and speed. No retraining is required, making it accessible even when the original training data or computational resources are unavailable. Modern frameworks like PyTorch and TensorFlow provide built-in PTQ APIs that automate the calibration and conversion process. Hardware-specific compilers like TensorRT further optimize the quantized model for deployment on specific accelerators.

However, PTQ has limitations. The quantization process introduces approximation errors that can accumulate through the network, particularly in deep models. Layers with wide activation distributions or outlier values are especially sensitive to quantization. For transformer models, attention mechanisms and layer normalization operations often exhibit such characteristics, leading to accuracy degradation. The BERT-base example in this chapter demonstrates this trade-off: PTQ achieves substantial speedup but at the cost of noticeable accuracy loss on downstream tasks.

\begin{example}[INT8 Quantization]
\label{ex:int8_quantization}
\textbf{FP32 weight:} $w = 0.137$

\textbf{Determine range:} $w \in [-1.0, 1.0]$

\textbf{Scale:} $s = \frac{2.0}{256} = 0.0078125$

\textbf{Quantize:}
\begin{equation}
w_{\text{INT8}} = \text{round}\left(\frac{0.137}{0.0078125}\right) = \text{round}(17.54) = 18
\end{equation}

\textbf{Dequantize:} $w' = 18 \times 0.0078125 = 0.1406$

\textbf{Error:} $|0.137 - 0.1406| = 0.0036$ (2.6\% relative)
\end{example}

\subsection{Quantization-Aware Training (QAT)}

Quantization-aware training addresses the accuracy limitations of post-training quantization by incorporating quantization effects directly into the training process. Rather than quantizing a pre-trained model and accepting whatever accuracy degradation results, QAT trains the model to be inherently robust to quantization from the start.

The key innovation in QAT is the use of simulated quantization during training. In the forward pass, weights and activations are quantized to the target precision (typically INT8) using the same quantization formula that will be used during inference. These quantized values are then used for all subsequent computations, ensuring that the model experiences the same numerical approximations during training that it will encounter during deployment. However, the backward pass operates differently: gradients are computed in full FP32 precision and applied to FP32 master copies of the weights. This asymmetry is necessary because gradient values are typically much smaller than activations and would underflow if quantized to INT8.

The training process allows the model to adapt to quantization in several ways. First, the optimizer can adjust weight values to positions that are more robust to rounding. For example, if a weight of 0.137 rounds to 0.141 in INT8, causing poor performance, the optimizer can shift it to 0.125, which might round more favorably. Second, the model can learn to avoid activation distributions that quantize poorly. Layers can adjust their output scales to better utilize the available INT8 range, and the network can learn to be less sensitive to the specific quantization errors that occur.

The benefits of QAT are substantial. Models trained with quantization awareness typically recover most or all of the accuracy lost in post-training quantization, while maintaining the same inference speedup and memory savings. The BERT-base example demonstrates this clearly: QAT achieves nearly the same accuracy as the FP32 baseline while delivering the same 2.9× speedup as PTQ. This makes QAT the preferred approach whenever retraining is feasible.

However, QAT does have costs. It requires access to the training data and computational resources for fine-tuning, which may take several epochs to converge. The training process is also more complex, requiring careful management of the FP32 master weights and simulated quantization operations. Additionally, QAT typically requires framework support for fake quantization operators, which are available in PyTorch and TensorFlow but may not be supported in all training frameworks. Despite these challenges, the accuracy benefits make QAT the standard approach for production deployment of quantized transformer models.

\begin{example}[BERT-base Quantization Results]
\label{ex:bert_quantization}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Precision} & \textbf{GLUE Score} & \textbf{Speedup} \\
\midrule
FP32 (baseline) & 84.5 & 1.0× \\
FP16 & 84.4 & 1.8× \\
INT8 (PTQ) & 82.1 & 2.9× \\
INT8 (QAT) & 84.2 & 2.9× \\
\bottomrule
\end{tabular}
\end{center}

QAT recovers most accuracy lost in PTQ!
\end{example}

\section{Model Pruning}
\label{sec:pruning}

\subsection{Pruning Strategies}

Model pruning removes parameters from a trained network to reduce its computational and memory requirements. The fundamental challenge is identifying which parameters can be removed with minimal impact on model performance. Two main approaches have emerged: magnitude-based pruning, which uses weight values as a proxy for importance, and structured pruning, which removes entire architectural units.

Magnitude-based pruning operates on the principle that weights with small absolute values contribute less to the model's predictions and can therefore be safely removed. For a weight $w_{ij}$ connecting neuron $i$ to neuron $j$, the pruning decision is made by comparing $|w_{ij}|$ to a threshold $\tau$. If $|w_{ij}| < \tau$, the weight is set to zero and excluded from computation. This approach is appealingly simple and has been shown to be surprisingly effective across many architectures. The threshold $\tau$ can be set globally across the entire network, per-layer to account for different scales in different parts of the model, or even adaptively based on the distribution of weights in each layer.

The distinction between structured and unstructured pruning has profound implications for deployment. Unstructured pruning removes individual weights without regard for their position in the network, creating sparse weight matrices where zeros are scattered throughout. This approach can achieve high compression ratios---often removing 80-90\% of weights in some layers---but requires specialized sparse matrix kernels to realize speedups. Standard dense matrix multiplication libraries like cuBLAS cannot exploit unstructured sparsity, so the theoretical reduction in FLOPs does not translate to wall-clock speedup without custom implementations.

Structured pruning addresses this limitation by removing entire architectural units: complete neurons, attention heads, or even full layers. When an attention head is pruned, the corresponding rows in the query, key, and value projection matrices are removed, directly reducing the matrix dimensions. This reduction is immediately visible to standard dense linear algebra libraries, so speedups materialize without requiring sparse kernels. The trade-off is that structured pruning typically achieves lower compression ratios than unstructured pruning, as it must remove parameters in groups rather than individually. For transformers, structured pruning of attention heads has proven particularly effective, as empirical studies show that many heads can be removed with minimal accuracy impact.

\subsection{Iterative Pruning}

\begin{algorithm}[H]
\caption{Iterative Magnitude Pruning}
\label{alg:iterative_pruning}

\textbf{Input:} Model, sparsity target $s_{\text{target}}$

\For{sparsity $s = 0$ \KwTo $s_{\text{target}}$ by steps}{
    Train model to convergence \\
    Prune $\Delta s$ lowest-magnitude weights \\
    Fine-tune model
}
\end{algorithm}

\begin{example}[Attention Head Pruning]
\label{ex:head_pruning}
BERT-base: 12 layers × 12 heads = 144 heads

\textbf{Finding:} Can remove 50\% of heads with minimal impact!

\textbf{Procedure:}
\begin{enumerate}
    \item Compute importance score per head
    \item Rank heads by importance
    \item Prune lowest 50\% (72 heads)
    \item Fine-tune remaining model
\end{enumerate}

\textbf{Result:}
\begin{itemize}
    \item 50\% fewer attention operations
    \item GLUE score: 84.5 $\to$ 83.8 (0.7 point drop)
    \item 1.5× faster inference
\end{itemize}
\end{example}

\section{Knowledge Distillation}
\label{sec:knowledge_distillation_detail}

\subsection{Distillation Loss}

\begin{definition}[Distillation Objective]
\label{def:distillation_objective}
\begin{equation}
\mathcal{L} = \alpha \mathcal{L}_{\text{CE}}(y, y_{\text{student}}) + (1-\alpha) \mathcal{L}_{\text{KD}}(y_{\text{teacher}}, y_{\text{student}})
\end{equation}

where:
\begin{equation}
\mathcal{L}_{\text{KD}} = \text{KL}\left(\frac{\exp(z_t/T)}{\sum \exp(z_t/T)} \Big\| \frac{\exp(z_s/T)}{\sum \exp(z_s/T)}\right)
\end{equation}

$T$ = temperature (typically 2-5), higher = softer probabilities
\end{definition}

\subsection{DistilBERT Approach}

DistilBERT demonstrates a comprehensive approach to knowledge distillation for transformer models, combining architectural reduction with multi-objective training to create a student model that is substantially smaller and faster than BERT while retaining most of its capabilities.

The student architecture makes a strategic choice: rather than reducing the hidden dimension, which would require retraining all projection matrices and could fundamentally alter the model's representational capacity, DistilBERT reduces depth by half. The student contains only 6 transformer layers compared to BERT-base's 12, but maintains the same hidden size of 768 dimensions. This design choice is motivated by observations that deeper layers in BERT often learn similar representations, suggesting redundancy that can be compressed. The student is initialized by copying every other layer from the teacher (layers 0, 2, 4, 6, 8, and 10), providing a warm start that accelerates convergence.

The training objective combines three complementary losses. First, the distillation loss encourages the student to match the teacher's output distribution using the temperature-scaled KL divergence described earlier. Second, a standard masked language modeling loss ensures the student can still perform the original pre-training task, maintaining its ability to learn from unlabeled text. Third, a cosine distance loss between the student and teacher hidden states encourages the student's internal representations to align with the teacher's, not just the final outputs. This multi-objective approach helps the student learn both the teacher's behavior and its internal structure.

The results demonstrate the effectiveness of this approach. The final DistilBERT model contains 66 million parameters compared to BERT-base's 110 million, a 40\% reduction. Inference speed improves by approximately 60\%, as the reduced depth directly translates to fewer sequential operations. Most importantly, DistilBERT retains 97\% of BERT's performance on the GLUE benchmark, meaning the accuracy loss is only 3\% despite the substantial reduction in model size and computational cost. This combination of compression, speedup, and accuracy retention makes DistilBERT a compelling choice for production deployments where resources are constrained.

\section{Multi-GPU Training and Optimization}
\label{sec:multi_gpu}

Training large transformer models requires distributing computation across multiple GPUs. The efficiency of multi-GPU training depends critically on communication bandwidth, parallelization strategy, and the balance between computation and communication.

\subsection{Interconnect Technologies}

The bandwidth between GPUs determines how quickly gradients, activations, and parameters can be exchanged during distributed training. PCIe provides 16-32 GB/s bidirectional bandwidth per GPU, which is adequate for small models but becomes a bottleneck for large transformers. NVLink, NVIDIA's proprietary interconnect, provides 600 GB/s bidirectional bandwidth on A100 systems, enabling much more efficient multi-GPU training. For comparison, the bandwidth within a single GPU (HBM) is 1600 GB/s, so NVLink provides roughly 40\% of intra-GPU bandwidth for inter-GPU communication.

The impact of interconnect bandwidth is most visible during gradient synchronization in data parallel training. After computing gradients on each GPU's local batch, all GPUs must exchange and average their gradients through an all-reduce operation. With PCIe, this communication can take longer than the backward pass itself for models with hundreds of millions of parameters. With NVLink, communication overhead is typically 10-20\% of total training time.

\subsection{Data Parallelism and Gradient Synchronization}

Data parallelism replicates the model on each GPU and processes different batches on each device. After the backward pass, gradients are averaged across all GPUs using an all-reduce collective operation. The communication volume is equal to the model size, independent of batch size, making data parallelism most efficient for large batch sizes where computation time dominates communication time.

\begin{example}[Data Parallel Scaling Efficiency]
\label{ex:data_parallel_scaling}
Training BERT-large (340M parameters) with batch size 32 per GPU on A100 GPUs:

\textbf{Single GPU:}
\begin{itemize}
    \item Forward + backward time: 145 ms
    \item Throughput: 221 sequences/second
\end{itemize}

\textbf{4 GPUs with NVLink:}
\begin{itemize}
    \item Forward + backward time: 145 ms (unchanged)
    \item Gradient all-reduce time: 18 ms
    \item Total time per step: 163 ms
    \item Throughput: 785 sequences/second
    \item Scaling efficiency: 89\% (ideal would be 884 seq/s)
\end{itemize}

\textbf{8 GPUs with NVLink:}
\begin{itemize}
    \item Forward + backward time: 145 ms
    \item Gradient all-reduce time: 22 ms
    \item Total time per step: 167 ms
    \item Throughput: 1533 sequences/second
    \item Scaling efficiency: 87\%
\end{itemize}

The high scaling efficiency demonstrates that NVLink bandwidth is sufficient for data parallel training of BERT-large. With PCIe, the all-reduce time would be approximately 85 ms, reducing scaling efficiency to 63\% for 8 GPUs.
\end{example}

\subsection{Pipeline and Tensor Parallelism}

For models too large to fit on a single GPU, pipeline parallelism splits the model across GPUs by layers, while tensor parallelism splits individual layers across GPUs. Pipeline parallelism has lower communication requirements but suffers from pipeline bubbles where some GPUs are idle. Tensor parallelism requires more communication (activations must be exchanged between layers) but maintains better GPU utilization.

Modern training frameworks like Megatron-LM combine data, pipeline, and tensor parallelism to train models with hundreds of billions of parameters. For example, training a 175B parameter model might use 8-way tensor parallelism, 8-way pipeline parallelism, and 32-way data parallelism across 2048 GPUs.

\subsection{Overlapping Communication and Computation}

Advanced implementations overlap gradient communication with backward pass computation. As soon as gradients for one layer are computed, they can begin synchronizing while the backward pass continues on earlier layers. This technique, called gradient bucketing, can hide most communication overhead when sufficient computation is available to overlap.

PyTorch's DistributedDataParallel automatically implements gradient bucketing, grouping parameters into buckets of approximately 25MB and launching all-reduce operations as soon as each bucket's gradients are ready. This optimization is particularly effective for large models where the backward pass takes much longer than gradient synchronization.

\section{Inference Optimization}
\label{sec:inference_optimization}

\subsection{ONNX Runtime}

ONNX (Open Neural Network Exchange) provides a framework-agnostic representation for neural networks, enabling models trained in PyTorch, TensorFlow, or other frameworks to be deployed using a unified inference engine. ONNX Runtime, the corresponding execution engine, applies a suite of graph-level optimizations that can substantially improve inference performance compared to the original training framework.

The ONNX format represents a neural network as a computational graph where nodes correspond to operations (such as matrix multiplication, layer normalization, or activation functions) and edges represent data flow between operations. This explicit graph representation enables analysis and transformation at a level of abstraction above individual framework implementations. Once a model is exported to ONNX format, it becomes independent of the training framework, allowing deployment in environments where PyTorch or TensorFlow may not be available or desirable.

ONNX Runtime's optimization pipeline applies several transformations to improve performance. Operator fusion combines sequences of operations into single, more efficient kernels. For example, a layer normalization followed by an addition (residual connection) can be fused into a single kernel that performs both operations in one pass, eliminating intermediate memory writes. Constant folding evaluates operations whose inputs are known at graph construction time, replacing them with their computed results and eliminating runtime computation. Dead code elimination removes operations whose outputs are never used, which can occur after other optimizations or when exporting models with unused branches.

Graph optimization goes beyond individual operations to restructure the computational graph for better performance. This includes reordering operations to enable better fusion opportunities, inserting explicit memory layout transformations to ensure optimal data formats for hardware accelerators, and selecting specialized kernel implementations based on input shapes and hardware capabilities. For transformer models, ONNX Runtime includes optimizations specifically targeting attention mechanisms, layer normalization, and embedding operations.

The combination of these optimizations typically delivers 1.5--2$\times$ speedup over PyTorch inference for transformer models, even before applying quantization or other model-level optimizations. When combined with INT8 quantization, ONNX Runtime can achieve 3--4$\times$ speedup, making it a popular choice for production deployment where inference efficiency is critical. See Chapter~\ref{chap:pytorchimplementation} for ONNX export implementation code.

\subsection{TensorRT}

NVIDIA TensorRT represents a comprehensive optimization framework specifically designed for deep learning inference on NVIDIA GPUs. Unlike ONNX Runtime's framework-agnostic approach, TensorRT leverages detailed knowledge of NVIDIA GPU architectures to extract maximum performance through hardware-specific optimizations.

At its core, TensorRT performs aggressive layer fusion, combining multiple operations into single, highly optimized CUDA kernels. These fused kernels are not generic implementations but are generated specifically for the target GPU architecture and the specific layer dimensions in the model. For example, TensorRT can fuse a matrix multiplication, bias addition, layer normalization, and GELU activation into a single kernel that processes data in registers and shared memory without writing intermediate results to global memory. This level of fusion goes beyond what general-purpose frameworks can achieve because TensorRT generates custom code for each specific combination of operations and dimensions.

Kernel auto-tuning is another key feature that distinguishes TensorRT. For each operation in the model, TensorRT maintains a library of multiple kernel implementations with different tile sizes, thread block configurations, and memory access patterns. During the optimization phase, TensorRT benchmarks these variants on the target hardware with the actual input dimensions from the model, selecting the fastest implementation for each operation. This empirical approach ensures optimal performance across different model architectures and hardware generations without requiring manual tuning.

INT8 calibration in TensorRT automates the post-training quantization process. Given a calibration dataset, TensorRT analyzes activation distributions to determine optimal scale factors for each layer, using sophisticated algorithms that minimize quantization error. The calibration process can use different strategies, from simple min-max scaling to more advanced entropy-based methods that better preserve information content. TensorRT then generates INT8 kernels that leverage Tensor Cores for maximum throughput.

The performance benefits are substantial. For BERT-base inference, TensorRT typically delivers 2-3× speedup over PyTorch even in FP16 mode, thanks to its aggressive fusion and kernel optimization. When INT8 quantization is applied, the speedup increases to 4-5× compared to PyTorch FP32, combining the benefits of reduced precision with optimized execution. These improvements make TensorRT the preferred choice for latency-critical applications on NVIDIA hardware, though the optimization process requires more setup time than ONNX Runtime and the resulting optimized models are hardware-specific.

\subsection{Batching Strategies}

The way requests are grouped into batches has a profound impact on both latency and throughput in transformer inference systems. Different batching strategies represent different points in the latency-throughput trade-off space, and the optimal choice depends on the specific requirements of the application.

Static batching uses a fixed batch size determined at deployment time. Incoming requests are accumulated until the batch is full, at which point the entire batch is processed together. Any remaining slots in the batch are filled with padding to reach the fixed size. This approach is simple to implement and provides predictable performance characteristics, but it is wasteful in several ways. Padding tokens consume memory and computation without contributing to useful work, particularly problematic when sequence lengths vary widely. Additionally, requests that arrive just after a batch is dispatched must wait for the entire next batch to fill, leading to variable and potentially high latency.

Dynamic batching improves upon static batching by using flexible batch sizes and timeout-based dispatch. Rather than waiting for a fixed number of requests, the system accumulates requests for a specified timeout period (for example, 10-50 milliseconds) and then processes whatever batch has accumulated. This approach reduces average latency by ensuring that requests are not delayed indefinitely waiting for a batch to fill, while still achieving good throughput by batching multiple requests together when load is high. The timeout parameter provides a tunable knob to balance latency and throughput: shorter timeouts reduce latency but may result in smaller batches and lower throughput, while longer timeouts increase batching opportunities at the cost of higher latency.

Continuous batching, also known as iteration-level batching, is specifically designed for autoregressive generation tasks where sequences are produced token by token. In traditional batching, all sequences in a batch must complete before any can be returned, even if some sequences finish early (for example, by generating an end-of-sequence token). Continuous batching addresses this inefficiency by allowing new sequences to join the batch as others complete. After each generation step, finished sequences are removed from the batch and new sequences are added, maintaining high GPU utilization throughout the generation process. This approach is particularly effective for applications like chatbots or code generation where sequence lengths vary unpredictably and maintaining consistent throughput is important. Modern serving systems like vLLM implement continuous batching as a core feature, achieving substantially higher throughput than traditional batching approaches for generative workloads.

\subsection{Inference Pipelines and Serving Architectures}

Inference optimization is not solely a matter of faster kernels; it is also a systems problem. The end-to-end latency and throughput of a transformer service depend on how requests are batched, how KV caches are managed, and how computation is distributed across processes and machines. A well-designed serving architecture exploits both model-level techniques (KV caching, quantization, pruning, distillation) and system-level mechanisms (asynchronous scheduling, autoscaling, and load balancing) to approach hardware limits in realistic workloads.

A typical deployment exposes a stateless HTTP or gRPC API backed by one or more worker processes, each owning one or more GPUs. Incoming requests are placed into per-model queues, where a scheduler forms dynamic batches that trade off latency and throughput. During the prefill phase, the model processes the full prompt, constructs the KV cache, and emits the first token; during the decode phase, subsequent tokens are generated one step at a time, reusing the KV cache. Techniques such as prefix sharing and speculative decoding further increase effective batch sizes by amortizing compute across similar or partially overlapping prompts.

KV cache management is a central bottleneck in LLM serving. Each sequence requires a cache that grows linearly with context length, and naive allocation leads to fragmentation and low GPU memory utilization. Recent work has proposed eviction and compression policies that operate during both prefill and decode, allowing larger batches within a fixed memory budget while preserving accuracy. Production systems routinely combine such cache compression with quantization of the KV tensors themselves, reducing both memory footprint and data-movement costs along the critical path.

At the node level, inference processes should be carefully pinned to GPUs, with explicit management of CUDA streams to overlap host-to-device transfers with computation. Kernel fusion and custom attention kernels like FlashAttention reduce memory traffic, while mixed precision and INT8 paths ensure that Tensor Cores operate near their peak throughput. Profiling tools such as Nsight Compute and framework-level profilers help identify whether the deployment is compute-bound, memory-bound, or scheduler-bound, and guide decisions about batch size, maximum context length, and concurrency limits.

\subsection{vLLM and PagedAttention}

vLLM is a production-grade LLM serving system designed to maximize throughput by managing KV cache memory with near-zero waste. Its core contribution is PagedAttention, an attention kernel and memory layout inspired by virtual memory in operating systems: instead of storing each sequence's KV cache in a single contiguous buffer, vLLM partitions the cache into fixed-size ``KV blocks'' and maintains a block table that maps each sequence's logical positions to physical blocks in GPU memory. This indirection layer decouples the logical view of a sequence from the physical layout of its cache, enabling fine-grained sharing and compaction.

During inference, new tokens for a request are appended by allocating additional KV blocks as needed and updating the block table, without relocating existing data. When a request finishes or a portion of its context becomes unnecessary, its blocks can be recycled immediately for other sequences, dramatically reducing external fragmentation compared to naive tensor allocations. From the model's perspective, attention is computed via PagedAttention kernels that follow the block table to gather keys and values, so the numerical results are identical to standard attention even though the underlying memory is non-contiguous.

This design has two critical consequences. First, vLLM can support much larger effective batch sizes under a fixed memory budget because it avoids both internal and external fragmentation of the KV cache. Second, it can efficiently batch together requests with heterogeneous decoding algorithms---such as greedy decoding, beam search, and parallel sampling---by treating all of them uniformly at the level of KV blocks. Empirical evaluations show that vLLM delivers 2--4× higher throughput than earlier systems like FasterTransformer at similar latency, with gains increasing for longer contexts and larger models.

vLLM also integrates model-parallelism strategies, including tensor and pipeline parallelism, to serve models that do not fit on a single GPU. In these configurations, tensor parallelism shards the model's layers across GPUs, while the KV cache for each request is partitioned or replicated according to the parallelism strategy. Care must be taken when sharding attention heads, since naive tensor parallelism can require duplicating KV cache across devices; recent work has introduced group-wise attention layouts and expert parallelism schemes that reduce duplication by aligning KV partitions with head groups or experts. Together, these techniques make vLLM a concrete instantiation of the hardware and memory optimization principles discussed throughout this chapter.

\subsection{Distributed Inference with Ray and Kubernetes}

When inference workloads exceed the capacity of a single machine, they are typically deployed on a cluster orchestrated by Kubernetes, with a serving framework such as Ray Serve handling request routing and autoscaling. In this architecture, a Ray cluster runs inside the Kubernetes cluster: a head pod manages cluster state, and worker pods host Ray processes that in turn manage GPU-backed model replicas. Kubernetes is responsible for provisioning and recovering pods, while Ray's own autoscaler adjusts the number of replicas and Ray workers based on application-level metrics such as request rate and queue length.

A Ray Serve deployment encapsulates a single model or ensemble behind a logical endpoint. Clients send HTTP or gRPC requests to a gateway, which forwards them to Ray Serve, where a router assigns them to replicas according to a configurable policy (for example, random, weighted, or load-aware). Within each replica, the model can implement dynamic batching, KV caching, and other optimizations described earlier, so that local GPU utilization remains high even when request arrivals are bursty or heterogeneous. Ray Serve's autoscaler monitors metrics such as in-flight requests and latency and scales the number of replicas between configured minima and maxima, triggering Kubernetes to allocate or deallocate pods and underlying nodes as necessary.

Designing such a system requires coordinating multiple layers of scaling. At the lowest level, each replica should be configured with an appropriate concurrency limit and maximum batch size to ensure that GPU memory is not oversubscribed, particularly with large KV caches. At the Ray layer, the autoscaler must be tuned to react quickly to sustained load while avoiding oscillations; at the Kubernetes layer, node autoscaling policies (for example, via cluster-autoscaler or Karpenter) must be aligned with Ray's resource requests to ensure that new GPU nodes are provisioned in time. Observability across these layers---via dashboards for Ray, Kubernetes, and the cloud provider---is essential for diagnosing issues such as stalled scaling, replica imbalance, or unexpected OOM events.

From the perspective of this chapter, the key point is that model-level optimizations and cluster-level architecture are tightly coupled. Quantization and pruning reduce per-replica memory footprint and cost, enabling higher replica density per node; vLLM's PagedAttention reduces KV cache fragmentation, allowing larger batch sizes; and careful scheduling and autoscaling with Ray and Kubernetes expose these gains to real-world, multi-tenant workloads. An effective deployment strategy therefore treats hardware, kernels, caching mechanisms, and orchestration as parts of a single, integrated system rather than independent concerns.

\section{Production Deployment}
\label{sec:production_deployment}

\subsection{Serving Frameworks}

Production deployment of transformer models requires robust serving infrastructure that handles request routing, batching, versioning, and monitoring. Several frameworks have emerged to address these needs, each with different design philosophies and trade-offs.

TorchServe provides native serving capabilities for PyTorch models, offering tight integration with the PyTorch ecosystem. Models can be packaged with their preprocessing and postprocessing logic into model archives that encapsulate all dependencies. TorchServe exposes both REST and gRPC APIs for inference requests, with built-in support for dynamic batching to improve throughput. The framework includes model versioning capabilities, allowing multiple versions of a model to be served simultaneously and enabling gradual rollout of new versions. Monitoring and logging are integrated, providing metrics on request latency, throughput, and error rates. For organizations already using PyTorch for training, TorchServe offers a natural deployment path with minimal friction.

Triton Inference Server takes a multi-framework approach, supporting models from PyTorch, TensorFlow, ONNX, and TensorRT within a single serving infrastructure. This flexibility is valuable in heterogeneous environments where different models may have been developed using different frameworks. Triton's key strength is its sophisticated scheduling and execution engine, which can execute multiple models concurrently on the same GPU, dynamically batch requests across models, and even create model ensembles where the output of one model feeds into another. The framework includes backends optimized for different hardware platforms, automatically selecting appropriate execution strategies based on the target device. Triton's dynamic batching is particularly advanced, supporting variable-size batching with configurable timeout and queue policies.

FastAPI combined with custom serving logic represents a lightweight alternative for organizations that need full control over their serving infrastructure. FastAPI provides a modern Python web framework with automatic API documentation, request validation, and asynchronous request handling. By building custom serving logic on top of FastAPI, teams can implement exactly the batching, caching, and optimization strategies their application requires without being constrained by framework limitations. This approach requires more development effort but offers maximum flexibility for specialized use cases. It is particularly popular for research deployments and applications with unique requirements that don't fit standard serving frameworks.

\subsection{Deployment Checklist}

Successful production deployment of transformer models requires attention to multiple dimensions beyond raw inference speed. This checklist organizes key considerations into three categories: performance optimization, reliability engineering, and operational monitoring.

\textbf{Performance optimization} focuses on maximizing throughput and minimizing latency within resource constraints. Quantization to INT8 or FP16 should be applied whenever accuracy requirements permit, as it provides substantial speedup with minimal implementation complexity. Exporting models to ONNX or TensorRT enables graph-level optimizations and hardware-specific acceleration that training frameworks cannot provide. Batch size must be carefully tuned to balance latency and throughput: larger batches improve GPU utilization and throughput but increase latency for individual requests. For autoregressive generation tasks, enabling KV caching is essential, as it eliminates redundant computation of attention keys and values for previously generated tokens, typically providing 2-3× speedup for generation workloads.

\textbf{Reliability engineering} ensures that the serving system degrades gracefully under adverse conditions rather than failing catastrophically. Error handling should distinguish between transient failures (such as temporary resource exhaustion) and permanent failures (such as malformed inputs), with appropriate retry logic for transient cases. Request timeouts prevent a single slow request from blocking resources indefinitely, with timeout values chosen based on application requirements and typical inference times. Health checks enable load balancers and orchestration systems to detect unhealthy instances and route traffic away from them, typically implemented as lightweight endpoints that verify model loading and basic inference capability. Model versioning allows safe deployment of new model versions through canary releases or A/B testing, with the ability to quickly roll back if issues are detected.

\textbf{Monitoring and observability} provide visibility into system behavior and enable rapid diagnosis of issues. Latency metrics should be tracked at multiple percentiles (p50, p95, p99) rather than just averages, as tail latencies often reveal problems that averages obscure. Throughput measured in requests per second indicates overall system capacity and helps identify when scaling is needed. GPU utilization shows whether the system is compute-bound or has headroom for additional load, with consistently low utilization suggesting inefficient batching or other optimization opportunities. Error rates broken down by error type help distinguish between client errors (such as invalid inputs) and server errors (such as out-of-memory conditions), guiding troubleshooting efforts. These metrics should be collected continuously and visualized in dashboards that enable both real-time monitoring and historical analysis.

\section{Hardware Selection and Cost Analysis}
\label{sec:hardware_selection}

Selecting appropriate hardware for transformer workloads requires balancing performance, cost, and operational requirements. This section provides guidance for different use cases and scales.

\subsection{CPU vs GPU Trade-offs}

The choice between CPU and GPU depends on model size, batch size, latency requirements, and cost constraints. CPUs excel at low-latency inference with small batch sizes, while GPUs provide superior throughput for larger batches and are essential for training.

CPUs are the appropriate choice for several specific scenarios. Small models with fewer than 100 million parameters running with batch sizes of 1-4 can achieve acceptable performance on modern CPUs without the overhead of GPU memory transfers. Latency-critical applications requiring response times below 10 milliseconds may benefit from CPU deployment, as the overhead of transferring data to and from GPU memory can dominate total latency for small workloads. Cost-sensitive deployments with low throughput requirements can leverage CPU instances that cost 3-5× less than GPU instances, making them more economical when the lower throughput is acceptable. Edge deployment scenarios where GPU hardware is unavailable or impractical, such as mobile devices or embedded systems, necessitate CPU inference.

For example, a distilled BERT model with 66 million parameters can achieve 15 millisecond latency on a modern CPU (Intel Xeon or AMD EPYC) with batch size 1. The same model on a T4 GPU achieves 8 millisecond latency but requires batching to amortize GPU overhead, making it less suitable for single-request scenarios where latency is paramount.

GPUs become the clear choice for several categories of workloads. Training any transformer model, regardless of size, benefits dramatically from GPU acceleration due to the massive parallelism in backpropagation and gradient computation. Large models exceeding 100 million parameters achieve substantially higher throughput on GPUs even for inference. Batch inference with batch sizes greater than 8 can fully utilize GPU compute resources, delivering throughput that CPUs cannot match. Throughput-oriented applications where processing many requests per second is more important than individual request latency should use GPUs to maximize overall system capacity.

A BERT-large model with 340 million parameters illustrates the GPU advantage for larger models: it achieves only 2.5 sequences per second on CPU but 45 sequences per second on a T4 GPU with batch size 16, demonstrating an 18× throughput advantage. This performance gap widens further for even larger models.

Cost analysis reveals that while cloud GPU instances cost approximately 3-5× more than equivalent CPU instances, the higher throughput often makes GPUs more cost-effective per inference. For BERT-large inference, a T4 GPU instance costs \$0.35 per hour and processes 162,000 sequences per hour, yielding \$0.0000022 per sequence. A CPU instance costs \$0.10 per hour and processes 9,000 sequences per hour, yielding \$0.000011 per sequence, making the GPU 5× more cost-effective despite the higher instance cost.

Energy efficiency considerations also favor GPUs for large models. The T4 GPU consumes 70 watts and processes 45 sequences per second, yielding 0.64 sequences per second per watt. A high-end CPU consumes 200 watts and processes 2.5 sequences per second, yielding 0.0125 sequences per second per watt, making the GPU 51× more energy-efficient. This efficiency advantage becomes increasingly important at scale and in environmentally conscious deployments.

\subsection{Training Hardware Selection}

Training requirements scale dramatically with model size, from single GPUs for small models to thousands of GPUs for the largest models.

For small models with fewer than 1 billion parameters, a single high-end GPU typically suffices. A V100 with 32GB of memory or an A100 with 40GB can accommodate most models in this category along with reasonable batch sizes. Training time remains manageable: BERT-base trained on a 16GB dataset completes in approximately 3 days on a V100. Cloud costs range from \$2.50 per hour for V100 instances to \$4.00 per hour for A100 instances, making this scale accessible for research projects, fine-tuning tasks, and domain-specific model development.

Medium models ranging from 1 to 10 billion parameters require multi-GPU configurations to achieve reasonable training times and accommodate model size. A typical setup uses 4-8 A100 GPUs with 40GB or 80GB memory each, connected via NVLink for efficient gradient synchronization. Training time scales accordingly: GPT-2 with 1.5 billion parameters trains in approximately 2 weeks on 8 A100 GPUs. Cloud costs reach \$32 per hour for an 8-GPU A100 instance, positioning this scale for production model development and large-scale fine-tuning projects. NVLink connectivity is essential at this scale, as it enables 85-90\% scaling efficiency through its 600 GB/s bandwidth, whereas PCIe would reduce efficiency to 60-70\%.

Large models spanning 10 to 100 billion parameters demand substantial infrastructure with 16-64 A100 GPUs equipped with 80GB memory each. These systems require both NVLink for intra-node communication and InfiniBand for inter-node connectivity to maintain training efficiency. The scale of these training runs is substantial: GPT-3 with 175 billion parameters consumed approximately 10,000 V100-days of compute. Cloud costs range from \$128 to \$256 per hour for 32-64 GPU configurations. These models require pipeline and tensor parallelism in addition to data parallelism to distribute both computation and memory across the cluster. This scale is appropriate for foundation model development and large-scale pretraining efforts.

Extreme models exceeding 100 billion parameters represent the frontier of current capabilities, requiring hundreds to thousands of A100 GPUs distributed across multiple nodes. Training times extend to months even on these large clusters, and full pretraining costs reach millions of dollars. InfiniBand provides 200-400 Gb/s bandwidth between nodes, enabling efficient multi-node training through sophisticated parallelism strategies. This scale is currently limited to state-of-the-art foundation models such as GPT-4 and PaLM, developed by organizations with substantial computational resources.

\subsection{Inference Hardware Selection}

Inference requirements vary widely based on latency, throughput, and cost constraints, leading to different optimal hardware choices for different deployment scenarios.

Batch inference optimized for throughput prioritizes processing large volumes of requests efficiently rather than minimizing individual request latency. A100 or A10 GPUs excel in this regime, processing large batches of 32-128 sequences simultaneously to maximize GPU utilization. This approach is ideal for offline processing tasks, data pipelines, and batch prediction workloads where results are not needed immediately. For example, BERT-large processes 520 sequences per second on an A100 with batch size 64, demonstrating the throughput achievable when latency constraints are relaxed.

Low-latency inference targets real-time applications where response time is critical. T4 or A10 GPUs combined with TensorRT optimization provide the best balance of latency and cost for this use case. These deployments use small batches of 1-8 sequences to minimize queuing delays and leverage TensorRT's aggressive kernel fusion and INT8 quantization to reduce computation time. Interactive systems, real-time applications, and user-facing services typically fall into this category. BERT-base achieves 5 millisecond latency on a T4 GPU with batch size 1 using INT8 quantization, meeting the requirements of most interactive applications.

Cost-optimized inference prioritizes minimizing operational expenses while maintaining acceptable performance. CPU instances or small GPUs like the T4 provide the most economical deployment options when combined with quantized models and efficient batching strategies. High-volume, cost-sensitive applications such as content moderation, spam detection, or large-scale classification tasks benefit from this approach. A distilled BERT model with 66 million parameters running on CPU costs approximately \$0.000005 per inference, making it viable for applications processing billions of requests per month.

Edge deployment brings inference to devices with limited computational resources, such as mobile phones, embedded systems, or IoT devices. Mobile CPUs, edge TPUs, or NVIDIA Jetson modules provide the necessary compute capability within tight power and cost budgets. Models must be heavily optimized through INT8 or INT4 quantization and aggressive pruning to fit within memory constraints and achieve acceptable latency. MobileBERT with 25 million parameters runs at 30 milliseconds on mobile CPUs, enabling on-device inference for privacy-sensitive applications or scenarios with limited connectivity.

\subsection{Hardware Selection Decision Tree}

The following decision tree provides guidance for hardware selection based on workload characteristics and requirements.

The first decision point distinguishes between training and inference workloads, as they have fundamentally different characteristics. Training requires backpropagation and gradient computation, which benefit dramatically from GPU parallelism regardless of model size. Inference can sometimes be performed efficiently on CPUs, particularly for small models with low throughput requirements. If the workload involves training, proceed to evaluate model size; if it involves inference, proceed to evaluate latency requirements.

For training workloads, model size determines the appropriate hardware configuration. Models with fewer than 1 billion parameters can typically be trained on a single V100 or A100 GPU, providing a cost-effective solution for research and fine-tuning. Models ranging from 1 to 10 billion parameters require 4-8 A100 GPUs with NVLink to achieve reasonable training times and accommodate model size. Models spanning 10 to 100 billion parameters demand 16-64 A100 GPUs with both NVLink and InfiniBand for efficient multi-node training. Models exceeding 100 billion parameters require hundreds to thousands of GPUs and are currently limited to organizations with substantial computational resources.

Budget constraints provide a secondary consideration for training hardware. Research projects and organizations with limited budgets may opt for V100 or A10 GPUs, which offer lower cost at the expense of reduced performance. Production deployments and performance-critical applications typically justify the higher cost of A100 GPUs, which provide superior throughput and memory capacity.

For inference workloads, latency requirements drive the initial hardware selection. Applications requiring response times below 10 milliseconds should consider T4 GPUs with TensorRT optimization or CPUs for small models, as these configurations minimize the overhead of data movement and kernel launch. Applications with latency requirements between 10 and 50 milliseconds can use T4 or A10 GPUs with moderate batch sizes to balance latency and throughput. Applications with latency requirements exceeding 50 milliseconds have flexibility to choose hardware based primarily on cost considerations.

Throughput requirements provide the final consideration for inference hardware. Applications processing fewer than 10 sequences per second can use CPU instances cost-effectively, as the lower throughput is sufficient and GPU overhead is not justified. Applications processing 10-100 sequences per second benefit from T4 GPUs, which provide good throughput at moderate cost. Applications processing more than 100 sequences per second require A10 or A100 GPUs to achieve the necessary throughput, with the choice between them depending on budget and specific latency requirements.

\section{Exercises}
\label{sec:exercises}

\begin{exercise}
Quantize BERT-base to INT8:
\begin{enumerate}
    \item Use PyTorch quantization APIs
    \item Calibrate on 1000 examples
    \item Measure: (a) Model size, (b) Inference speed, (c) GLUE accuracy
    \item Compare PTQ vs QAT
\end{enumerate}
\end{exercise}

\begin{exercise}
Implement attention head pruning:
\begin{enumerate}
    \item Compute importance scores for all heads
    \item Prune 25\%, 50\%, 75\% of heads
    \item Fine-tune after pruning
    \item Plot accuracy vs sparsity
\end{enumerate}
\end{exercise}

\begin{exercise}
Optimize inference pipeline:
\begin{enumerate}
    \item Baseline: PyTorch FP32
    \item Convert to ONNX, measure speedup
    \item Apply INT8 quantization
    \item Implement dynamic batching
    \item Report final throughput improvement
\end{enumerate}
\end{exercise}


\begin{exercise}
Analyze Tensor Core utilization:
\begin{enumerate}
    \item Profile BERT-base training with FP32 and FP16
    \item Measure Tensor Core utilization using NVIDIA Nsight Compute
    \item Experiment with different batch sizes and sequence lengths
    \item Identify which operations benefit most from Tensor Cores
    \item Calculate achieved TFLOPS as percentage of theoretical peak
\end{enumerate}
\end{exercise}

\begin{exercise}
Implement and benchmark kernel fusion:
\begin{enumerate}
    \item Create separate kernels for layer norm and dropout
    \item Implement a fused layer norm + dropout kernel
    \item Measure memory bandwidth utilization for both approaches
    \item Compare execution time across different tensor sizes
    \item Analyze the speedup and explain the performance difference
\end{enumerate}
\end{exercise}

\begin{exercise}
Optimize memory access patterns:
\begin{enumerate}
    \item Implement matrix multiplication with and without coalesced access
    \item Add padding to eliminate shared memory bank conflicts
    \item Profile both implementations using Nsight Compute
    \item Measure effective memory bandwidth for each version
    \item Document the impact of access patterns on performance
\end{enumerate}
\end{exercise}

\begin{exercise}
Multi-GPU scaling analysis:
\begin{enumerate}
    \item Train BERT-base on 1, 2, 4, and 8 GPUs
    \item Measure training time and throughput for each configuration
    \item Calculate scaling efficiency relative to single GPU
    \item Profile communication overhead using NVIDIA Nsight Systems
    \item Compare PCIe vs NVLink if available
\end{enumerate}
\end{exercise}

\begin{exercise}
Hardware selection analysis:
\begin{enumerate}
    \item Choose a transformer model and deployment scenario
    \item Estimate throughput requirements and latency constraints
    \item Compare cost per inference for CPU, T4, and A100
    \item Calculate break-even point where GPU becomes cost-effective
    \item Recommend hardware configuration with justification
\end{enumerate}
\end{exercise}

\begin{exercise}
Flash Attention implementation study:
\begin{enumerate}
    \item Implement standard attention with separate kernels
    \item Analyze memory traffic for different sequence lengths
    \item Study Flash Attention paper and implementation
    \item Benchmark Flash Attention vs standard attention
    \item Plot speedup as a function of sequence length
\end{enumerate}
\end{exercise}


\section{Solutions}

Full solutions for all exercises are available at \url{https://deeplearning.hofkensvermeulen.be}.

\begin{solution}
\textbf{Exercise 1: Quantize BERT-base to INT8}

\textbf{Results:}

\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method} & \textbf{Size (MB)} & \textbf{Speed (ms)} & \textbf{GLUE Acc} \\
\hline
FP32 Baseline & 438 & 45.2 & 84.5\% \\
PTQ INT8 & 110 & 18.3 & 83.8\% \\
QAT INT8 & 110 & 18.1 & 84.2\% \\
\hline
\end{tabular}

\textbf{Analysis:}
\begin{itemize}
    \item \textbf{Model size:} 4x reduction (438 MB $\to$ 110 MB)
    \item \textbf{Speed:} 2.5x faster inference
    \item \textbf{PTQ:} 0.7\% accuracy drop, no retraining
    \item \textbf{QAT:} 0.3\% accuracy drop, requires fine-tuning
\end{itemize}

\textbf{Recommendation:} Use QAT for production (better accuracy with same speed/size)
\end{solution}

\begin{solution}
\textbf{Exercise 2: Attention Head Pruning}

\textbf{Results:}

\begin{tabular}{|c|c|c|}
\hline
\textbf{Pruning \%} & \textbf{Params Remaining} & \textbf{Accuracy} \\
\hline
0\% (baseline) & 110M & 84.5\% \\
25\% & 82.5M & 84.1\% \\
50\% & 55M & 82.8\% \\
75\% & 27.5M & 78.3\% \\
\hline
\end{tabular}

\textbf{Key Insight:} Can prune 25-50\% of heads with minimal accuracy loss (<2\%), but 75\% pruning causes significant degradation.
\end{solution}

\begin{solution}
\textbf{Exercise 3: Optimize Inference Pipeline}

\textbf{Progressive Optimization:}

\begin{tabular}{|l|c|c|}
\hline
\textbf{Stage} & \textbf{Latency (ms)} & \textbf{Throughput (samples/s)} \\
\hline
PyTorch FP32 & 45.2 & 22.1 \\
+ ONNX & 32.8 & 30.5 \\
+ INT8 Quant & 14.7 & 68.0 \\
+ Dynamic Batch & 12.3 & 162.4 \\
\hline
\end{tabular}

\textbf{Final Improvement:} 7.3x throughput increase (22.1 $\to$ 162.4 samples/s)
\end{solution}



\begin{solution}
\textbf{Exercise 4: Tensor Core Utilization}

\textbf{Results (A100 GPU):}

\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Config} & \textbf{TC Util} & \textbf{TFLOPS} & \textbf{\% Peak} \\
\hline
FP32, batch=16 & 0\% & 8.2 & 5.3\% \\
FP16, batch=16 & 45\% & 89.4 & 28.7\% \\
FP16, batch=64 & 78\% & 187.3 & 60.1\% \\
FP16, batch=128 & 85\% & 214.6 & 68.9\% \\
\hline
\end{tabular}

\textbf{Key Findings:}
\begin{itemize}
    \item Attention and MLP benefit most from Tensor Cores
    \item Larger batches improve utilization (more parallelism)
    \item FP16 essential for Tensor Core activation
    \item Achieved 68.9\% of theoretical peak (good for transformers)
\end{itemize}
\end{solution}

\begin{solution}
\textbf{Exercise 5: Kernel Fusion}

\textbf{Results:}

\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Implementation} & \textbf{Time (μs)} & \textbf{Bandwidth (GB/s)} & \textbf{Speedup} \\
\hline
Separate Kernels & 142 & 385 & 1.0x \\
Fused Kernel & 58 & 943 & 2.45x \\
\hline
\end{tabular}

\textbf{Analysis:}
\begin{itemize}
    \item Fused kernel eliminates intermediate memory writes
    \item 2.45x speedup from reduced memory traffic
    \item Bandwidth utilization improves from 385 to 943 GB/s
    \item Most beneficial for memory-bound operations
\end{itemize}
\end{solution}

\begin{solution}
\textbf{Exercise 6: Memory Access Patterns}

\textbf{Results:}

\begin{tabular}{|l|c|c|}
\hline
\textbf{Implementation} & \textbf{Bandwidth (GB/s)} & \textbf{Speedup} \\
\hline
Uncoalesced & 287 & 1.0x \\
Coalesced & 823 & 2.87x \\
+ No Bank Conflicts & 1,142 & 3.98x \\
\hline
\end{tabular}

\textbf{Key Insight:} Proper memory access patterns provide 4x speedup through coalescing and eliminating bank conflicts.
\end{solution}

\begin{solution}
\textbf{Exercise 7: Multi-GPU Scaling}

\textbf{Results:}

\begin{tabular}{|c|c|c|c|}
\hline
\textbf{GPUs} & \textbf{Time (min)} & \textbf{Throughput} & \textbf{Efficiency} \\
\hline
1 & 240 & 1.0x & 100\% \\
2 & 126 & 1.90x & 95.2\% \\
4 & 67 & 3.58x & 89.6\% \\
8 & 36 & 6.67x & 83.3\% \\
\hline
\end{tabular}

\textbf{Communication Overhead:}
\begin{itemize}
    \item PCIe: 12-15\% overhead
    \item NVLink: 5-8\% overhead (2x better)
\end{itemize}

\textbf{Recommendation:} NVLink for multi-GPU training (better scaling)
\end{solution}

\begin{solution}
\textbf{Exercise 8: Hardware Selection}

\textbf{Scenario:} BERT-base inference, 1M requests/day, <50ms latency

\textbf{Cost Analysis:}

\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Hardware} & \textbf{Throughput} & \textbf{Cost/hour} & \textbf{Cost/1M inferences} \\
\hline
CPU (32 cores) & 45 req/s & \$1.20 & \$7.41 \\
T4 GPU & 180 req/s & \$0.35 & \$0.54 \\
A100 GPU & 650 req/s & \$2.50 & \$1.07 \\
\hline
\end{tabular}

\textbf{Break-even:} T4 becomes cost-effective at >10k requests/day

\textbf{Recommendation:} T4 GPU (best cost/performance for this workload)
\end{solution}

