\chapter{Hardware Optimization and Deployment}
\label{chap:hardware_optimization}

\section*{Chapter Overview}

Deploying transformers efficiently requires understanding hardware architectures, optimization techniques, and deployment strategies. This chapter covers GPUs, TPUs, model quantization, pruning, distillation, and production deployment best practices.

\subsection*{Learning Objectives}

\begin{enumerate}
    \item Understand GPU/TPU architectures for transformers
    \item Apply model quantization (INT8, FP16)
    \item Implement pruning and sparsity
    \item Use knowledge distillation for compression
    \item Optimize inference latency and throughput
    \item Deploy models in production environments
\end{enumerate}

\section{Hardware Architectures}
\label{sec:hardware_architectures}

\subsection{GPU Architecture for Deep Learning}

Modern GPUs contain two primary types of compute units that are critical for transformer training and inference. CUDA cores are general-purpose floating-point units that can execute arbitrary arithmetic operations, while Tensor Cores are specialized matrix multiplication units designed specifically for deep learning workloads. Understanding the distinction between these units is essential for achieving optimal performance.

\textbf{CUDA Cores vs Tensor Cores.} CUDA cores provide flexibility for general computation but operate at lower throughput for matrix operations. A single NVIDIA A100 GPU contains 6912 CUDA cores capable of 19.5 TFLOPS at FP32 precision. In contrast, Tensor Cores are specialized hardware units that perform fused multiply-add operations on small matrix tiles. The same A100 GPU contains 432 third-generation Tensor Cores that deliver 312 TFLOPS for FP16 matrix multiplication, representing a 16× advantage over CUDA cores for this specific operation. This dramatic performance difference makes Tensor Cores essential for transformer workloads, which are dominated by matrix multiplications in attention mechanisms and feed-forward layers.

\textbf{Memory Hierarchy.} GPU memory is organized in a hierarchy that trades capacity for access speed. At the top of the hierarchy, each streaming multiprocessor (SM) contains registers that provide the fastest access with approximately 256KB of storage per SM. These registers are private to individual threads and have single-cycle latency. The next level is shared memory, a software-managed cache that allows threads within a block to communicate efficiently. The A100 provides 164KB of shared memory per SM with latency of approximately 20-30 cycles. Below this sits the L2 cache, a 40MB hardware-managed cache shared across all SMs with latency around 200 cycles. Finally, high-bandwidth memory (HBM) provides the largest capacity at 40-80GB but with the highest latency of 300-400 cycles. This hierarchy means that keeping data in faster memory levels is critical for performance.

The memory bandwidth available at each level determines how quickly data can be moved. The A100's HBM provides 1.6 TB/s of bandwidth, while the V100 provides 900 GB/s. Although these numbers seem large, they are often the bottleneck for transformer operations. Consider that the A100's Tensor Cores can consume data at a rate of 312 TFLOPS × 2 bytes (FP16) = 624 TB/s if fully utilized, far exceeding the 1.6 TB/s that HBM can supply. This mismatch between compute capability and memory bandwidth is why memory optimization is crucial for transformers.

\textbf{Streaming Multiprocessors.} The A100 contains 108 streaming multiprocessors, each capable of executing multiple thread blocks concurrently. Each SM has its own register file, shared memory, and L1 cache, along with 4 Tensor Cores. The SM scheduler can switch between thread warps (groups of 32 threads) with zero overhead, hiding memory latency by executing other warps while some wait for data. Achieving high occupancy, defined as the ratio of active warps to maximum possible warps, is essential for hiding latency and maximizing throughput.

\subsection{Computational Intensity}

\begin{definition}[Arithmetic Intensity]
\label{def:arithmetic_intensity}
\begin{equation}
\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes transferred}}
\end{equation}
\end{definition}

\textbf{For attention:}
\begin{itemize}
    \item $\mQ \mK\transpose$: Intensity = $\frac{2n^2d}{2nd + 2nd} = \frac{nd}{2d} = \frac{n}{2}$
    \item For small $n$ (< 1024): Memory-bound
    \item For large $n$ (> 4096): Compute-bound
\end{itemize}

\subsection{Tensor Core Optimization}

Tensor Cores achieve their peak performance only when specific conditions are met regarding data types, matrix dimensions, and memory layout. Understanding these requirements is essential for extracting maximum performance from modern GPUs.

\textbf{Precision Requirements.} Tensor Cores support several precision modes, each with different performance characteristics. FP16 (half precision) provides 312 TFLOPS on the A100, making it the standard choice for training. BF16 (bfloat16) offers the same throughput but with a larger dynamic range that better matches FP32, reducing the need for loss scaling during mixed-precision training. For inference, INT8 provides 624 TOPS, doubling throughput at the cost of reduced precision. The choice of precision involves trading off between speed, memory usage, and numerical accuracy.

\textbf{Dimension Requirements.} Tensor Cores operate on small matrix tiles and achieve peak performance when matrix dimensions are multiples of specific values. For FP16 operations, dimensions should be multiples of 8, while BF16 and INT8 operations prefer multiples of 16. When dimensions are not multiples of these values, the hardware must pad matrices or fall back to slower execution paths. For example, a matrix multiplication with dimensions 1023×1023 will perform significantly worse than 1024×1024 because the former requires padding or partial tile operations.

\begin{example}[BERT-base Tensor Core Utilization]
\label{ex:bert_tensor_core}
Consider BERT-base with hidden size $d=768$ and 12 attention heads. The per-head dimension is $d_k = 768/12 = 64$, which is a multiple of 8, allowing efficient Tensor Core usage. The query, key, and value projections have shape $[b \times n, 768] \times [768, 768]$ where $b$ is batch size and $n$ is sequence length.

\textbf{Baseline configuration:} Batch size 16, sequence length 128, FP32 precision
\begin{itemize}
    \item Throughput: 145 sequences/second
    \item GPU utilization: 42\%
    \item Memory bandwidth: 890 GB/s (56\% of peak)
\end{itemize}

\textbf{Optimized configuration:} Batch size 32, sequence length 128, FP16 with Tensor Cores
\begin{itemize}
    \item Throughput: 520 sequences/second (3.6× improvement)
    \item GPU utilization: 78\%
    \item Memory bandwidth: 1.45 TB/s (91\% of peak)
    \item Tensor Core utilization: 85\%
\end{itemize}

The optimization involved: (1) switching to FP16 to enable Tensor Cores, (2) increasing batch size to improve arithmetic intensity, and (3) ensuring all matrix dimensions are multiples of 8. The result is a 3.6× speedup with negligible accuracy loss when using mixed-precision training with loss scaling.
\end{example}

\textbf{Achieving Peak Performance.} Reaching 90\% of theoretical peak TFLOPS requires careful attention to several factors. First, ensure sufficient work is available by using large batch sizes or long sequences to keep all Tensor Cores busy. Second, minimize memory transfers by fusing operations and reusing data in shared memory. Third, maintain high occupancy by using appropriate thread block sizes and avoiding resource limitations. Finally, profile the application to identify bottlenecks using tools like NVIDIA Nsight Compute, which can show Tensor Core utilization and identify whether operations are compute-bound or memory-bound.

\subsection{TPU Architecture}

\textbf{Tensor Processing Units (TPUs):}
\begin{itemize}
    \item Systolic array architecture
    \item Optimized for matrix multiplications
    \item High memory bandwidth (900 GB/s, TPU v4)
    \item 275 TFLOPS (bfloat16)
\end{itemize}

\textbf{TPU vs GPU:}
\begin{table}[htbp]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{GPU} & \textbf{TPU} \\
\midrule
Flexibility & High (general purpose) & Medium (ML-specific) \\
Peak FLOPS & 312 (A100 FP16) & 275 (v4 bf16) \\
Memory & 40-80 GB & 32 GB (per chip) \\
Batch size & Medium-Large & Very Large \\
Best for & Flexibility, research & Large-scale training \\
\bottomrule
\end{tabular}
\end{table}

\section{Memory Optimization Techniques}
\label{sec:memory_optimization}

Memory access patterns have a profound impact on GPU performance, often determining whether an operation runs at 10\% or 90\% of peak throughput. This section explores the key memory optimization techniques that are essential for efficient transformer implementations.

\subsection{Coalesced Memory Access}

When threads in a warp access global memory, the hardware attempts to combine these accesses into a single transaction. Coalesced access occurs when consecutive threads access consecutive memory locations, allowing the hardware to issue one memory transaction instead of 32 separate ones. For example, if thread 0 accesses address 0, thread 1 accesses address 4, thread 2 accesses address 8, and so on (assuming 4-byte elements), the hardware can coalesce these into a single 128-byte transaction. In contrast, if threads access random or strided locations, each access may require a separate transaction, reducing effective bandwidth by up to 32×.

For transformer operations, coalesced access is particularly important in matrix multiplications and attention computations. When loading a row of the query matrix, ensuring that consecutive threads load consecutive elements allows full utilization of memory bandwidth. This often requires careful consideration of matrix layout (row-major vs column-major) and access patterns in custom CUDA kernels.

\subsection{Shared Memory and Bank Conflicts}

Shared memory is divided into 32 banks that can be accessed simultaneously. When multiple threads in a warp access the same bank but different addresses, a bank conflict occurs, serializing the accesses and reducing throughput. The A100's shared memory has 32 banks with 4-byte bank width, meaning addresses that differ by 128 bytes map to the same bank.

In attention implementations, shared memory is commonly used to cache tiles of the query, key, and value matrices. Careful padding of these tiles can eliminate bank conflicts. For example, if each thread block loads a 64×64 tile of FP16 values, adding 8 elements of padding to each row ensures that consecutive rows start at different banks, eliminating conflicts when threads access columns.

\begin{example}[Shared Memory Optimization in Attention]
\label{ex:shared_memory_attention}
Consider computing attention scores $\mA = \mQ\mK\transpose$ where $\mQ, \mK \in \mathbb{R}^{n \times d}$. A naive implementation loads tiles of $\mQ$ and $\mK$ into shared memory and computes partial results.

\textbf{Unoptimized approach:}
\begin{itemize}
    \item Tile size: 64×64 FP16 values
    \item Shared memory per tile: $64 \times 64 \times 2 = 8192$ bytes
    \item Bank conflicts when accessing columns: 32-way conflicts
    \item Effective bandwidth: 50 GB/s (3\% of peak)
\end{itemize}

\textbf{Optimized approach with padding:}
\begin{itemize}
    \item Tile size: 64×72 FP16 values (8 elements padding per row)
    \item Shared memory per tile: $64 \times 72 \times 2 = 9216$ bytes
    \item No bank conflicts: consecutive rows in different banks
    \item Effective bandwidth: 1.4 TB/s (88\% of peak)
\end{itemize}

The 12.5\% increase in shared memory usage eliminates bank conflicts and increases bandwidth by 28×, demonstrating the critical importance of memory access patterns.
\end{example}

\subsection{Memory Bandwidth Utilization}

Maximizing memory bandwidth requires ensuring that memory operations are large enough to amortize transaction overhead and that the GPU has sufficient concurrent operations to hide latency. Small transfers are inefficient because they cannot fully utilize the 32-byte or 128-byte cache line sizes. Additionally, launching enough thread blocks to keep all memory controllers busy is essential for achieving peak bandwidth.

For transformers, memory bandwidth is often the limiting factor during attention computation with short sequences. When sequence length $n < 1024$, the arithmetic intensity of attention is low, meaning each floating-point operation requires loading relatively more data from memory. Techniques like Flash Attention address this by restructuring computations to maximize data reuse in shared memory, reducing the number of global memory accesses.

\section{Kernel Fusion and Operation Optimization}
\label{sec:kernel_fusion}

Kernel fusion combines multiple operations into a single GPU kernel, reducing memory traffic and kernel launch overhead. This technique is particularly effective for transformers, where many operations are memory-bound and benefit from data reuse.

\subsection{Fusion Opportunities in Transformers}

Standard transformer implementations launch separate kernels for each operation, requiring intermediate results to be written to and read from global memory. For example, computing layer normalization followed by dropout requires writing the normalized values to memory, then reading them back for the dropout operation. Fusing these operations allows the normalized values to remain in registers or shared memory, eliminating the round-trip to global memory.

\textbf{Common fusion patterns:}
\begin{enumerate}
    \item \textbf{Layer norm + dropout:} Normalize activations and apply dropout in a single pass, keeping intermediate values in registers.
    \item \textbf{GELU + bias:} Compute the GELU activation and add bias without storing intermediate results.
    \item \textbf{Attention score + softmax + dropout:} Compute $\text{softmax}(\mQ\mK\transpose/\sqrt{d_k})$ and apply dropout in one kernel.
    \item \textbf{Residual + layer norm:} Add residual connection and normalize in a single operation.
\end{enumerate}

\begin{example}[Layer Norm + Dropout Fusion]
\label{ex:layernorm_dropout_fusion}
Consider layer normalization followed by dropout on a tensor of shape $[32, 128, 768]$ (batch size 32, sequence length 128, hidden size 768).

\textbf{Unfused implementation:}
\begin{itemize}
    \item Layer norm kernel: Read 3.1M elements, write 3.1M elements
    \item Dropout kernel: Read 3.1M elements, write 3.1M elements
    \item Total memory traffic: 12.4M elements = 24.8 MB (FP16)
    \item Execution time: 0.18 ms
\end{itemize}

\textbf{Fused implementation:}
\begin{itemize}
    \item Single kernel: Read 3.1M elements, write 3.1M elements
    \item Total memory traffic: 6.2M elements = 12.4 MB (FP16)
    \item Execution time: 0.10 ms
\end{itemize}

The fused kernel achieves 1.8× speedup by halving memory traffic and eliminating kernel launch overhead. For a 12-layer transformer, this fusion appears 24 times per forward pass (twice per layer), providing substantial cumulative savings.
\end{example}

\subsection{Flash Attention: Fused Attention Implementation}

Flash Attention represents a sophisticated application of kernel fusion to the attention mechanism. Standard attention implementations compute $\mA = \mQ\mK\transpose$, write $\mA$ to memory, read it back for softmax, write the result, read it again for multiplication with $\mV$, and finally write the output. This results in $O(n^2)$ memory reads and writes where $n$ is sequence length.

Flash Attention restructures the computation to work on tiles that fit in shared memory. It computes attention for one tile at a time, keeping intermediate results in fast memory and only writing the final output. This reduces memory traffic from $O(n^2)$ to $O(n)$, providing dramatic speedups for long sequences.

\begin{example}[Flash Attention Performance]
\label{ex:flash_attention_perf}
BERT-base with sequence length 512, batch size 16, on A100 GPU:

\textbf{Standard attention:}
\begin{itemize}
    \item Memory traffic: 48 GB per forward pass
    \item Execution time: 8.2 ms
    \item Memory bandwidth utilization: 45\%
\end{itemize}

\textbf{Flash Attention:}
\begin{itemize}
    \item Memory traffic: 12 GB per forward pass (4× reduction)
    \item Execution time: 3.8 ms (2.2× speedup)
    \item Memory bandwidth utilization: 82\%
\end{itemize}

For longer sequences, the benefits are even more pronounced. At sequence length 2048, Flash Attention provides 3.5× speedup, and at 8192, it provides 5.2× speedup while also enabling sequences that would otherwise exceed memory capacity.
\end{example}

\subsection{Implementing Fused Kernels}

Creating fused kernels requires careful consideration of register usage, shared memory capacity, and thread block dimensions. The goal is to maximize data reuse while maintaining high occupancy. Modern deep learning frameworks provide tools for kernel fusion, including PyTorch's JIT compiler and TensorRT's graph optimizer, which can automatically fuse compatible operations. For custom fusion patterns, libraries like CUTLASS provide templates for efficient CUDA implementations.

\section{Model Quantization}
\label{sec:quantization}

\subsection{Quantization Fundamentals}

\begin{definition}[Quantization]
\label{def:quantization}
Map FP32 weights to lower precision (INT8, FP16):
\begin{equation}
w_{\text{quant}} = \text{round}\left(\frac{w_{\text{float}}}{s}\right) + z
\end{equation}
where $s$ is scale factor, $z$ is zero-point.
\end{definition}

\textbf{Precision options:}
\begin{itemize}
    \item \textbf{FP32:} 32 bits, full precision (baseline)
    \item \textbf{FP16:} 16 bits, 2× compression
    \item \textbf{BF16:} 16 bits, better range than FP16
    \item \textbf{INT8:} 8 bits, 4× compression
    \item \textbf{INT4:} 4 bits, 8× compression (extreme)
\end{itemize}

\subsection{Post-Training Quantization (PTQ)}

\textbf{Procedure:}
\begin{enumerate}
    \item Train model in FP32
    \item Collect activation statistics on calibration set
    \item Determine scale factors
    \item Convert weights and activations to INT8
\end{enumerate}

\begin{example}[INT8 Quantization]
\label{ex:int8_quantization}
\textbf{FP32 weight:} $w = 0.137$

\textbf{Determine range:} $w \in [-1.0, 1.0]$

\textbf{Scale:} $s = \frac{2.0}{256} = 0.0078125$

\textbf{Quantize:}
\begin{equation}
w_{\text{INT8}} = \text{round}\left(\frac{0.137}{0.0078125}\right) = \text{round}(17.54) = 18
\end{equation}

\textbf{Dequantize:} $w' = 18 \times 0.0078125 = 0.1406$

\textbf{Error:} $|0.137 - 0.1406| = 0.0036$ (2.6\% relative)
\end{example}

\subsection{Quantization-Aware Training (QAT)}

\textbf{Simulate quantization during training:}
\begin{enumerate}
    \item Forward pass: Quantize weights/activations
    \item Compute loss with quantized values
    \item Backward pass: FP32 gradients
    \item Update FP32 weights
\end{enumerate}

\textbf{Benefits:}
\begin{itemize}
    \item Model learns to be robust to quantization
    \item Better accuracy than PTQ
    \item Minimal accuracy loss with INT8
\end{itemize}

\begin{example}[BERT-base Quantization Results]
\label{ex:bert_quantization}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Precision} & \textbf{GLUE Score} & \textbf{Speedup} \\
\midrule
FP32 (baseline) & 84.5 & 1.0× \\
FP16 & 84.4 & 1.8× \\
INT8 (PTQ) & 82.1 & 2.9× \\
INT8 (QAT) & 84.2 & 2.9× \\
\bottomrule
\end{tabular}
\end{center}

QAT recovers most accuracy lost in PTQ!
\end{example}

\section{Model Pruning}
\label{sec:pruning}

\subsection{Pruning Strategies}

\textbf{Magnitude-based pruning:}
\begin{equation}
\text{Prune if } |w_{ij}| < \tau
\end{equation}

\textbf{Structured pruning:}
\begin{itemize}
    \item Remove entire neurons, heads, layers
    \item Easier to deploy (no sparse kernels needed)
    \item Less aggressive compression
\end{itemize}

\textbf{Unstructured pruning:}
\begin{itemize}
    \item Remove individual weights
    \item Higher compression ratios
    \item Requires sparse matrix operations
\end{itemize}

\subsection{Iterative Pruning}

\begin{algorithm}[H]
\caption{Iterative Magnitude Pruning}
\label{alg:iterative_pruning}

\textbf{Input:} Model, sparsity target $s_{\text{target}}$

\For{sparsity $s = 0$ \KwTo $s_{\text{target}}$ by steps}{
    Train model to convergence \\
    Prune $\Delta s$ lowest-magnitude weights \\
    Fine-tune model
}
\end{algorithm}

\begin{example}[Attention Head Pruning]
\label{ex:head_pruning}
BERT-base: 12 layers × 12 heads = 144 heads

\textbf{Finding:} Can remove 50\% of heads with minimal impact!

\textbf{Procedure:}
\begin{enumerate}
    \item Compute importance score per head
    \item Rank heads by importance
    \item Prune lowest 50\% (72 heads)
    \item Fine-tune remaining model
\end{enumerate}

\textbf{Result:}
\begin{itemize}
    \item 50\% fewer attention operations
    \item GLUE score: 84.5 $\to$ 83.8 (0.7 point drop)
    \item 1.5× faster inference
\end{itemize}
\end{example}

\section{Knowledge Distillation}
\label{sec:knowledge_distillation_detail}

\subsection{Distillation Loss}

\begin{definition}[Distillation Objective]
\label{def:distillation_objective}
\begin{equation}
\mathcal{L} = \alpha \mathcal{L}_{\text{CE}}(y, y_{\text{student}}) + (1-\alpha) \mathcal{L}_{\text{KD}}(y_{\text{teacher}}, y_{\text{student}})
\end{equation}

where:
\begin{equation}
\mathcal{L}_{\text{KD}} = \text{KL}\left(\frac{\exp(z_t/T)}{\sum \exp(z_t/T)} \Big\| \frac{\exp(z_s/T)}{\sum \exp(z_s/T)}\right)
\end{equation}

$T$ = temperature (typically 2-5), higher = softer probabilities
\end{definition}

\subsection{DistilBERT Approach}

\textbf{Student architecture:}
\begin{itemize}
    \item 6 layers (vs 12 in BERT)
    \item Same hidden size (768)
    \item Initialize from teacher's even layers
\end{itemize}

\textbf{Training:}
\begin{itemize}
    \item Distillation loss from teacher
    \item Masked language modeling loss
    \item Cosine distance between hidden states
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item 40\% smaller (66M vs 110M params)
    \item 60\% faster
    \item Retains 97\% of BERT performance
\end{itemize}

\section{Multi-GPU Training and Optimization}
\label{sec:multi_gpu}

Training large transformer models requires distributing computation across multiple GPUs. The efficiency of multi-GPU training depends critically on communication bandwidth, parallelization strategy, and the balance between computation and communication.

\subsection{Interconnect Technologies}

The bandwidth between GPUs determines how quickly gradients, activations, and parameters can be exchanged during distributed training. PCIe provides 16-32 GB/s bidirectional bandwidth per GPU, which is adequate for small models but becomes a bottleneck for large transformers. NVLink, NVIDIA's proprietary interconnect, provides 600 GB/s bidirectional bandwidth on A100 systems, enabling much more efficient multi-GPU training. For comparison, the bandwidth within a single GPU (HBM) is 1600 GB/s, so NVLink provides roughly 40\% of intra-GPU bandwidth for inter-GPU communication.

The impact of interconnect bandwidth is most visible during gradient synchronization in data parallel training. After computing gradients on each GPU's local batch, all GPUs must exchange and average their gradients through an all-reduce operation. With PCIe, this communication can take longer than the backward pass itself for models with hundreds of millions of parameters. With NVLink, communication overhead is typically 10-20\% of total training time.

\subsection{Data Parallelism and Gradient Synchronization}

Data parallelism replicates the model on each GPU and processes different batches on each device. After the backward pass, gradients are averaged across all GPUs using an all-reduce collective operation. The communication volume is equal to the model size, independent of batch size, making data parallelism most efficient for large batch sizes where computation time dominates communication time.

\begin{example}[Data Parallel Scaling Efficiency]
\label{ex:data_parallel_scaling}
Training BERT-large (340M parameters) with batch size 32 per GPU on A100 GPUs:

\textbf{Single GPU:}
\begin{itemize}
    \item Forward + backward time: 145 ms
    \item Throughput: 221 sequences/second
\end{itemize}

\textbf{4 GPUs with NVLink:}
\begin{itemize}
    \item Forward + backward time: 145 ms (unchanged)
    \item Gradient all-reduce time: 18 ms
    \item Total time per step: 163 ms
    \item Throughput: 785 sequences/second
    \item Scaling efficiency: 89\% (ideal would be 884 seq/s)
\end{itemize}

\textbf{8 GPUs with NVLink:}
\begin{itemize}
    \item Forward + backward time: 145 ms
    \item Gradient all-reduce time: 22 ms
    \item Total time per step: 167 ms
    \item Throughput: 1533 sequences/second
    \item Scaling efficiency: 87\%
\end{itemize}

The high scaling efficiency demonstrates that NVLink bandwidth is sufficient for data parallel training of BERT-large. With PCIe, the all-reduce time would be approximately 85 ms, reducing scaling efficiency to 63\% for 8 GPUs.
\end{example}

\subsection{Pipeline and Tensor Parallelism}

For models too large to fit on a single GPU, pipeline parallelism splits the model across GPUs by layers, while tensor parallelism splits individual layers across GPUs. Pipeline parallelism has lower communication requirements but suffers from pipeline bubbles where some GPUs are idle. Tensor parallelism requires more communication (activations must be exchanged between layers) but maintains better GPU utilization.

Modern training frameworks like Megatron-LM combine data, pipeline, and tensor parallelism to train models with hundreds of billions of parameters. For example, training a 175B parameter model might use 8-way tensor parallelism, 8-way pipeline parallelism, and 32-way data parallelism across 2048 GPUs.

\subsection{Overlapping Communication and Computation}

Advanced implementations overlap gradient communication with backward pass computation. As soon as gradients for one layer are computed, they can begin synchronizing while the backward pass continues on earlier layers. This technique, called gradient bucketing, can hide most communication overhead when sufficient computation is available to overlap.

PyTorch's DistributedDataParallel automatically implements gradient bucketing, grouping parameters into buckets of approximately 25MB and launching all-reduce operations as soon as each bucket's gradients are ready. This optimization is particularly effective for large models where the backward pass takes much longer than gradient synchronization.

\section{Inference Optimization}
\label{sec:inference_optimization}

\subsection{ONNX Runtime}

\textbf{ONNX (Open Neural Network Exchange):}
\begin{itemize}
    \item Framework-agnostic model format
    \item Optimized inference engine
    \item Supports quantization, pruning
\end{itemize}

\textbf{Optimizations:}
\begin{itemize}
    \item Operator fusion (combine LayerNorm + Add)
    \item Constant folding
    \item Dead code elimination
    \item Graph optimization
\end{itemize}

\subsection{TensorRT}

\textbf{NVIDIA TensorRT:}
\begin{itemize}
    \item Deep learning inference optimizer
    \item Layer fusion
    \item Kernel auto-tuning
    \item INT8 calibration
\end{itemize}

\textbf{Typical speedups:}
\begin{itemize}
    \item BERT-base: 2-3× over PyTorch
    \item With INT8: 4-5× over PyTorch FP32
\end{itemize}

\subsection{Batching Strategies}

\textbf{Static batching:}
\begin{itemize}
    \item Fixed batch size
    \item Pad to max length
    \item Simple but wasteful
\end{itemize}

\textbf{Dynamic batching:}
\begin{itemize}
    \item Accumulate requests until batch full or timeout
    \item Reduces latency while maintaining throughput
\end{itemize}

\textbf{Continuous batching:}
\begin{itemize}
    \item For autoregressive generation
    \item Add new sequences as others finish
    \item Maximizes GPU utilization
\end{itemize}

\section{Production Deployment}
\label{sec:production_deployment}

\subsection{Serving Frameworks}

\textbf{TorchServe:}
\begin{itemize}
    \item PyTorch native serving
    \item REST/gRPC APIs
    \item Batching, versioning, monitoring
\end{itemize}

\textbf{Triton Inference Server:}
\begin{itemize}
    \item Multi-framework (PyTorch, TensorFlow, ONNX)
    \item Concurrent model execution
    \item Dynamic batching
    \item Model ensembles
\end{itemize}

\textbf{FastAPI + Custom:}
\begin{itemize}
    \item Lightweight, flexible
    \item Full control over serving logic
    \item Easy integration with existing systems
\end{itemize}

\subsection{Deployment Checklist}

\textbf{Performance:}
\begin{itemize}
    \item Quantize to INT8/FP16
    \item Export to ONNX/TensorRT
    \item Optimize batch size for latency/throughput
    \item Enable KV caching for generation
\end{itemize}

\textbf{Reliability:}
\begin{itemize}
    \item Graceful degradation on errors
    \item Request timeouts
    \item Health checks
    \item Model versioning
\end{itemize}

\textbf{Monitoring:}
\begin{itemize}
    \item Latency (p50, p95, p99)
    \item Throughput (requests/second)
    \item GPU utilization
    \item Error rates
\end{itemize}

\section{Hardware Selection and Cost Analysis}
\label{sec:hardware_selection}

Selecting appropriate hardware for transformer workloads requires balancing performance, cost, and operational requirements. This section provides guidance for different use cases and scales.

\subsection{CPU vs GPU Trade-offs}

The choice between CPU and GPU depends on model size, batch size, latency requirements, and cost constraints. CPUs excel at low-latency inference with small batch sizes, while GPUs provide superior throughput for larger batches and are essential for training.

\textbf{When to use CPUs:}
\begin{itemize}
    \item Small models (< 100M parameters) with batch size 1-4
    \item Latency-critical applications requiring < 10ms response time
    \item Cost-sensitive deployments with low throughput requirements
    \item Edge deployment where GPU hardware is unavailable
\end{itemize}

For example, a distilled BERT model with 66M parameters can achieve 15ms latency on a modern CPU (Intel Xeon or AMD EPYC) with batch size 1. The same model on a T4 GPU achieves 8ms latency but requires batching to amortize GPU overhead, making it less suitable for single-request scenarios.

\textbf{When to use GPUs:}
\begin{itemize}
    \item Training any transformer model
    \item Large models (> 100M parameters)
    \item Batch inference with batch size > 8
    \item Throughput-oriented applications
\end{itemize}

A BERT-large model (340M parameters) achieves 2.5 sequences/second on CPU but 45 sequences/second on a T4 GPU with batch size 16, demonstrating the GPU's 18× throughput advantage for larger models.

\textbf{Cost analysis.} Cloud GPU instances cost approximately 3-5× more than equivalent CPU instances. However, the higher throughput often makes GPUs more cost-effective per inference. For BERT-large inference, a T4 GPU instance costs \$0.35/hour and processes 162,000 sequences/hour, yielding \$0.0000022 per sequence. A CPU instance costs \$0.10/hour and processes 9,000 sequences/hour, yielding \$0.000011 per sequence, making the GPU 5× more cost-effective despite higher instance cost.

\textbf{Energy efficiency.} GPUs also provide better energy efficiency for large models. The T4 GPU consumes 70W and processes 45 sequences/second, yielding 0.64 sequences/second/watt. A high-end CPU consumes 200W and processes 2.5 sequences/second, yielding 0.0125 sequences/second/watt, making the GPU 51× more energy-efficient.

\subsection{Training Hardware Selection}

Training requirements scale dramatically with model size, from single GPUs for small models to thousands of GPUs for the largest models.

\textbf{Small models (< 1B parameters):}
\begin{itemize}
    \item Hardware: Single V100 (32GB) or A100 (40GB)
    \item Training time: BERT-base on 16GB dataset trains in 3 days on V100
    \item Cost: \$2.50/hour (V100) to \$4.00/hour (A100) on cloud
    \item Use case: Research, fine-tuning, domain-specific models
\end{itemize}

\textbf{Medium models (1-10B parameters):}
\begin{itemize}
    \item Hardware: 4-8× A100 (40GB or 80GB) with NVLink
    \item Training time: GPT-2 (1.5B) trains in 2 weeks on 8× A100
    \item Cost: \$32/hour for 8× A100 on cloud
    \item Use case: Production models, large-scale fine-tuning
\end{itemize}

For these models, NVLink is essential for efficient data parallelism. The 600 GB/s NVLink bandwidth enables 85-90\% scaling efficiency, while PCIe would reduce efficiency to 60-70\%.

\textbf{Large models (10-100B parameters):}
\begin{itemize}
    \item Hardware: 16-64× A100 (80GB) with NVLink and InfiniBand
    \item Training time: GPT-3 (175B) trained on 10,000× V100-days
    \item Cost: \$128-256/hour for 32-64× A100 on cloud
    \item Use case: Foundation models, large-scale pretraining
\end{itemize}

These models require pipeline and tensor parallelism in addition to data parallelism. InfiniBand provides 200-400 Gb/s bandwidth between nodes, enabling efficient multi-node training.

\textbf{Extreme models (100B+ parameters):}
\begin{itemize}
    \item Hardware: 100s-1000s of A100 GPUs across multiple nodes
    \item Training time: Months on large clusters
    \item Cost: Millions of dollars for full pretraining
    \item Use case: State-of-the-art foundation models (GPT-4, PaLM)
\end{itemize}

\subsection{Inference Hardware Selection}

Inference requirements vary widely based on latency, throughput, and cost constraints.

\textbf{Batch inference (throughput-oriented):}
\begin{itemize}
    \item Hardware: A100 or A10 GPUs
    \item Characteristics: Process large batches (32-128), optimize for throughput
    \item Use case: Offline processing, data pipelines, batch predictions
    \item Example: BERT-large processes 520 sequences/second on A100 with batch size 64
\end{itemize}

\textbf{Low-latency inference:}
\begin{itemize}
    \item Hardware: T4 or A10 GPUs with TensorRT
    \item Characteristics: Small batches (1-8), optimize for latency
    \item Use case: Real-time applications, interactive systems
    \item Example: BERT-base achieves 5ms latency on T4 with batch size 1 using INT8
\end{itemize}

\textbf{Cost-optimized inference:}
\begin{itemize}
    \item Hardware: CPU or small GPUs (T4)
    \item Characteristics: Quantized models, efficient batching
    \item Use case: High-volume, cost-sensitive applications
    \item Example: Distilled BERT (66M params) on CPU costs \$0.000005 per inference
\end{itemize}

\textbf{Edge deployment:}
\begin{itemize}
    \item Hardware: Mobile CPUs, edge TPUs, or NVIDIA Jetson
    \item Characteristics: Heavily quantized (INT8/INT4), pruned models
    \item Use case: On-device inference, privacy-sensitive applications
    \item Example: MobileBERT (25M params) runs at 30ms on mobile CPU
\end{itemize}

\subsection{Hardware Selection Decision Tree}

The following decision tree provides guidance for hardware selection:

\begin{enumerate}
    \item \textbf{Training or inference?}
    \begin{itemize}
        \item Training: Proceed to step 2
        \item Inference: Proceed to step 4
    \end{itemize}
    
    \item \textbf{Model size?}
    \begin{itemize}
        \item < 1B params: Single V100/A100
        \item 1-10B params: 4-8× A100 with NVLink
        \item 10-100B params: 16-64× A100 with InfiniBand
        \item > 100B params: 100s-1000s of GPUs
    \end{itemize}
    
    \item \textbf{Budget constraints?}
    \begin{itemize}
        \item Research/limited budget: V100 or A10
        \item Production/performance-critical: A100
    \end{itemize}
    
    \item \textbf{Latency requirements?}
    \begin{itemize}
        \item < 10ms: T4 GPU with TensorRT or CPU for small models
        \item 10-50ms: T4 or A10 GPU
        \item > 50ms: Any GPU or CPU based on cost
    \end{itemize}
    
    \item \textbf{Throughput requirements?}
    \begin{itemize}
        \item < 10 seq/s: CPU
        \item 10-100 seq/s: T4 GPU
        \item > 100 seq/s: A10 or A100 GPU
    \end{itemize}
\end{enumerate}

\section{Exercises}
\label{sec:exercises}

\begin{exercise}
Quantize BERT-base to INT8:
\begin{enumerate}
    \item Use PyTorch quantization APIs
    \item Calibrate on 1000 examples
    \item Measure: (a) Model size, (b) Inference speed, (c) GLUE accuracy
    \item Compare PTQ vs QAT
\end{enumerate}
\end{exercise}

\begin{exercise}
Implement attention head pruning:
\begin{enumerate}
    \item Compute importance scores for all heads
    \item Prune 25\%, 50\%, 75\% of heads
    \item Fine-tune after pruning
    \item Plot accuracy vs sparsity
\end{enumerate}
\end{exercise}

\begin{exercise}
Optimize inference pipeline:
\begin{enumerate}
    \item Baseline: PyTorch FP32
    \item Convert to ONNX, measure speedup
    \item Apply INT8 quantization
    \item Implement dynamic batching
    \item Report final throughput improvement
\end{enumerate}
\end{exercise}


\begin{exercise}
Analyze Tensor Core utilization:
\begin{enumerate}
    \item Profile BERT-base training with FP32 and FP16
    \item Measure Tensor Core utilization using NVIDIA Nsight Compute
    \item Experiment with different batch sizes and sequence lengths
    \item Identify which operations benefit most from Tensor Cores
    \item Calculate achieved TFLOPS as percentage of theoretical peak
\end{enumerate}
\end{exercise}

\begin{exercise}
Implement and benchmark kernel fusion:
\begin{enumerate}
    \item Create separate kernels for layer norm and dropout
    \item Implement a fused layer norm + dropout kernel
    \item Measure memory bandwidth utilization for both approaches
    \item Compare execution time across different tensor sizes
    \item Analyze the speedup and explain the performance difference
\end{enumerate}
\end{exercise}

\begin{exercise}
Optimize memory access patterns:
\begin{enumerate}
    \item Implement matrix multiplication with and without coalesced access
    \item Add padding to eliminate shared memory bank conflicts
    \item Profile both implementations using Nsight Compute
    \item Measure effective memory bandwidth for each version
    \item Document the impact of access patterns on performance
\end{enumerate}
\end{exercise}

\begin{exercise}
Multi-GPU scaling analysis:
\begin{enumerate}
    \item Train BERT-base on 1, 2, 4, and 8 GPUs
    \item Measure training time and throughput for each configuration
    \item Calculate scaling efficiency relative to single GPU
    \item Profile communication overhead using NVIDIA Nsight Systems
    \item Compare PCIe vs NVLink if available
\end{enumerate}
\end{exercise}

\begin{exercise}
Hardware selection analysis:
\begin{enumerate}
    \item Choose a transformer model and deployment scenario
    \item Estimate throughput requirements and latency constraints
    \item Compare cost per inference for CPU, T4, and A100
    \item Calculate break-even point where GPU becomes cost-effective
    \item Recommend hardware configuration with justification
\end{enumerate}
\end{exercise}

\begin{exercise}
Flash Attention implementation study:
\begin{enumerate}
    \item Implement standard attention with separate kernels
    \item Analyze memory traffic for different sequence lengths
    \item Study Flash Attention paper and implementation
    \item Benchmark Flash Attention vs standard attention
    \item Plot speedup as a function of sequence length
\end{enumerate}
\end{exercise}
