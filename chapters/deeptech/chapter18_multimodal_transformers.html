<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 18: Multimodal Transformers - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../../deeptech.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Multimodal Transformers</h1>

<h2>Chapter Overview</h2>

<p>Multimodal transformers process multiple modalities (text, images, audio, video) in a unified framework. This chapter covers vision-language models (CLIP, DALL-E), audio-text models (Whisper), and unified architectures that handle arbitrary combinations of modalities.</p>

<h3>Learning Objectives</h3>

<ol>
    <li>Understand multimodal fusion strategies
    <li>Implement contrastive learning (CLIP)
    <li>Apply vision-language models to zero-shot classification
    <li>Generate images from text (DALL-E, Stable Diffusion)
    <li>Process audio with transformers (Whisper)
    <li>Build unified multimodal models
</ol>

<h2>Multimodal Learning Fundamentals</h2>

<h3>Fusion Strategies</h3>

<p>The fundamental challenge in multimodal learning is determining how to combine information from different modalities‚Äîsuch as vision, language, and audio‚Äîinto a unified representation. The choice of fusion strategy has profound implications for model architecture, computational cost, and the types of cross-modal interactions the model can learn. Three primary approaches have emerged, each with distinct trade-offs in terms of expressiveness, efficiency, and implementation complexity.</p>

<figure>
<div class="tikz-diagram"><img src="../diagrams/chapter18_multimodal_transformers_8d2834139e81.svg" alt="TikZ Diagram" /></div>
<figcaption>Three multimodal fusion strategies. <strong>Early fusion</strong> (left): concatenates modalities and processes with unified encoder, enabling rich interactions but with quadratic cost $O((N+M)^2)$. <strong>Late fusion</strong> (center): separate encoders with fusion only at output, efficient $O(N^2 + M^2)$ but limited cross-modal interaction. <strong>Cross-modal attention</strong> (right): separate encoders with explicit cross-attention, balancing efficiency $O(N^2 + M^2 + NM)$ with rich interactions.</figcaption>
</figure>

<p><strong>Early fusion</strong> combines modalities at the input level, concatenating or interleaving embeddings from different sources before processing them through a single unified encoder. For example, an image might be encoded into a sequence of patch embeddings $\vv_1, \ldots, \vv_N \in \R^d$, while text is tokenized into embeddings $\vt_1, \ldots, \vt_M \in \R^d$. These are concatenated into a single sequence $[\vv_1, \ldots, \vv_N, \vt_1, \ldots, \vt_M]$ and processed by a standard transformer encoder. The advantage of this approach is its simplicity and the fact that cross-modal interactions can occur at every layer through self-attention. However, early fusion has significant computational drawbacks: the attention complexity is $O((N+M)^2 d)$, meaning that adding image patches dramatically increases the cost. For a ViT-Base image with 196 patches and text with 128 tokens, the combined sequence length of 324 tokens results in attention matrices of size $324 \times 324$, consuming substantial memory and compute compared to processing each modality separately.</p>

<p><strong>Late fusion</strong> takes the opposite approach, using separate encoders for each modality and combining their outputs only at the final decision stage. An image encoder produces a single embedding $\vv \in \R^d$, a text encoder produces $\vt \in \R^d$, and these are combined through simple operations like concatenation, averaging, or element-wise multiplication before a final classification layer. This approach is computationally efficient since each encoder processes only its own modality with complexity $O(N^2 d)$ for images and $O(M^2 d)$ for text, which can be computed in parallel. The total complexity is $O(N^2 d + M^2 d)$ rather than $O((N+M)^2 d)$. However, late fusion severely limits cross-modal interactions‚Äîthe encoders cannot attend to information from other modalities, and the combination happens only at the very end. This makes it difficult to learn fine-grained alignments, such as which image regions correspond to which words in a caption.</p>

<p><strong>Cross-modal attention</strong> provides a middle ground, using separate encoders but introducing explicit attention mechanisms that allow one modality to query information from another. In this architecture, an image encoder produces a sequence of patch embeddings $\mV = [\vv_1, \ldots, \vv_N] \in \R^{N \times d}$, and a text encoder produces token embeddings $\mT = [\vt_1, \ldots, \vt_M] \in \R^{M \times d}$. Additional cross-attention layers are then inserted where text tokens attend to image patches (queries from text, keys and values from image) and vice versa. The computational cost is $O(N^2 d + M^2 d + NMd)$ for the self-attention in each modality plus the cross-attention between them. The cross-attention term $NMd$ is linear in both sequence lengths, making it much more efficient than early fusion's quadratic term. For our example with 196 image patches and 128 text tokens, cross-attention requires $196 \times 128 = 25{,}088$ attention computations per head, compared to $324^2 = 104{,}976$ for early fusion‚Äîa 4√ó reduction. This approach is used in models like BLIP and Flamingo, where it enables rich cross-modal interactions while maintaining computational tractability.</p>

<h3>Alignment Objectives</h3>

<p><strong>Contrastive Learning:</strong>
<div class="equation">
$$
\mathcal{L}_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(v_i, t_i)/\tau)}{\sum_j \exp(\text{sim}(v_i, t_j)/\tau)}
$$
</div>
where $v_i$ = image embedding, $t_i$ = text embedding, $\tau$ = temperature</p>

<p><strong>Matching Loss:</strong>
<div class="equation">
$$
\mathcal{L}_{\text{match}} = -\mathbb{E}[\log P(\text{match}|v, t)]
$$
</div>

<p><strong>Reconstruction:</strong>
<div class="equation">
$$
\mathcal{L}_{\text{recon}} = \|f_{\text{dec}}(v) - t\|^2
$$
</div>

<h2>CLIP: Contrastive Language-Image Pre-training</h2>

<h3>CLIP Architecture</h3>

<p>CLIP (Contrastive Language-Image Pre-training) represents a breakthrough in vision-language learning by training image and text encoders jointly using a contrastive objective on 400 million image-text pairs collected from the internet. Unlike traditional supervised learning that requires manually labeled categories, CLIP learns to align images with their natural language descriptions, enabling zero-shot transfer to downstream tasks without any task-specific training data.</p>

<div class="definition"><strong>Definition:</strong> 
The CLIP architecture consists of three main components that work together to create a shared embedding space for images and text. The <strong>image encoder</strong> can be either a Vision Transformer (ViT) or a ResNet, which processes an input image and produces a fixed-dimensional embedding $\vv \in \R^{d}$. For the largest CLIP model (ViT-L/14), the image encoder is a ViT with patch size 14, hidden dimension 1024, 24 layers, and 16 attention heads, totaling approximately 304 million parameters. The <strong>text encoder</strong> is a transformer decoder (similar to GPT) with a context length of 77 tokens, hidden dimension 768, 12 layers, and 12 attention heads, containing roughly 63 million parameters. Both encoders are followed by learned linear <strong>projection layers</strong> that map their outputs to a shared embedding space of dimension $d = 512$, where cosine similarity can be computed directly.

<p>The training procedure processes batches of $(image, text)$ pairs simultaneously. For each batch of size $N$, all $N$ images are encoded to produce embeddings $\vv_1, \ldots, \vv_N \in \R^{512}$, and all $N$ text descriptions are encoded to produce $\vt_1, \ldots, \vt_N \in \R^{512}$. The model then computes an $N \times N$ similarity matrix where entry $(i,j)$ represents the cosine similarity between image $i$ and text $j$. The contrastive loss maximizes the similarity along the diagonal (correct image-text pairs) while minimizing off-diagonal similarities (incorrect pairings). This symmetric loss is computed in both directions‚Äîpredicting text from image and image from text‚Äîand averaged.
</div>

<p>The parameter count for CLIP varies significantly across model scales. CLIP ResNet-50 contains approximately 102 million parameters (38M for ResNet-50 image encoder, 63M for text encoder, 1M for projections), while CLIP ViT-L/14 totals around 428 million parameters (304M for ViT-L image encoder, 123M for a larger text encoder with 768 dimensions and 12 layers, 1M for projections). The largest variant, ViT-L/14@336px, processes higher-resolution images (336√ó336 instead of 224√ó224) with the same architecture, increasing computational cost but improving performance on fine-grained tasks.</p>

<div class="example"><strong>Example:</strong> 
Consider a training batch with size $N = 4$ and embedding dimension $d = 512$. The image encoder processes four images to produce embeddings arranged as rows in matrix $\mV = [\vv_1, \vv_2, \vv_3, \vv_4]\transpose \in \R^{4 \times 512}$, while the text encoder processes their corresponding captions to produce $\mT = [\vt_1, \vt_2, \vt_3, \vt_4]\transpose \in \R^{4 \times 512}$.

<p>The similarity matrix is computed as $\mS = \mV \mT\transpose \in \R^{4 \times 4}$, where each entry $S_{ij}$ represents the dot product between image embedding $i$ and text embedding $j$. To make this scale-invariant, CLIP uses cosine similarity: $S_{ij} = \frac{\vv_i \cdot \vt_j}{\|\vv_i\| \|\vt_j\|}$, which normalizes each embedding to unit length before computing the dot product. This ensures that the similarity is determined by the angle between embeddings rather than their magnitudes.</p>

<p>The contrastive loss for the image-to-text direction is computed as:
<div class="equation">
$$
\mathcal{L}_i^{\text{img}\to\text{txt}} = -\log \frac{\exp(S_{ii}/\tau)}{\sum_{j=1}^{N} \exp(S_{ij}/\tau)}
$$
</div>
where $\tau$ is a learned temperature parameter, initialized to $0.07$ and trained jointly with the model. The temperature controls the sharpness of the distribution: smaller values make the model more confident (sharper peaks), while larger values produce softer distributions. The symmetric text-to-image loss $\mathcal{L}_i^{\text{txt}\to\text{img}}$ is computed analogously by treating text as queries and images as candidates. The total loss averages both directions:
<div class="equation">
$$
\mathcal{L} = \frac{1}{2N} \sum_{i=1}^{N} (\mathcal{L}_i^{\text{img}\to\text{txt}} + \mathcal{L}_i^{\text{txt}\to\text{img}})
$$
</div>

<p>In practice, CLIP uses very large batch sizes to provide more negative examples for contrastive learning. The original CLIP was trained with batch size 32,768, requiring distributed training across multiple GPUs. With such large batches, each positive pair has 32,767 negative examples, providing a strong learning signal. However, this creates substantial memory requirements: storing the $32{,}768 \times 512$ embedding matrices for images and text requires $32{,}768 \times 512 \times 4 = 67$ MB per modality in FP32, and the $32{,}768 \times 32{,}768$ similarity matrix requires $4.3$ GB. To make this tractable, CLIP uses gradient checkpointing and distributes the batch across many GPUs, computing the similarity matrix in chunks.
</div>

<h3>Computational Analysis of CLIP Training</h3>

<p>Training CLIP at scale requires careful consideration of computational and memory costs across both the image and text encoding paths. For the ViT-L/14 image encoder processing 224√ó224 images, each image is divided into $16 \times 16 = 256$ patches of size $14 \times 14$. These patches are linearly projected to dimension 1024 and processed through 24 transformer layers. The computational cost per image is approximately $2 \times 24 \times 256^2 \times 1024 = 3.2$ GFLOPS for the attention operations (using the $2Ld^2n^2$ formula from Chapter 12) plus $2 \times 24 \times 256 \times 4 \times 1024^2 = 51.5$ GFLOPS for the feed-forward networks, totaling roughly 55 GFLOPS per image.</p>

<p>The text encoder processes sequences of up to 77 tokens through 12 transformer layers with dimension 768. The computational cost per text is approximately $2 \times 12 \times 77^2 \times 768 = 1.1$ GFLOPS for attention plus $2 \times 12 \times 77 \times 4 \times 768^2 = 4.4$ GFLOPS for feed-forward networks, totaling about 5.5 GFLOPS per text. This asymmetry‚Äîimages requiring 10√ó more compute than text‚Äîmeans that image encoding dominates the computational budget during training.</p>

<p>For a batch of 32,768 examples, the total forward pass requires approximately $32{,}768 \times (55 + 5.5) = 1{,}982{,}464$ GFLOPS or roughly 2 PFLOPS. On an NVIDIA A100 GPU with 312 TFLOPS of FP16 compute, this would take approximately 6.4 seconds per batch for the forward pass alone, not including backward propagation (which typically costs 2√ó the forward pass) or the contrastive loss computation. The full training of CLIP on 400 million image-text pairs with batch size 32,768 requires approximately $400{,}000{,}000 / 32{,}768 = 12{,}207$ batches. At roughly 20 seconds per batch (forward + backward + optimizer step), this amounts to 68 hours of continuous training on a single A100. In practice, OpenAI trained CLIP on 256 V100 GPUs for approximately 12 days, suggesting a total training cost of around 73,728 GPU-hours.</p>

<p>Memory requirements are equally demanding. Each image in the batch requires storing activations for 24 layers with 256 tokens and dimension 1024, totaling approximately $24 \times 256 \times 1024 \times 2 = 12.6$ MB per image in FP16 (the factor of 2 accounts for storing both pre- and post-activation values for backpropagation). For batch size 32,768, this amounts to 413 GB just for image activations. Text activations are smaller at approximately $12 \times 77 \times 768 \times 2 = 1.4$ MB per text, or 46 GB for the full batch. The similarity matrix requires $32{,}768 \times 32{,}768 \times 2 = 2.1$ GB in FP16. Combined with model parameters (428M parameters √ó 2 bytes = 856 MB) and optimizer states (typically 2√ó parameters for Adam), the total memory footprint exceeds 500 GB, necessitating distribution across many GPUs using techniques like ZeRO (Chapter 22) to partition optimizer states and activations.</p>

<h3>Zero-Shot Classification with CLIP</h3>

<p>One of CLIP's most remarkable capabilities is zero-shot classification: the ability to classify images into categories the model has never been explicitly trained on. This works by leveraging the natural language understanding of the text encoder to create classifiers on the fly from text descriptions. The procedure begins by creating text prompts for each class in the target classification task. For example, for a 10-class animal classification task, we might create prompts like "a photo of a dog", "a photo of a cat", "a photo of a bird", and so on. These prompts are encoded by the text encoder to produce class embeddings $\vt_1, \ldots, \vt_C \in \R^{512}$ where $C$ is the number of classes.</p>

<p>To classify a new image, we encode it with the image encoder to produce $\vv \in \R^{512}$, then compute the cosine similarity between the image embedding and each class embedding: $s_i = \frac{\vv \cdot \vt_i}{\|\vv\| \|\vt_i\|}$. The predicted class is simply $\arg\max_i s_i$, the class whose text description has the highest similarity to the image. This approach requires no training on the target dataset‚Äîthe model uses only its pre-trained knowledge of how images and text relate.</p>

<p>The performance of this zero-shot approach is surprisingly strong. CLIP ViT-L/14 achieves 76.2\% top-1 accuracy on ImageNet without ever seeing a single ImageNet training example, matching the performance of a ResNet-50 trained directly on ImageNet's 1.28 million labeled images. This demonstrates that CLIP has learned visual concepts that generalize far beyond its training distribution. Moreover, CLIP shows remarkable robustness to distribution shift: when evaluated on ImageNet variants with different image styles (sketches, cartoons, adversarial examples), CLIP's performance degrades much less than supervised models, suggesting it has learned more robust visual representations.</p>

<p>The prompt engineering aspect of zero-shot classification is crucial for performance. Simple prompts like "dog" perform worse than more descriptive prompts like "a photo of a dog". OpenAI found that using prompt ensembles‚Äîaveraging predictions across multiple prompt templates like "a photo of a \{class\}", "a picture of a \{class\}", "an image of a \{class\}"‚Äîimproves accuracy by 1-2\% by reducing sensitivity to prompt phrasing. For fine-grained classification tasks, more specific prompts help: "a photo of a \{species\}, a type of bird" outperforms "a photo of a \{species\}" for bird species classification.</p>

<h3>CLIP Variants and Training Requirements</h3>

<p>Following CLIP's success, several variants have been developed with different scales and training procedures. <strong>OpenCLIP</strong> is an open-source reproduction that has trained models ranging from small (ResNet-50 with 102M parameters) to very large (ViT-G/14 with 1.8B parameters) on datasets including LAION-400M and LAION-2B. The largest OpenCLIP models require training on clusters of 128-512 A100 GPUs for several weeks, with estimated costs exceeding \$100,000 for the full training run. The training uses mixed precision (FP16) to reduce memory consumption and enable larger batch sizes, typically 32,768 to 65,536 examples distributed across all GPUs.</p>

<p><strong>ALIGN</strong>, developed by Google, scales up the training data to 1.8 billion noisy image-text pairs collected from the web without extensive filtering. This demonstrates that contrastive learning is robust to noise in the training data‚Äîthe model learns to ignore mismatched pairs through the contrastive objective. ALIGN uses an EfficientNet-L2 image encoder (480M parameters) and a BERT-Large text encoder (340M parameters), totaling approximately 820M parameters. Training ALIGN required a cluster of 1024 Cloud TPU v3 cores for approximately 6 days, representing roughly 150,000 TPU-hours.</p>

<p><strong>Florence</strong>, Microsoft's unified vision foundation model, extends the CLIP approach to 900 million image-text pairs with a focus on creating a single model that can be adapted to diverse vision tasks. Florence uses a CoSwin transformer as the image encoder (637M parameters) and achieves state-of-the-art results on zero-shot classification, retrieval, and object detection after fine-tuning. The training infrastructure required 512 NVIDIA A100 GPUs for approximately 10 days, with an estimated cost of over \$200,000 in cloud compute.</p>

<p>The hardware requirements for training CLIP-scale models are substantial. A minimum viable setup might use 8-16 A100 GPUs (80GB each) to train a CLIP ResNet-50 model on a smaller dataset like Conceptual Captions (3M pairs) with batch size 2048-4096, requiring approximately 1-2 weeks. Scaling to the full CLIP ViT-L/14 with 400M training pairs and batch size 32,768 necessitates at least 64-128 A100 GPUs with high-bandwidth interconnects (NVLink or InfiniBand) to efficiently synchronize gradients across the distributed batch. The total training cost for reproducing CLIP ViT-L/14 is estimated at \$50,000-\$100,000 in cloud GPU costs, depending on the provider and optimization techniques employed.</p>

<h2>DALL-E and Stable Diffusion</h2>

<h3>DALL-E: Text-to-Image Generation</h3>

<div class="definition"><strong>Definition:</strong> 
<strong>DALL-E 1 (2021):</strong>
<ul>
    <li>Encoder: Compress images to discrete tokens (VQ-VAE)
    <li>Transformer: Autoregressive model over text + image tokens
    <li>Training: Next token prediction
</ul>

<p><strong>Sequence:</strong>
<div class="equation">
$$
[\text{BOS}, \text{text tokens}, \text{image tokens}, \text{EOS}]
$$
</div>

<p>Generate image by: (1) Encode text, (2) Sample image tokens autoregressively
</div>

<p><strong>DALL-E 2 (2022):</strong>
<ul>
    <li>Use CLIP embeddings
    <li>Prior: Text embedding $\to$ Image embedding
    <li>Decoder: Image embedding $\to$ Image (diffusion model)
    <li>Much higher quality than DALL-E 1
</ul>

<h3>Stable Diffusion</h3>

<p><strong>Latent Diffusion Model:</strong>
<ol>
    <li>Encode image to latent space (VAE)
    <li>Add noise iteratively (forward diffusion)
    <li>Learn to denoise (reverse diffusion)
    <li>Condition on text via cross-attention
</ol>

<p><strong>Text conditioning:</strong>
<ul>
    <li>Text encoder: CLIP or T5
    <li>Cross-attention: Latent queries attend to text keys/values
    <li>Enables text-guided image generation
</ul>

<div class="example"><strong>Example:</strong> 
<strong>Components:</strong>

<p><strong>1. Text Encoder:</strong> CLIP text encoder
<div class="equation">
$$
\text{prompt} \to \vt \in \R^{77 \times 768}
$$
</div>

<p><strong>2. VAE Encoder:</strong> Image $\to$ latent
<div class="equation">
$$
\mI \in \R^{512 \times 512 \times 3} \to \vz \in \R^{64 \times 64 \times 4}
$$
</div>

<p><strong>3. U-Net Denoiser:</strong> Diffusion model with cross-attention
<ul>
    <li>Input: Noisy latent $\vz_t$
    <li>Condition: Text embedding $\vt$
    <li>Output: Predicted noise $\epsilon_\theta(\vz_t, t, \vt)$
</ul>

<p><strong>4. VAE Decoder:</strong> Latent $\to$ image
<div class="equation">
$$
\vz \in \R^{64 \times 64 \times 4} \to \mI \in \R^{512 \times 512 \times 3}
$$
</div>

<p><strong>Parameters:</strong> $\approx 860$M total
</div>

<h2>Vision-Language Understanding</h2>

<h3>BLIP: Bootstrapped Language-Image Pre-training</h3>

<p><strong>Architecture:</strong>
<ul>
    <li>Image encoder (ViT)
    <li>Text encoder (BERT)
    <li>Multimodal encoder (cross-attention between vision and text)
</ul>

<p><strong>Training objectives:</strong>
<ol>
    <li><strong>ITC:</strong> Image-Text Contrastive (like CLIP)
    <li><strong>ITM:</strong> Image-Text Matching (binary: match or not)
    <li><strong>LM:</strong> Language Modeling on text
</ol>

<p><strong>Bootstrapping:</strong> Generate synthetic captions, filter with model, retrain</p>

<h3>Flamingo: Few-Shot Learning</h3>

<p>Flamingo represents a significant architectural innovation in multimodal transformers by enabling models to process arbitrarily interleaved sequences of images and text, supporting few-shot learning through in-context examples. Unlike CLIP, which processes single image-text pairs, Flamingo can handle inputs like "Here is an image of a cat: <code><image1></code>. Here is an image of a dog: <code><image2></code>. What animal is in this image: <code><image3></code>?" This capability enables few-shot visual learning where the model learns new tasks from just a few examples provided in the prompt, without any parameter updates.</p>

<p>The Flamingo architecture consists of three main components, carefully designed to leverage pre-trained models while adding minimal trainable parameters. The <strong>vision encoder</strong> is a frozen CLIP ViT-L/14 model that processes each image independently to produce a sequence of patch embeddings. For a 224√ó224 image with patch size 14, this yields 256 patch tokens of dimension 1024. The vision encoder's 304M parameters remain frozen throughout training, preserving the strong visual representations learned during CLIP pre-training.</p>

<p>The <strong>language model</strong> is a frozen Chinchilla 70B model, a large autoregressive transformer trained on text-only data. Chinchilla uses 70 billion parameters across 80 layers with hidden dimension 8192 and 64 attention heads. Keeping this massive language model frozen is crucial for computational tractability‚Äîtraining 70B parameters would require prohibitive memory and compute. Instead, Flamingo inserts new trainable layers that allow the frozen language model to attend to visual information without modifying its core text processing capabilities.</p>

<p>The key innovation is the <strong>Perceiver Resampler</strong>, a learned module that compresses the variable-length sequence of image patch embeddings into a fixed number of visual tokens that can be efficiently processed by the language model. The Perceiver Resampler uses cross-attention where a fixed set of learned queries $\mQ \in \R^{64 \times 2048}$ (64 visual tokens, dimension 2048) attends to the image patch embeddings $\mK, \mV \in \R^{256 \times 1024}$ from the vision encoder. This produces a fixed-size representation regardless of input image resolution or the number of images in the sequence. The Perceiver Resampler contains approximately 1.4B parameters (6 layers of cross-attention and feed-forward networks with dimension 2048), making it the primary trainable component of Flamingo.</p>

<p>Between every few layers of the frozen language model, Flamingo inserts new <strong>cross-attention layers</strong> that allow text tokens to attend to the visual tokens produced by the Perceiver Resampler. Specifically, for Flamingo-80B (built on Chinchilla-70B), cross-attention layers are inserted after every 7th transformer layer, resulting in approximately 11 cross-attention insertions across the 80 layers. Each cross-attention layer adds roughly 134M parameters (for dimension 8192), totaling about 1.5B parameters for all insertions. Combined with the Perceiver Resampler, Flamingo adds approximately 2.9B trainable parameters to the 70B frozen base model, representing just 4\% additional parameters while enabling full multimodal capabilities.</p>

<p>The memory requirements for Flamingo are dominated by the frozen language model. Storing 70B parameters in FP16 requires 140 GB, which exceeds the memory of any single GPU. Flamingo uses model parallelism to partition the language model across multiple GPUs‚Äîfor example, distributing across 8 A100 GPUs (80GB each) places roughly 8.75B parameters per GPU, consuming about 17.5 GB for parameters. Activations for a sequence of 2048 tokens (including both text and visual tokens) across 80 layers with dimension 8192 require approximately $2048 \times 80 \times 8192 \times 2 = 2.6$ GB per example in FP16. With batch size 8, activations consume 21 GB per GPU, leaving sufficient memory for gradients of the trainable parameters (2.9B parameters √ó 2 bytes √ó 2 for gradients = 11.6 GB) and optimizer states (23.2 GB for Adam).</p>

<p>Training Flamingo on a mixture of image-text pairs, interleaved image-text documents, and video-text pairs requires substantial computational resources. The training dataset consists of 2.3 billion image-text pairs (similar to CLIP), 43 million interleaved image-text web pages, and 27 million video clips. Training Flamingo-80B for 1 epoch through this data with batch size 256 distributed across 256 A100 GPUs takes approximately 15 days, representing roughly 92,000 GPU-hours. The estimated training cost exceeds \$300,000 in cloud compute. However, the key advantage is that only 2.9B parameters are trained while leveraging the capabilities of a 70B language model, making training far more efficient than training a 70B multimodal model from scratch.</p>

<p>For inference, Flamingo's few-shot learning capability means that users can provide 2-32 example image-text pairs in the prompt to demonstrate a new task, and the model adapts its predictions based on these examples without any fine-tuning. This in-context learning works because the cross-attention mechanism allows the model to attend to the example images when processing the query image. The computational cost of inference scales linearly with the number of examples in the context: each additional image adds 256 patch tokens (after vision encoding) compressed to 64 visual tokens (after Perceiver Resampler), increasing the sequence length and thus the attention cost. For a prompt with 4 example images and 1 query image (5 images total), the visual tokens contribute $5 \times 64 = 320$ tokens to the sequence, which combined with text tokens (typically 500-1000) results in sequences of 800-1300 tokens. On a single A100 GPU, Flamingo-80B can process approximately 2-3 such sequences per second, limited primarily by the memory bandwidth required to load the 70B parameter model.</p>

<h2>Computational Analysis of Multimodal Transformers</h2>

<h3>Image Encoding Cost</h3>

<p>The computational cost of encoding images in multimodal transformers depends critically on the choice of vision encoder architecture and input resolution. For a Vision Transformer processing an image of resolution $H \times W$ with patch size $P$, the image is divided into $n = \frac{HW}{P^2}$ patches. Each patch is linearly projected to dimension $d$, and the resulting sequence of $n$ tokens is processed through $L$ transformer layers. The total computational cost is approximately $2Ld^2n^2$ FLOPs for attention operations plus $8Ld^2n$ FLOPs for feed-forward networks (using the standard 4√ó expansion ratio).</p>

<p>For concrete analysis, consider three common configurations. A ViT-Base model with 224√ó224 images, patch size 16, dimension 768, and 12 layers processes $n = 196$ patches. The attention cost is $2 \times 12 \times 768^2 \times 196^2 = 5.5$ GFLOPs, and the feed-forward cost is $8 \times 12 \times 768^2 \times 196 = 11.0$ GFLOPs, totaling 16.5 GFLOPs per image. A ViT-Large model with patch size 14, dimension 1024, and 24 layers processes $n = 256$ patches, requiring approximately $2 \times 24 \times 1024^2 \times 256^2 = 3.2$ TFLOPs for attention and $8 \times 24 \times 1024^2 \times 256 = 51.5$ GFLOPs for feed-forward, totaling 3.25 TFLOPs per image‚Äînearly 200√ó more than ViT-Base. Increasing resolution to 336√ó336 (as in CLIP ViT-L/14@336px) increases the patch count to $n = 576$, quadrupling the attention cost to 14.4 TFLOPs and more than doubling the total cost to 14.5 TFLOPs per image.</p>

<p>The memory requirements for image encoding are dominated by storing activations for backpropagation during training. For ViT-Large processing 256 patches through 24 layers with dimension 1024, each layer stores attention outputs ($256 \times 1024$), feed-forward outputs ($256 \times 4096$ intermediate, $256 \times 1024$ output), and layer norm statistics. Accounting for all activations, each image requires approximately $24 \times 256 \times (1024 + 4096 + 1024) \times 2 = 75$ MB in FP16. For a batch of 256 images (typical for CLIP training when distributed across many GPUs), activations consume 19.2 GB per GPU, leaving limited memory for model parameters and optimizer states on a 40GB A100 GPU.</p>

<h3>Text Encoding Cost</h3>

<p>Text encoding is typically far less expensive than image encoding due to shorter sequence lengths. For a transformer processing text with sequence length $m$ tokens through $L$ layers with dimension $d$, the computational cost is $2Ld^2m^2$ for attention plus $8Ld^2m$ for feed-forward networks. The CLIP text encoder uses $m = 77$ tokens, $d = 768$, and $L = 12$ layers, resulting in attention cost $2 \times 12 \times 768^2 \times 77^2 = 1.1$ GFLOPs and feed-forward cost $8 \times 12 \times 768^2 \times 77 = 4.4$ GFLOPs, totaling 5.5 GFLOPs per text‚Äîapproximately 10√ó less than encoding a single image with ViT-Large.</p>

<p>However, for large language models used in architectures like Flamingo, text encoding becomes more expensive. Chinchilla-70B uses 80 layers with dimension 8192, processing sequences of up to 2048 tokens. The attention cost is $2 \times 80 \times 8192^2 \times 2048^2 = 1.1 \times 10^{17}$ FLOPs or 110 PFLOPs per sequence, and the feed-forward cost is $8 \times 80 \times 8192^2 \times 2048 = 8.8$ TFLOPs, totaling approximately 110 TFLOPs per sequence. This is 34,000√ó more expensive than the CLIP text encoder and 34√ó more expensive than ViT-Large image encoding, demonstrating how large language models dominate the computational budget in models like Flamingo.</p>

<p>Memory for text activations scales with sequence length and model size. For Chinchilla-70B processing 2048 tokens through 80 layers with dimension 8192, storing activations requires approximately $80 \times 2048 \times (8192 + 32768 + 8192) \times 2 = 13$ GB per sequence in FP16 (accounting for attention outputs, feed-forward intermediate activations with 4√ó expansion, and final outputs). With batch size 8 distributed across 8 GPUs, each GPU stores activations for 1 sequence, consuming 13 GB of the 80 GB available on an A100.</p>

<h3>Cross-Modal Attention Cost</h3>

<p>Cross-modal attention, where one modality attends to another, introduces additional computational costs that scale with the product of sequence lengths from both modalities. For text tokens attending to image patches, the attention operation computes $\mQ \mK\transpose$ where $\mQ \in \R^{m \times d}$ (text queries) and $\mK \in \R^{n \times d}$ (image keys), producing an attention matrix of size $m \times n$. The computational cost is $2mnd$ FLOPs for the matrix multiplication, plus $2mnd$ for computing the weighted sum with values, totaling $4mnd$ FLOPs per cross-attention layer.</p>

<p>For a model like BLIP with $m = 128$ text tokens, $n = 196$ image patches, and $d = 768$, each cross-attention layer costs $4 \times 128 \times 196 \times 768 = 77$ MFLOPs. With 6 cross-attention layers in the multimodal encoder, the total cross-attention cost is 462 MFLOPs‚Äînegligible compared to the self-attention costs in the image and text encoders. However, the memory for storing the $m \times n$ attention matrices is $128 \times 196 \times 2 = 50$ KB per layer in FP16, or 300 KB for 6 layers, which is also minimal.</p>

<p>In Flamingo, the cross-attention cost is higher due to the large language model dimension. With $m = 2048$ text tokens, $n = 64$ visual tokens (after Perceiver Resampler compression), and $d = 8192$, each cross-attention layer costs $4 \times 2048 \times 64 \times 8192 = 4.3$ GFLOPs. With approximately 11 cross-attention layers inserted throughout the 80-layer language model, the total cross-attention cost is 47 GFLOPs per forward pass. While this is small compared to the 110 TFLOPs for the language model self-attention, it represents a non-trivial addition. The memory for attention matrices is $2048 \times 64 \times 2 = 262$ KB per layer, or 2.9 MB for all 11 layers‚Äîagain minimal compared to activation memory.</p>

<h3>Total Memory Requirements</h3>

<p>The total memory footprint for training multimodal transformers includes model parameters, activations, gradients, and optimizer states. For CLIP ViT-L/14 with 428M parameters, the parameters require $428 \times 10^6 \times 2 = 856$ MB in FP16. Gradients require an additional 856 MB. The Adam optimizer maintains two states (momentum and variance) per parameter, requiring $428 \times 10^6 \times 4 = 1.7$ GB in FP32 (optimizer states are typically kept in higher precision for numerical stability). Activations for a batch of 256 images and 256 texts consume approximately $256 \times (75 + 1.4) = 19.6$ GB as calculated earlier. The total memory per GPU is approximately $0.9 + 0.9 + 1.7 + 19.6 = 23.1$ GB, fitting comfortably on a 40GB A100 GPU.</p>

<p>For Flamingo-80B, the memory requirements are far more demanding. The 70B frozen parameters require $70 \times 10^9 \times 2 = 140$ GB in FP16, necessitating distribution across at least 2 A100 GPUs (80GB each). The 2.9B trainable parameters require $2.9 \times 10^9 \times 2 = 5.8$ GB for parameters, 5.8 GB for gradients, and $2.9 \times 10^9 \times 4 = 11.6$ GB for optimizer states, totaling 23.2 GB. Activations for a sequence of 2048 tokens through 80 layers with dimension 8192 require approximately 13 GB per sequence as calculated earlier. For batch size 8 distributed across 8 GPUs (1 sequence per GPU), the total memory per GPU is approximately $140/8 + 23.2 + 13 = 53.7$ GB, fitting on an 80GB A100 but leaving limited headroom. In practice, Flamingo uses gradient checkpointing to reduce activation memory by recomputing activations during backpropagation, trading compute for memory and enabling larger batch sizes or longer sequences.</p>

<h2>Training Challenges for Multimodal Transformers</h2>

<h3>Batch Size Requirements for Contrastive Learning</h3>

<p>Contrastive learning methods like CLIP require very large batch sizes to provide sufficient negative examples for effective learning. The contrastive loss compares each positive image-text pair against all other pairs in the batch, treating them as negatives. With batch size $N$, each example has $N-1$ negative examples, and the quality of the learned representations improves significantly as $N$ increases. Empirically, CLIP's performance scales log-linearly with batch size: increasing from 256 to 32,768 improves ImageNet zero-shot accuracy from approximately 58\% to 76\%, a gain of 18 percentage points.</p>

<p>However, large batch sizes create severe memory constraints. For batch size 32,768 with CLIP ViT-L/14, storing the image embeddings requires $32{,}768 \times 512 \times 4 = 67$ MB in FP32 (embeddings are kept in FP32 for numerical stability in the contrastive loss computation). Text embeddings require another 67 MB. The $32{,}768 \times 32{,}768$ similarity matrix requires $32{,}768^2 \times 4 = 4.3$ GB in FP32. During backpropagation, gradients of the loss with respect to the similarity matrix also require 4.3 GB. The total memory for just the contrastive loss computation is approximately 8.7 GB, not including model parameters, activations, or optimizer states.</p>

<p>To make such large batches tractable, CLIP uses distributed training across many GPUs with careful memory management. Each GPU processes a local batch of size $N_{\text{local}} = N / G$ where $G$ is the number of GPUs. For $N = 32{,}768$ distributed across 256 GPUs, each GPU processes 128 examples. The key insight is that the similarity matrix can be computed in a distributed fashion: each GPU computes similarities between its local images and all texts (gathered from all GPUs via all-gather communication), producing a $128 \times 32{,}768$ matrix that requires only $128 \times 32{,}768 \times 4 = 16.8$ MB per GPU. The full $32{,}768 \times 32{,}768$ matrix is never materialized on any single GPU, reducing memory requirements by a factor of $G$.</p>

<p>The communication cost for distributed contrastive learning is substantial. Each GPU must perform an all-gather operation to collect embeddings from all other GPUs, communicating $32{,}768 \times 512 \times 4 = 67$ MB for images and 67 MB for text, totaling 134 MB per GPU per training step. With high-bandwidth interconnects like NVLink (300 GB/s) or InfiniBand (200 Gb/s = 25 GB/s), this communication takes approximately 5-10 milliseconds. For comparison, the forward and backward pass through the model takes approximately 50-100 milliseconds per step, so communication overhead is 5-10\% of the total training time‚Äîsignificant but not prohibitive.</p>

<h3>Memory Optimization Techniques</h3>

<p>Beyond distributed training, several memory optimization techniques are essential for training large multimodal models. <strong>Gradient checkpointing</strong> trades computation for memory by not storing all activations during the forward pass. Instead, only a subset of activations (typically at layer boundaries) are stored, and intermediate activations are recomputed during backpropagation when needed. For CLIP ViT-L/14, gradient checkpointing reduces activation memory from approximately 75 MB per image to 25 MB per image (a 3√ó reduction) at the cost of increasing training time by approximately 20\% due to recomputation. This enables increasing batch size from 128 to 384 per GPU, which more than compensates for the slowdown by improving convergence.</p>

<p><strong>Mixed precision training</strong> uses FP16 for most computations while maintaining FP32 master weights and loss scaling to prevent gradient underflow. For multimodal transformers, this reduces activation memory by 50\% and accelerates matrix multiplications on modern GPUs with Tensor Cores. CLIP training with mixed precision reduces memory from 23.1 GB to approximately 14.5 GB per GPU, enabling batch sizes of 256 per GPU instead of 128. The training speedup is approximately 1.8√ó on A100 GPUs, reducing total training time from 12 days to 7 days on the same hardware.</p>

<p><strong>ZeRO (Zero Redundancy Optimizer)</strong> partitions optimizer states, gradients, and optionally parameters across GPUs to reduce per-GPU memory consumption. For Flamingo-80B with 2.9B trainable parameters, the optimizer states alone require 11.6 GB per GPU without ZeRO. With ZeRO Stage 2 (partitioning optimizer states and gradients), this is reduced to $11.6 / G$ GB per GPU where $G$ is the number of GPUs. With 8 GPUs, optimizer memory drops from 11.6 GB to 1.45 GB per GPU, freeing memory for larger batch sizes or longer sequences. ZeRO Stage 3 additionally partitions parameters, reducing the 5.8 GB parameter memory to 0.73 GB per GPU, though this increases communication overhead as parameters must be gathered during forward and backward passes.</p>

<h3>Distributed Training Infrastructure</h3>

<p>Training multimodal transformers at scale requires sophisticated distributed training infrastructure with high-bandwidth interconnects and efficient parallelism strategies. For CLIP-scale models (400M parameters, 400M training examples), a typical setup uses 64-256 A100 GPUs connected via NVLink within nodes and InfiniBand between nodes. Each node contains 8 GPUs with NVLink providing 600 GB/s bidirectional bandwidth between GPUs in the same node, while InfiniBand provides 200 Gb/s (25 GB/s) between nodes. This hierarchical network topology means that communication within a node is 24√ó faster than between nodes, making it crucial to minimize cross-node communication.</p>

<p>The parallelism strategy combines data parallelism (different GPUs process different data) with model parallelism (different GPUs hold different parts of the model). For CLIP, pure data parallelism is typically sufficient since the model fits on a single GPU. Each GPU holds a full copy of the 428M parameter model (856 MB in FP16) and processes a local batch of images and texts. Gradients are synchronized across all GPUs using all-reduce operations after each backward pass, communicating $428 \times 10^6 \times 2 = 856$ MB per GPU. With 256 GPUs and ring all-reduce, this takes approximately $856 \text{ MB} \times 2 \times (256-1) / 256 / 25 \text{ GB/s} = 68$ ms, adding roughly 10-15\% overhead to the training step time.</p>

<p>For Flamingo-80B, model parallelism is essential since the 70B parameter language model cannot fit on a single GPU. The model is partitioned across 8 GPUs using tensor parallelism, where each linear layer's weight matrix is split across GPUs. For a linear layer with weight $\mW \in \R^{8192 \times 8192}$, each GPU holds $\mW_i \in \R^{8192 \times 1024}$ (1/8th of the columns). During the forward pass, each GPU computes a partial output, then an all-reduce operation sums the results. This requires communicating $8192 \times 2 = 16$ KB per linear layer, which is minimal. However, with hundreds of linear layers across 80 transformer layers, the cumulative communication overhead is approximately 5-10\% of training time.</p>

<h3>Training Time and Cost Estimates</h3>

<p>The total training time for multimodal transformers depends on the number of training examples, batch size, model size, and hardware configuration. For CLIP ViT-L/14 trained on 400M image-text pairs with batch size 32,768, the number of training steps is $400{,}000{,}000 / 32{,}768 = 12{,}207$ steps. Each step requires a forward pass (approximately 2 PFLOPs as calculated earlier), backward pass (approximately 4 PFLOPs, twice the forward pass), and optimizer step (negligible compute). The total compute is approximately $12{,}207 \times 6 \text{ PFLOPs} = 73{,}242$ PFLOPs.</p>

<p>On an NVIDIA A100 GPU with 312 TFLOPS of FP16 compute and approximately 50\% utilization (accounting for memory bandwidth limitations and non-compute operations), the effective throughput is 156 TFLOPS. Training on a single A100 would take $73{,}242 \text{ PFLOPs} / 156 \text{ TFLOPS} = 469{,}500$ seconds or approximately 130 hours. However, this ignores memory constraints‚Äîa single A100 cannot hold the activations for batch size 32,768. With 256 A100 GPUs, the training time is approximately $130 / 256 = 0.5$ hours, but this assumes perfect scaling, which is unrealistic due to communication overhead. In practice, with 10-15\% communication overhead, the training time is approximately 0.6 hours per epoch. For multiple epochs (CLIP was trained for approximately 32 epochs), the total training time is approximately 19 hours on 256 A100 GPUs, though the actual training took several days due to additional factors like data loading, checkpointing, and debugging.</p>

<p>The cost of training CLIP at cloud pricing can be estimated from GPU-hours. Training for 19 hours on 256 A100 GPUs requires $19 \times 256 = 4{,}864$ GPU-hours. At typical cloud pricing of \$2.50-\$4.00 per A100 GPU-hour, the total cost is approximately \$12,000-\$19,000 for a single training run. However, the actual development cost is much higher due to hyperparameter tuning, ablation studies, and failed experiments. OpenAI likely spent 10-20√ó this amount in total compute to develop CLIP, suggesting a total cost of \$120,000-\$380,000.</p>

<p>For Flamingo-80B, the training cost is substantially higher due to the 70B parameter language model. Training on 2.3B image-text pairs plus interleaved documents with batch size 256 distributed across 256 A100 GPUs for approximately 15 days requires $15 \times 24 \times 256 = 92{,}160$ GPU-hours. At \$3.00 per A100 GPU-hour, the total cost is approximately \$276,000 for a single training run. Including development costs, the total investment in training Flamingo likely exceeded \$1 million in compute costs alone, not including researcher time, data collection, and infrastructure development.</p>

<h2>Audio Transformers</h2>

<h3>Whisper: Speech Recognition</h3>

<div class="definition"><strong>Definition:</strong> 
Encoder-decoder transformer for speech:

<p><strong>Input:</strong> Audio waveform $\to$ Log-mel spectrogram</p>

<p><strong>Encoder:</strong>
<ul>
    <li>Input: Spectrogram (80 mel bins)
    <li>Convolution layers (downsample)
    <li>Transformer encoder layers
</ul>

<p><strong>Decoder:</strong>
<ul>
    <li>Autoregressive text generation
    <li>Special tokens for language, task, timestamps
</ul>
</div>

<p><strong>Training data:</strong> 680,000 hours of multilingual audio</p>

<p><strong>Tasks supported:</strong>
<ul>
    <li>Speech recognition (transcription)
    <li>Translation (to English)
    <li>Language identification
    <li>Voice activity detection
    <li>Timestamp prediction
</ul>

<div class="example"><strong>Example:</strong> 
<strong>Special tokens:</strong>
<pre><code>
<|startoftranscript|><|en|><|transcribe|><|notimestamps|>
</code></pre>

<p><strong>Spectrogram:</strong>
<ul>
    <li>80 mel bins
    <li>3000 frames (30 seconds audio at 100 Hz)
    <li>Input: $3000 \times 80$
</ul>

<p><strong>Encoder:</strong>
<ul>
    <li>Conv layers: $3000 \times 80 \to 1500 \times 768$
    <li>Transformer: Process 1500 tokens
</ul>

<p><strong>Decoder:</strong> Generate text tokens autoregressively
</div>

<h3>Audio-Text Pre-training</h3>

<p><strong>Contrastive learning:</strong> Like CLIP but audio-text</p>

<p><strong>AudioCLIP:</strong> Tri-modal (image, text, audio)</p>

<p><strong>Applications:</strong>
<ul>
    <li>Zero-shot audio classification
    <li>Audio captioning
    <li>Text-to-audio generation
</ul>

<h2>Unified Multimodal Models</h2>

<h3>Perceiver and Perceiver IO</h3>

<p><strong>Key idea:</strong> Map arbitrary modalities to latent space via cross-attention</p>

<div class="definition"><strong>Definition:</strong> 
<strong>Components:</strong>

<p><strong>1. Latent array:</strong> Fixed set of learned queries $\mZ \in \R^{M \times d}$</p>

<p><strong>2. Cross-attention:</strong> Latents attend to inputs
<div class="equation">
$$
\mZ_1 = \text{CrossAttn}(\mQ=\mZ, \mK=\mX, \mV=\mX)
$$
</div>

<p><strong>3. Transformer:</strong> Process latents
<div class="equation">
$$
\mZ_L = \text{Transformer}(\mZ_1)
$$
</div>

<p><strong>4. Output:</strong> Decode latents to task outputs
</div>

<p><strong>Benefits:</strong>
<ul>
    <li>Handles arbitrary input sizes
    <li>Computation independent of input size (fixed latents)
    <li>Unified architecture for images, video, audio, text
</ul>

<h3>GPT-4V and LLaVA</h3>

<p><strong>GPT-4V (Vision):</strong> GPT-4 with vision capabilities
<ul>
    <li>Interleaved image and text inputs
    <li>Strong vision-language understanding
    <li>Details not fully disclosed
</ul>

<p><strong>LLaVA (Open-source):</strong>
<ul>
    <li>CLIP vision encoder
    <li>LLaMA language model
    <li>Linear projection to align embeddings
    <li>Instruction tuning on visual conversations
</ul>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> Implement CLIP contrastive loss for batch size 8:
<ol>
    <li>Generate random image embeddings $(8, 512)$
    <li>Generate random text embeddings $(8, 512)$
    <li>Compute $8 \times 8$ similarity matrix
    <li>Calculate contrastive loss with $\tau = 0.07$
</ol>
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Use CLIP for zero-shot classification on CIFAR-10:
<ol>
    <li>Load pre-trained CLIP model
    <li>Create text prompts for 10 classes
    <li>Encode images and prompts
    <li>Compute accuracy
    <li>Compare to supervised baseline
</ol>
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> Analyze Whisper architecture:
<ol>
    <li>Calculate parameters for encoder (24 layers, $d=1024$)
    <li>Calculate parameters for decoder (24 layers)
    <li>Estimate memory for 30-second audio
    <li>Compare to text-only GPT-2
</ol>
</div>

<div class="exercise" id="exercise-4"><strong>Exercise 4:</strong> Design multimodal fusion strategy for video understanding (visual + audio + captions):
<ol>
    <li>Propose architecture
    <li>Define fusion mechanism
    <li>Specify training objective
    <li>Estimate parameter count
</ol>
</div>

<h2>Solutions</h2>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 1: CLIP Contrastive Loss Implementation</strong>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

def clip_contrastive_loss(image_embeddings, text_embeddings, temperature=0.07):
    """
    Compute CLIP contrastive loss
    Args:
        image_embeddings: (B, D) normalized image embeddings
        text_embeddings: (B, D) normalized text embeddings
        temperature: temperature parameter tau
    Returns:
        loss: scalar contrastive loss
    """
    # Normalize embeddings
    image_embeddings = F.normalize(image_embeddings, dim=-1)
    text_embeddings = F.normalize(text_embeddings, dim=-1)
    
    # Compute similarity matrix (B, B)
    logits = torch.matmul(image_embeddings, text_embeddings.t()) / temperature
    
    # Labels: diagonal elements are positive pairs
    batch_size = image_embeddings.shape[0]
    labels = torch.arange(batch_size, device=image_embeddings.device)
    
    # Symmetric loss: image-to-text + text-to-image
    loss_i2t = F.cross_entropy(logits, labels)
    loss_t2i = F.cross_entropy(logits.t(), labels)
    
    loss = (loss_i2t + loss_t2i) / 2
    
    return loss, logits

# Part (a): Generate random embeddings
batch_size = 8
embed_dim = 512

image_embeddings = torch.randn(batch_size, embed_dim)
text_embeddings = torch.randn(batch_size, embed_dim)

print(f"Image embeddings shape: {image_embeddings.shape}")
print(f"Text embeddings shape: {text_embeddings.shape}")

# Part (b): Normalize embeddings
image_embeddings = F.normalize(image_embeddings, dim=-1)
text_embeddings = F.normalize(text_embeddings, dim=-1)

print(f"\nAfter normalization:")
print(f"Image embedding norms: {torch.norm(image_embeddings, dim=-1)}")
print(f"Text embedding norms: {torch.norm(text_embeddings, dim=-1)}")

# Part (c): Compute similarity matrix
temperature = 0.07
similarity_matrix = torch.matmul(image_embeddings, text_embeddings.t()) / temperature

print(f"\nSimilarity matrix shape: {similarity_matrix.shape}")
print(f"Similarity matrix:\n{similarity_matrix}")

# Part (d): Calculate contrastive loss
loss, logits = clip_contrastive_loss(image_embeddings, text_embeddings, temperature)

print(f"\nContrastive loss: {loss.item():.4f}")
print(f"Logits shape: {logits.shape}")

# Analyze the loss
labels = torch.arange(batch_size)
predictions_i2t = logits.argmax(dim=1)
predictions_t2i = logits.t().argmax(dim=1)

accuracy_i2t = (predictions_i2t == labels).float().mean()
accuracy_t2i = (predictions_t2i == labels).float().mean()

print(f"\nImage-to-Text accuracy: {accuracy_i2t.item():.2
print(f"Text-to-Image accuracy: {accuracy_t2i.item():.2
</code></pre>

<p><strong>Mathematical Derivation:</strong></p>

<p><strong>Part (a) \& (b): Embeddings</strong></p>

<p>Image embeddings: $\vI = [\vi_1, \vi_2, \ldots, \vi_8] \in \mathbb{R}^{8 \times 512}$</p>

<p>Text embeddings: $\vT = [\vt_1, \vt_2, \ldots, \vt_8] \in \mathbb{R}^{8 \times 512}$</p>

<p>Normalize to unit sphere:
$\hat{\vi}_i = \frac{\vi_i}{\|\vi_i\|_2}, \quad \hat{\vt}_i = \frac{\vt_i}{\|\vt_i\|_2}$</p>

<p><strong>Part (c): Similarity Matrix</strong></p>

<p>Cosine similarity matrix:
$\vS_{ij} = \frac{\hat{\vi}_i \cdot \hat{\vt}_j}{\tau}$</p>

<p>where $\tau = 0.07$ is the temperature parameter.</p>

<p>Full matrix:
$\vS = \frac{1}{\tau} \hat{\vI} \hat{\vT}^T \in \mathbb{R}^{8 \times 8}$</p>

<p>Example:
\[
\vS = \begin{bmatrix}
s_{11} & s_{12} & \cdots & s_{18} \\
s_{21} & s_{22} & \cdots & s_{28} \\
\vdots & \vdots & \ddots & \vdots \\
s_{81} & s_{82} & \cdots & s_{88}
\end{bmatrix}
\]</p>

<p>Diagonal elements $s_{ii}$ are positive pairs (matched image-text).</p>

<p>Off-diagonal elements $s_{ij}$ ($i \neq j$) are negative pairs.</p>

<p><strong>Part (d): Contrastive Loss</strong></p>

<p><strong>Image-to-Text Loss:</strong></p>

<p>For each image $i$, predict its matching text from 8 candidates:</p>

<p>$\mathcal{L}_{i2t} = -\frac{1}{8} \sum_{i=1}^{8} \log \frac{\exp(s_{ii})}{\sum_{j=1}^{8} \exp(s_{ij})}$</p>

<p>This is cross-entropy with labels $y_i = i$ (diagonal).</p>

<p><strong>Text-to-Image Loss:</strong></p>

<p>For each text $j$, predict its matching image from 8 candidates:</p>

<p>$\mathcal{L}_{t2i} = -\frac{1}{8} \sum_{j=1}^{8} \log \frac{\exp(s_{jj})}{\sum_{i=1}^{8} \exp(s_{ij})}$</p>

<p><strong>Total CLIP Loss:</strong></p>

<p>$\mathcal{L}_{\text{CLIP}} = \frac{1}{2}(\mathcal{L}_{i2t} + \mathcal{L}_{t2i})$</p>

<p>Symmetric loss ensures both modalities learn aligned representations.</p>

<p><strong>Why Temperature $\tau = 0.07$?</strong></p>

<ul>
    <li><strong>Sharpens distribution:</strong> Small $\tau$ makes softmax more peaked
    <li><strong>Emphasizes hard negatives:</strong> Distinguishes similar but incorrect pairs
    <li><strong>Empirically optimal:</strong> Found through hyperparameter search
    <li><strong>Typical range:</strong> $\tau \in [0.01, 0.1]$
</ul>

<p>Effect of temperature:
<ul>
    <li>$\tau \to 0$: Approaches hard assignment (argmax)
    <li>$\tau \to \infty$: Uniform distribution (no learning)
    <li>$\tau = 0.07$: Good balance for contrastive learning
</ul>

<p><strong>Numerical Example:</strong></p>

<p>Suppose for image 1:
<ul>
    <li>$s_{11} = 0.9$ (correct text)
    <li>$s_{12} = 0.3, s_{13} = 0.2, \ldots, s_{18} = 0.1$ (incorrect texts)
</ul>

<p>Softmax probabilities:
$p_1 = \frac{\exp(0.9/0.07)}{\exp(0.9/0.07) + \sum_{j=2}^{8} \exp(s_{1j}/0.07)}$</p>

<p>Loss for image 1:
$\ell_1 = -\log p_1$</p>

<p>If $p_1 \approx 1$, then $\ell_1 \approx 0$ (good alignment).</p>

<p>If $p_1 \approx 0.125$ (uniform), then $\ell_1 \approx 2.08$ (poor alignment).</p>

<p><strong>Training Dynamics:</strong></p>

<ol>
    <li><strong>Initial:</strong> Random embeddings, $\mathcal{L} \approx \log(8) = 2.08$
    <li><strong>Training:</strong> Embeddings align, diagonal elements increase
    <li><strong>Converged:</strong> $s_{ii} \gg s_{ij}$ for $i \neq j$, $\mathcal{L} \to 0$
</ol>

<p><strong>Key Insights:</strong></p>

<ul>
    <li>Batch size acts as number of negative samples
    <li>Larger batches improve contrastive learning (more negatives)
    <li>CLIP uses batch sizes up to 32,768 in practice
    <li>Symmetric loss prevents modality collapse
    <li>Temperature is a critical hyperparameter
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 2: CLIP Zero-Shot Classification on CIFAR-10</strong>

<pre><code>import torch
import clip
from PIL import Image
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from tqdm import tqdm

# Part (a): Load pre-trained CLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

print(f"CLIP model loaded on {device}")
print(f"Model: ViT-B/32")

# Part (b): Create text prompts for 10 CIFAR-10 classes
cifar10_classes = [
    "airplane", "automobile", "bird", "cat", "deer",
    "dog", "frog", "horse", "ship", "truck"
]

# Template-based prompts (improves accuracy)
templates = [
    "a photo of a {}.",
    "a blurry photo of a {}.",
    "a photo of many {}.",
    "a photo of the small {}.",
    "a photo of the large {}.",
]

# Encode text prompts
def encode_text_prompts(model, classes, templates):
    """Encode text prompts with multiple templates"""
    text_features = []
    
    for classname in classes:
        # Create prompts from templates
        texts = [template.format(classname) for template in templates]
        texts = clip.tokenize(texts).to(device)
        
        # Encode texts
        with torch.no_grad():
            class_embeddings = model.encode_text(texts)
            class_embeddings = class_embeddings / class_embeddings.norm(dim=-1, keepdim=True)
            
            # Average over templates
            class_embedding = class_embeddings.mean(dim=0)
            class_embedding = class_embedding / class_embedding.norm()
            
            text_features.append(class_embedding)
    
    text_features = torch.stack(text_features, dim=0)
    return text_features

text_features = encode_text_prompts(model, cifar10_classes, templates)
print(f"\nText features shape: {text_features.shape}")  # (10, 512)

# Part (c): Load CIFAR-10 test set
test_dataset = torchvision.datasets.CIFAR10(
    root='./data', 
    train=False, 
    download=True,
    transform=preprocess
)

test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)

# Zero-shot classification
def zero_shot_classify(model, loader, text_features):
    """Perform zero-shot classification"""
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in tqdm(loader):
            images = images.to(device)
            labels = labels.to(device)
            
            # Encode images
            image_features = model.encode_image(images)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # Compute similarity with text features
            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
            
            # Predict
            predictions = similarity.argmax(dim=-1)
            
            correct += (predictions == labels).sum().item()
            total += labels.size(0)
    
    accuracy = 100.0 * correct / total
    return accuracy

# Part (d): Compute accuracy
zero_shot_accuracy = zero_shot_classify(model, test_loader, text_features)
print(f"\nZero-shot accuracy: {zero_shot_accuracy:.2f}



# Part (e): Compare to supervised baseline
# Train a simple supervised classifier
class SimpleCNN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = torch.nn.Conv2d(32, 64, 3, padding=1)
        self.pool = torch.nn.MaxPool2d(2, 2)
        self.fc1 = torch.nn.Linear(64 * 8 * 8, 128)
        self.fc2 = torch.nn.Linear(128, 10)
    
    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Train supervised model (simplified)
supervised_model = SimpleCNN().to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(supervised_model.parameters(), lr=0.001)

# Training loop (10 epochs for quick comparison)
train_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True,
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)

for epoch in range(10):
    supervised_model.train()
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = supervised_model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Evaluate supervised model
supervised_model.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = supervised_model(images)
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

supervised_accuracy = 100.0 * correct / total

print(f"\nComparison:")
print(f"CLIP Zero-shot: {zero_shot_accuracy:.2f}
print(f"Supervised CNN (10 epochs): {supervised_accuracy:.2f}
</code></pre>

<p><strong>Expected Results:</strong></p>

<table>
<tr><th><strong>Method</strong></th><th><strong>Accuracy</strong></th><th><strong>Training Data</strong></th></tr>
<tr><td>CLIP Zero-shot (ViT-B/32)</td><td>89-91\%</td><td>0 (CIFAR-10)</td></tr>
<tr><td>CLIP Zero-shot (ViT-L/14)</td><td>93-95\%</td><td>0 (CIFAR-10)</td></tr>
<tr><td>Supervised CNN (10 epochs)</td><td>70-75\%</td><td>50k (CIFAR-10)</td></tr>
<tr><td>Supervised ResNet-50 (200 epochs)</td><td>95-96\%</td><td>50k (CIFAR-10)</td></tr>
</table>

<p><strong>Analysis:</strong></p>

<p><strong>Part (a): Pre-trained CLIP Model</strong></p>

<p>CLIP models available:
<ul>
    <li><strong>RN50:</strong> ResNet-50 image encoder
    <li><strong>ViT-B/32:</strong> ViT-Base with patch size 32
    <li><strong>ViT-B/16:</strong> ViT-Base with patch size 16 (better)
    <li><strong>ViT-L/14:</strong> ViT-Large with patch size 14 (best)
</ul>

<p>Pre-training:
<ul>
    <li>Dataset: 400M image-text pairs from internet
    <li>Training: Contrastive learning for 32 epochs
    <li>Batch size: 32,768 (large-scale)
    <li>Compute: 256 V100 GPUs for 12 days
</ul>

<p><strong>Part (b): Text Prompts</strong></p>

<strong>Simple prompts:</strong>
<pre><code>
"airplane", "automobile", "bird", ...
</code></pre>

<strong>Template-based prompts (better):</strong>
<pre><code>
"a photo of a airplane."
"a blurry photo of a airplane."
"a photo of many airplanes."
</code></pre>

<p>Why templates help:
<ul>
    <li>Match training distribution (natural sentences)
    <li>Provide context for ambiguous classes
    <li>Ensemble over multiple descriptions
    <li>Improve robustness to variations
</ul>

<p>Prompt engineering tips:
<ul>
    <li>Use natural language sentences
    <li>Include domain-specific context
    <li>Try multiple templates and average
    <li>Avoid overly specific descriptions
</ul>

<p><strong>Part (c): Encoding Process</strong></p>

<p><strong>Image encoding:</strong>
<ol>
    <li>Preprocess: Resize to $224 \times 224$, normalize
    <li>ViT encoder: Extract features
    <li>Projection: Map to shared embedding space (512-dim)
    <li>Normalize: $\hat{\vi} = \vi / \|\vi\|_2$
</ol>

<p><strong>Text encoding:</strong>
<ol>
    <li>Tokenize: Convert text to token IDs
    <li>Text encoder: Transformer processes tokens
    <li>Projection: Map to shared embedding space (512-dim)
    <li>Normalize: $\hat{\vt} = \vt / \|\vt\|_2$
</ol>

<p><strong>Part (d): Zero-Shot Classification</strong></p>

<p><strong>Algorithm:</strong></p>

<p>For each test image $\vx$:
<ol>
    <li>Encode image: $\vi = \text{ImageEncoder}(\vx)$
    <li>Compute similarity with all class embeddings:
    $s_k = \vi \cdot \vt_k$ for $k = 1, \ldots, 10$
    <li>Apply softmax: $p_k = \frac{\exp(s_k / \tau)}{\sum_{j=1}^{10} \exp(s_j / \tau)}$
    <li>Predict: $\hat{y} = \argmax_k p_k$
</ol>

<p>Temperature $\tau = 0.01$ (learned during training).</p>

<p><strong>Mathematical Formulation:</strong></p>

<p>$P(y = k | \vx) = \frac{\exp(\text{sim}(\vi, \vt_k) / \tau)}{\sum_{j=1}^{10} \exp(\text{sim}(\vi, \vt_j) / \tau)}$</p>

<p>where $\text{sim}(\vi, \vt) = \vi \cdot \vt$ (cosine similarity after normalization).</p>

<p><strong>Part (e): Comparison with Supervised Baseline</strong></p>

<p><strong>Why CLIP Zero-Shot Outperforms Supervised CNN:</strong></p>

<ol>
    <li><strong>Pre-training scale:</strong> 400M image-text pairs vs 50k CIFAR-10 images
    <li><strong>Transfer learning:</strong> Leverages knowledge from diverse data
    <li><strong>Better architecture:</strong> ViT-B/32 vs simple CNN
    <li><strong>Semantic understanding:</strong> Learns concepts, not just patterns
    <li><strong>Robustness:</strong> Generalizes better to distribution shifts
</ol>

<p><strong>When Supervised Wins:</strong></p>

<ul>
    <li><strong>Sufficient training data:</strong> ResNet-50 with 200 epochs reaches 95-96\%
    <li><strong>Domain-specific:</strong> Fine-tuned models beat zero-shot on specialized tasks
    <li><strong>Computational constraints:</strong> Smaller models are faster
</ul>

<p><strong>CLIP Advantages:</strong></p>

<ol>
    <li><strong>No training required:</strong> Instant deployment
    <li><strong>Flexible:</strong> Change classes without retraining
    <li><strong>Interpretable:</strong> Natural language descriptions
    <li><strong>Robust:</strong> Handles distribution shifts better
    <li><strong>Multimodal:</strong> Can do image-text retrieval, captioning, etc.
</ol>

<p><strong>Practical Recommendations:</strong></p>

<table>
<tr><th><strong>Scenario</strong></th><th><strong>Recommendation</strong></th></tr>
<tr><td>Quick prototype</td><td>CLIP zero-shot</td></tr>
<tr><td>Fixed classes, lots of data</td><td>Supervised training</td></tr>
<tr><td>Changing classes frequently</td><td>CLIP zero-shot</td></tr>
<tr><td>Maximum accuracy</td><td>Fine-tune CLIP</td></tr>
<tr><td>Limited compute</td><td>Supervised small model</td></tr>
<tr><td>Interpretability needed</td><td>CLIP with prompts</td></tr>
</table>

<p><strong>Improving CLIP Zero-Shot:</strong></p>

<ol>
    <li><strong>Better prompts:</strong> Domain-specific templates
    <li><strong>Larger model:</strong> ViT-L/14 instead of ViT-B/32
    <li><strong>Ensemble:</strong> Average predictions from multiple prompts
    <li><strong>Few-shot:</strong> Add a few examples with linear probe
    <li><strong>Fine-tuning:</strong> Adapt to target domain
</ol>

<p><strong>Key Takeaways:</strong></p>

<ul>
    <li>CLIP achieves strong zero-shot performance through large-scale pre-training
    <li>Natural language prompts enable flexible classification
    <li>Zero-shot CLIP often matches or exceeds supervised baselines
    <li>Prompt engineering is crucial for optimal performance
    <li>CLIP's multimodal nature enables many downstream tasks
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 3: Whisper Architecture Analysis</strong>

<p><strong>Part (a): Encoder Parameters (24 layers, $d=1024$)</strong></p>

<p><strong>Whisper Encoder Configuration:</strong>
<ul>
    <li>Layers: $L = 24$
    <li>Hidden size: $d = 1024$
    <li>Attention heads: $h = 16$
    <li>MLP ratio: $4.0$ (MLP size = $4096$)
    <li>Audio features: 80-dimensional log-mel spectrogram
    <li>Sequence length: $T = 3000$ (30 seconds at 100 Hz)
</ul>

<p><strong>Parameter Breakdown:</strong></p>

<p><strong>1. Input Convolution Layers:</strong>
<ul>
    <li>Conv1: $80 \times 3 \times 1024 = 245{,}760$
    <li>Conv2: $1024 \times 3 \times 1024 = 3{,}145{,}728$
    <li>Total: $3{,}391{,}488$ parameters
</ul>

<p><strong>2. Position Embeddings:</strong>
<ul>
    <li>Sinusoidal (not learned): 0 parameters
</ul>

<p><strong>3. Per Transformer Layer:</strong></p>

<p><em>Multi-Head Attention:</em>
<ul>
    <li>$Q, K, V$ projections: $3 \times 1024^2 = 3{,}145{,}728$
    <li>Output projection: $1024^2 = 1{,}048{,}576$
    <li>Total attention: $4{,}194{,}304$
</ul>

<p><em>MLP:</em>
<ul>
    <li>First linear: $1024 \times 4096 = 4{,}194{,}304$
    <li>Second linear: $4096 \times 1024 = 4{,}194{,}304$
    <li>Total MLP: $8{,}388{,}608$
</ul>

<p><em>Layer Normalization:</em>
<ul>
    <li>2 LayerNorms: $2 \times 2 \times 1024 = 4{,}096$
</ul>

<p><strong>Total per layer: $12{,}587{,}008$ parameters</strong></p>

<p><strong>4. All 24 Encoder Layers:</strong>
$24 \times 12{,}587{,}008 = 302{,}088{,}192$ parameters</p>

<p><strong>Total Encoder: $\approx 305.5$M parameters</strong></p>

<p><strong>Part (b): Decoder Parameters (24 layers)</strong></p>

<p><strong>Whisper Decoder Configuration:</strong>
<ul>
    <li>Layers: $L = 24$
    <li>Hidden size: $d = 1024$
    <li>Attention heads: $h = 16$
    <li>Vocabulary size: $V = 51{,}865$
    <li>Max sequence length: $448$ tokens
</ul>

<p><strong>Parameter Breakdown:</strong></p>

<p><strong>1. Token Embedding:</strong>
<ul>
    <li>$51{,}865 \times 1024 = 53{,}109{,}760$ parameters
</ul>

<p><strong>2. Position Embeddings:</strong>
<ul>
    <li>$448 \times 1024 = 458{,}752$ parameters
</ul>

<p><strong>3. Per Decoder Layer:</strong></p>

<p><em>Masked Self-Attention:</em>
<ul>
    <li>Same as encoder: $4{,}194{,}304$ parameters
</ul>

<p><em>Cross-Attention:</em>
<ul>
    <li>$Q$ projection: $1024^2 = 1{,}048{,}576$
    <li>$K, V$ projections (from encoder): $2 \times 1024^2 = 2{,}097{,}152$
    <li>Output projection: $1024^2 = 1{,}048{,}576$
    <li>Total cross-attention: $4{,}194{,}304$
</ul>

<p><em>MLP:</em>
<ul>
    <li>Same as encoder: $8{,}388{,}608$ parameters
</ul>

<p><em>Layer Normalization:</em>
<ul>
    <li>3 LayerNorms: $3 \times 2 \times 1024 = 6{,}144$
</ul>

<p><strong>Total per decoder layer: $16{,}783{,}360$ parameters</strong></p>

<p><strong>4. All 24 Decoder Layers:</strong>
$24 \times 16{,}783{,}360 = 402{,}800{,}640$ parameters</p>

<p><strong>5. Output Projection:</strong>
<ul>
    <li>Shared with token embedding: 0 additional parameters
</ul>

<p><strong>Total Decoder: $\approx 456.4$M parameters</strong></p>

<p><strong>Total Whisper Model: $305.5 + 456.4 = 761.9$M parameters</strong></p>

<p>(Actual Whisper-large: $\approx 1.55$B parameters due to additional components)</p>

<p><strong>Part (c): Memory for 30-Second Audio</strong></p>

<p><strong>Input Processing:</strong></p>

<p><strong>1. Audio Preprocessing:</strong>
<ul>
    <li>Sample rate: 16 kHz
    <li>30 seconds: $30 \times 16{,}000 = 480{,}000$ samples
    <li>Raw audio: $480{,}000 \times 4$ bytes = 1.92 MB
</ul>

<p><strong>2. Log-Mel Spectrogram:</strong>
<ul>
    <li>Window size: 25 ms (400 samples)
    <li>Hop length: 10 ms (160 samples)
    <li>Number of frames: $\frac{480{,}000}{160} = 3{,}000$
    <li>Mel bins: 80
    <li>Features: $3{,}000 \times 80 = 240{,}000$ values
    <li>Memory: $240{,}000 \times 4$ bytes = 0.96 MB
</ul>

<p><strong>Encoder Memory (Inference):</strong></p>

<p><strong>1. Activations per layer:</strong>
<ul>
    <li>Input: $3{,}000 \times 1024 = 3{,}072{,}000$ values
    <li>Attention scores: $16 \times 3{,}000 \times 3{,}000 = 144{,}000{,}000$ values
    <li>MLP intermediate: $3{,}000 \times 4096 = 12{,}288{,}000$ values
</ul>

<p>Peak per layer: $\approx 159$M values $\times$ 4 bytes = 636 MB</p>

<p><strong>2. Total encoder activations:</strong>
$24 \times 636$ MB = 15.3 GB (if storing all layers)</p>

<p>With activation checkpointing: $\approx 1.3$ GB</p>

<p><strong>Decoder Memory (Inference):</strong></p>

<p>For generating 448 tokens:
<ul>
    <li>Decoder activations: $448 \times 1024 = 458{,}752$ values per layer
    <li>Cross-attention: $448 \times 3{,}000 = 1{,}344{,}000$ values per layer
    <li>KV cache: $2 \times 24 \times 448 \times 1024 = 22{,}020{,}096$ values
</ul>

<p>Decoder memory: $\approx 500$ MB</p>

<p><strong>Total Memory (Inference):</strong>
<ul>
    <li>Model parameters: $1.55$B $\times$ 4 bytes = 6.2 GB
    <li>Encoder activations: $\approx 1.3$ GB (with checkpointing)
    <li>Decoder activations: $\approx 0.5$ GB
    <li>KV cache: $\approx 0.1$ GB
    <li><strong>Total: $\approx 8.1$ GB</strong>
</ul>

<p>For FP16: $\approx 4.1$ GB</p>

<p>For INT8 quantization: $\approx 2.1$ GB</p>

<p><strong>Part (d): Compare to Text-Only GPT-2</strong></p>

<p><strong>GPT-2 (1.5B parameters):</strong>
<ul>
    <li>Layers: 48
    <li>Hidden size: 1600
    <li>Attention heads: 25
    <li>Vocabulary: 50,257
    <li>Context length: 1024 tokens
</ul>

<p><strong>Comparison Table:</strong></p>

<table>
<tr><th><strong>Metric</strong></th><th><strong>Whisper-large</strong></th><th><strong>GPT-2 (1.5B)</strong></th></tr>
<tr><td>Total Parameters</td><td>1.55B</td><td>1.5B</td></tr>
<tr><td>Encoder Layers</td><td>24</td><td>N/A</td></tr>
<tr><td>Decoder Layers</td><td>24</td><td>48</td></tr>
<tr><td>Hidden Size</td><td>1024</td><td>1600</td></tr>
<tr><td>Attention Heads</td><td>16</td><td>25</td></tr>
<tr><td>Input Modality</td><td>Audio</td><td>Text</td></tr>
<tr><td>Output Modality</td><td>Text</td><td>Text</td></tr>
<tr><td>Context Length</td><td>3000 (audio) + 448 (text)</td><td>1024 (text)</td></tr>
<tr><td>Memory (FP32)</td><td>8.1 GB</td><td>6.5 GB</td></tr>
<tr><td>Inference Speed</td><td>Slower (audio encoding)</td><td>Faster</td></tr>
</table>

<p><strong>Key Differences:</strong></p>

<ol>
    <li><strong>Architecture:</strong>
    <ul>
        <li>Whisper: Encoder-decoder (like T5)
        <li>GPT-2: Decoder-only
    </ul>
    
    <li><strong>Input Processing:</strong>
    <ul>
        <li>Whisper: Audio $\to$ Log-mel $\to$ Encoder
        <li>GPT-2: Text $\to$ Tokens $\to$ Decoder
    </ul>
    
    <li><strong>Computational Cost:</strong>
    <ul>
        <li>Whisper encoder: $O(T^2 d)$ where $T = 3000$
        <li>GPT-2: $O(n^2 d)$ where $n = 1024$
        <li>Whisper is $\approx 9\times$ more expensive for encoder
    </ul>
    
    <li><strong>Memory Footprint:</strong>
    <ul>
        <li>Whisper: Larger due to long audio sequences
        <li>GPT-2: Smaller, text-only
    </ul>
    
    <li><strong>Use Cases:</strong>
    <ul>
        <li>Whisper: Speech recognition, translation, transcription
        <li>GPT-2: Text generation, completion, summarization
    </ul>
</ol>

<p><strong>Why Whisper Needs Encoder-Decoder:</strong></p>

<ul>
    <li><strong>Cross-modal:</strong> Audio input, text output
    <li><strong>Compression:</strong> Encoder compresses 3000 audio frames
    <li><strong>Attention:</strong> Decoder attends to compressed audio
    <li><strong>Efficiency:</strong> Encoder processes audio once, decoder generates text autoregressively
</ul>

<p><strong>Performance Comparison:</strong></p>

<table>
<tr><th><strong>Task</strong></th><th><strong>Whisper</strong></th><th><strong>GPT-2</strong></th></tr>
<tr><td>Speech Recognition</td><td>Excellent</td><td>N/A</td></tr>
<tr><td>Text Generation</td><td>N/A</td><td>Excellent</td></tr>
<tr><td>Multilingual</td><td>99 languages</td><td>Limited</td></tr>
<tr><td>Robustness</td><td>High (noisy audio)</td><td>N/A</td></tr>
<tr><td>Zero-shot</td><td>Strong</td><td>Strong</td></tr>
</table>

<p><strong>Practical Considerations:</strong></p>

<ol>
    <li><strong>Deployment:</strong>
    <ul>
        <li>Whisper: Requires audio preprocessing
        <li>GPT-2: Simple tokenization
    </ul>
    
    <li><strong>Latency:</strong>
    <ul>
        <li>Whisper: Higher (audio encoding + decoding)
        <li>GPT-2: Lower (text-only)
    </ul>
    
    <li><strong>Hardware:</strong>
    <ul>
        <li>Whisper: Needs GPU for real-time (8+ GB VRAM)
        <li>GPT-2: Can run on CPU for small batches
    </ul>
</ol>

<p><strong>Key Insights:</strong></p>

<ul>
    <li>Whisper and GPT-2 have similar parameter counts but different architectures
    <li>Encoder-decoder is essential for cross-modal tasks
    <li>Audio sequences are much longer than text, requiring more memory
    <li>Both models benefit from large-scale pre-training
    <li>Whisper's multimodal nature enables speech-to-text applications
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 4: Multimodal Fusion for Video Understanding</strong>

<p><strong>Part (a): Proposed Architecture</strong></p>

<pre><code>import torch
import torch.nn as nn

class MultimodalVideoTransformer(nn.Module):
    def __init__(self, 
                 visual_dim=768,      # ViT features
                 audio_dim=512,       # Audio features
                 text_dim=768,        # BERT features
                 hidden_dim=1024,     # Fusion dimension
                 num_layers=12,       # Fusion transformer layers
                 num_heads=16,
                 num_classes=400):    # Action recognition classes
        super().__init__()
        
        # Modality-specific encoders
        self.visual_encoder = VisualEncoder(visual_dim, hidden_dim)
        self.audio_encoder = AudioEncoder(audio_dim, hidden_dim)
        self.text_encoder = TextEncoder(text_dim, hidden_dim)
        
        # Modality-specific tokens
        self.visual_token = nn.Parameter(torch.randn(1, 1, hidden_dim))
        self.audio_token = nn.Parameter(torch.randn(1, 1, hidden_dim))
        self.text_token = nn.Parameter(torch.randn(1, 1, hidden_dim))
        
        # Fusion transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=0.1,
            batch_first=True
        )
        self.fusion_transformer = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=num_layers
        )
        
        # Classification head
        self.classifier = nn.Sequential(
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, num_classes)
        )
    
    def forward(self, visual_features, audio_features, text_features):
        """
        Args:
            visual_features: (B, T_v, D_v) - video frames
            audio_features: (B, T_a, D_a) - audio segments
            text_features: (B, T_t, D_t) - caption tokens
        Returns:
            logits: (B, num_classes)
        """
        B = visual_features.shape[0]
        
        # Encode each modality
        visual_emb = self.visual_encoder(visual_features)  # (B, T_v, H)
        audio_emb = self.audio_encoder(audio_features)     # (B, T_a, H)
        text_emb = self.text_encoder(text_features)        # (B, T_t, H)
        
        # Add modality tokens
        visual_token = self.visual_token.expand(B, -1, -1)
        audio_token = self.audio_token.expand(B, -1, -1)
        text_token = self.text_token.expand(B, -1, -1)
        
        visual_emb = torch.cat([visual_token, visual_emb], dim=1)
        audio_emb = torch.cat([audio_token, audio_emb], dim=1)
        text_emb = torch.cat([text_token, text_emb], dim=1)
        
        # Concatenate all modalities
        multimodal_emb = torch.cat([visual_emb, audio_emb, text_emb], dim=1)
        # Shape: (B, 1+T_v + 1+T_a + 1+T_t, H)
        
        # Fusion transformer
        fused = self.fusion_transformer(multimodal_emb)
        
        # Aggregate: average modality tokens
        visual_rep = fused[:, 0, :]
        audio_rep = fused[:, 1+visual_features.shape[1], :]
        text_rep = fused[:, 1+visual_features.shape[1]+1+audio_features.shape[1], :]
        
        # Combine representations
        combined = (visual_rep + audio_rep + text_rep) / 3
        
        # Classification
        logits = self.classifier(combined)
        
        return logits

class VisualEncoder(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.proj = nn.Linear(input_dim, output_dim)
        self.norm = nn.LayerNorm(output_dim)
    
    def forward(self, x):
        return self.norm(self.proj(x))

class AudioEncoder(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.proj = nn.Linear(input_dim, output_dim)
        self.norm = nn.LayerNorm(output_dim)
    
    def forward(self, x):
        return self.norm(self.proj(x))

class TextEncoder(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.proj = nn.Linear(input_dim, output_dim)
        self.norm = nn.LayerNorm(output_dim)
    
    def forward(self, x):
        return self.norm(self.proj(x))

# Example usage
model = MultimodalVideoTransformer()

# Simulate inputs
batch_size = 4
visual = torch.randn(batch_size, 16, 768)   # 16 frames
audio = torch.randn(batch_size, 32, 512)    # 32 audio segments
text = torch.randn(batch_size, 20, 768)     # 20 caption tokens

logits = model(visual, audio, text)
print(f"Output shape: {logits.shape}")  # (4, 400)
</code></pre>

<p><strong>Part (b): Fusion Mechanism</strong></p>

<p><strong>Architecture Overview:</strong></p>

<pre><code>
Input:
  Visual: (B, 16, 768)  - 16 video frames from ViT
  Audio:  (B, 32, 512)  - 32 audio segments from audio encoder
  Text:   (B, 20, 768)  - 20 caption tokens from BERT

Step 1: Modality-Specific Projection
  Visual -> (B, 16, 1024)
  Audio  -> (B, 32, 1024)
  Text   -> (B, 20, 1024)

Step 2: Add Modality Tokens
  Visual: [V_token, v1, v2, ..., v16]  -> (B, 17, 1024)
  Audio:  [A_token, a1, a2, ..., a32]  -> (B, 33, 1024)
  Text:   [T_token, t1, t2, ..., t20]  -> (B, 21, 1024)

Step 3: Concatenate
  Multimodal: [V_token, v1, ..., v16, A_token, a1, ..., a32, T_token, t1, ..., t20]
  Shape: (B, 71, 1024)

Step 4: Fusion Transformer (12 layers)
  Cross-modal attention enables interaction
  Output: (B, 71, 1024)

Step 5: Aggregate
  Extract modality tokens: V_token, A_token, T_token
  Average: (V_token + A_token + T_token) / 3
  Shape: (B, 1024)

Step 6: Classification
  MLP: (B, 1024) -> (B, 400)
</code></pre>

<p><strong>Fusion Strategies Comparison:</strong></p>

<ol>
    <li><strong>Early Fusion (Concatenation):</strong>
    <ul>
        <li>Concatenate features before transformer
        <li>Simple but limited cross-modal interaction
        <li>Used in this design
    </ul>
    
    <li><strong>Late Fusion (Ensemble):</strong>
    <ul>
        <li>Process modalities separately
        <li>Combine predictions at the end
        <li>No cross-modal learning
    </ul>
    
    <li><strong>Cross-Modal Attention:</strong>
    <ul>
        <li>Visual attends to audio and text
        <li>Audio attends to visual and text
        <li>More complex but better interaction
    </ul>
    
    <li><strong>Bottleneck Fusion:</strong>
    <ul>
        <li>Compress each modality to bottleneck tokens
        <li>Fuse bottlenecks
        <li>More efficient for long sequences
    </ul>
</ol>

<p><strong>Why This Design:</strong></p>

<ul>
    <li><strong>Modality tokens:</strong> Aggregate information from each modality
    <li><strong>Shared transformer:</strong> Enables cross-modal attention
    <li><strong>Flexible:</strong> Can handle missing modalities
    <li><strong>Scalable:</strong> Easy to add more modalities
</ul>

<p><strong>Part (c): Training Objective</strong></p>

<p><strong>Primary Objective: Action Recognition</strong></p>

<p>$\mathcal{L}_{\text{action}} = -\frac{1}{B} \sum_{i=1}^{B} \log P(y_i | \vv_i, \va_i, \vt_i)$</p>

<p>where:
<ul>
    <li>$\vv_i$: visual features for sample $i$
    <li>$\va_i$: audio features for sample $i$
    <li>$\vt_i$: text features for sample $i$
    <li>$y_i$: ground truth action class
</ul>

<p><strong>Auxiliary Objectives (Multi-Task Learning):</strong></p>

<p><strong>1. Contrastive Loss (Cross-Modal Alignment):</strong></p>

<p>Align visual-audio, visual-text, audio-text pairs:</p>

<p>$\mathcal{L}_{\text{contrast}} = \mathcal{L}_{\text{VA}} + \mathcal{L}_{\text{VT}} + \mathcal{L}_{\text{AT}}$</p>

<p>where each term is CLIP-style contrastive loss:</p>

<p>$\mathcal{L}_{\text{VA}} = -\frac{1}{B} \sum_{i=1}^{B} \log \frac{\exp(\text{sim}(\vv_i, \va_i) / \tau)}{\sum_{j=1}^{B} \exp(\text{sim}(\vv_i, \va_j) / \tau)}$</p>

<p><strong>2. Masked Modality Modeling:</strong></p>

<p>Randomly mask one modality and predict it from others:</p>

<p>$\mathcal{L}_{\text{mask}} = \mathcal{L}_{\text{mask-V}} + \mathcal{L}_{\text{mask-A}} + \mathcal{L}_{\text{mask-T}}$</p>

<p>Example (mask visual):
$\mathcal{L}_{\text{mask-V}} = \|\hat{\vv} - \vv\|_2^2$</p>

<p>where $\hat{\vv} = f(\va, \vt)$ is predicted visual features.</p>

<p><strong>3. Temporal Ordering:</strong></p>

<p>Predict correct temporal order of video segments:</p>

<p>$\mathcal{L}_{\text{temporal}} = -\log P(\text{order} | \vv, \va, \vt)$</p>

<p><strong>Total Training Objective:</strong></p>

<p>$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{action}} + \lambda_1 \mathcal{L}_{\text{contrast}} + \lambda_2 \mathcal{L}_{\text{mask}} + \lambda_3 \mathcal{L}_{\text{temporal}}$</p>

<p>Typical weights: $\lambda_1 = 0.1$, $\lambda_2 = 0.05$, $\lambda_3 = 0.05$</p>

<p><strong>Training Recipe:</strong></p>

<pre><code># Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)

# Learning rate schedule
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

# Training loop
for epoch in range(100):
    for batch in dataloader:
        visual, audio, text, labels = batch
        
        # Forward pass
        logits = model(visual, audio, text)
        
        # Action recognition loss
        loss_action = F.cross_entropy(logits, labels)
        
        # Contrastive loss (optional)
        visual_rep = model.get_visual_rep(visual)
        audio_rep = model.get_audio_rep(audio)
        loss_contrast = contrastive_loss(visual_rep, audio_rep)
        
        # Total loss
        loss = loss_action + 0.1 * loss_contrast
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    scheduler.step()
</code></pre>

<p><strong>Data Augmentation:</strong></p>

<ul>
    <li><strong>Visual:</strong> Random crop, color jitter, temporal sampling
    <li><strong>Audio:</strong> Time stretching, pitch shifting, noise injection
    <li><strong>Text:</strong> Synonym replacement, back-translation
    <li><strong>Multimodal:</strong> Random modality dropout (robustness)
</ul>

<p><strong>Part (d): Parameter Count Estimation</strong></p>

<p><strong>Component Breakdown:</strong></p>

<p><strong>1. Modality-Specific Encoders:</strong></p>

<p><em>Visual Encoder:</em>
<ul>
    <li>Projection: $768 \times 1024 = 786{,}432$
    <li>LayerNorm: $2 \times 1024 = 2{,}048$
    <li>Total: $788{,}480$
</ul>

<p><em>Audio Encoder:</em>
<ul>
    <li>Projection: $512 \times 1024 = 524{,}288$
    <li>LayerNorm: $2 \times 1024 = 2{,}048$
    <li>Total: $526{,}336$
</ul>

<p><em>Text Encoder:</em>
<ul>
    <li>Projection: $768 \times 1024 = 786{,}432$
    <li>LayerNorm: $2 \times 1024 = 2{,}048$
    <li>Total: $788{,}480$
</ul>

<p><strong>Encoder total: $2{,}103{,}296$ parameters</strong></p>

<p><strong>2. Modality Tokens:</strong>
<ul>
    <li>3 tokens $\times$ 1024 = $3{,}072$ parameters
</ul>

<p><strong>3. Fusion Transformer (12 layers):</strong></p>

<p>Per layer:
<ul>
    <li>Self-attention: $4 \times 1024^2 = 4{,}194{,}304$
    <li>MLP: $2 \times 1024 \times 4096 = 8{,}388{,}608$
    <li>LayerNorm: $2 \times 2 \times 1024 = 4{,}096$
    <li>Total per layer: $12{,}587{,}008$
</ul>

<p>12 layers: $12 \times 12{,}587{,}008 = 151{,}044{,}096$ parameters</p>

<p><strong>4. Classification Head:</strong>
<ul>
    <li>LayerNorm: $2 \times 1024 = 2{,}048$
    <li>Linear 1: $1024 \times 1024 = 1{,}048{,}576$
    <li>Linear 2: $1024 \times 400 = 409{,}600$
    <li>Total: $1{,}460{,}224$
</ul>

<p><strong>Total Model Parameters:</strong></p>

<p>$2{,}103{,}296 + 3{,}072 + 151{,}044{,}096 + 1{,}460{,}224 = 154{,}610{,}688$</p>

<p><strong>Total: $\approx 155$M parameters</strong></p>

<p><strong>Memory Footprint (FP32):</strong></p>

<ul>
    <li>Parameters: $155$M $\times$ 4 bytes = 620 MB
    <li>Activations (batch size 4):
    <ul>
        <li>Input: $4 \times 71 \times 1024 = 290{,}816$ values
        <li>Per layer: $\approx 2$M values
        <li>Total: $\approx 24$M values $\times$ 4 bytes = 96 MB
    </ul>
    <li>Gradients: 620 MB (same as parameters)
    <li>Optimizer states (AdamW): $2 \times 620$ MB = 1.24 GB
</ul>

<p><strong>Total training memory: $\approx 2.6$ GB</strong></p>

<p><strong>Comparison with Baselines:</strong></p>

<table>
<tr><th><strong>Model</strong></th><th><strong>Parameters</strong></th><th><strong>Modalities</strong></th></tr>
<tr><td>Single-modal (visual only)</td><td>86M</td><td>1</td></tr>
<tr><td>Two-modal (visual + audio)</td><td>120M</td><td>2</td></tr>
<tr><td>Our three-modal</td><td>155M</td><td>3</td></tr>
<tr><td>CLIP (ViT-B/32)</td><td>151M</td><td>2</td></tr>
<tr><td>Whisper-large</td><td>1.55B</td><td>2</td></tr>
</table>

<p><strong>Design Trade-offs:</strong></p>

<ol>
    <li><strong>Parameter efficiency:</strong>
    <ul>
        <li>Shared fusion transformer reduces parameters
        <li>Modality-specific encoders are lightweight
        <li>Could use pre-trained encoders (ViT, BERT, etc.)
    </ul>
    
    <li><strong>Computational cost:</strong>
    <ul>
        <li>Sequence length: 71 tokens (manageable)
        <li>Attention complexity: $O(71^2 \times 1024) \approx 5$M operations
        <li>Inference time: $\approx 50$ ms on GPU
    </ul>
    
    <li><strong>Scalability:</strong>
    <ul>
        <li>Easy to add more modalities (depth, optical flow, etc.)
        <li>Can increase fusion layers for better interaction
        <li>Bottleneck fusion for longer sequences
    </ul>
</ol>

<p><strong>Practical Recommendations:</strong></p>

<ol>
    <li><strong>Use pre-trained encoders:</strong> ViT for visual, Wav2Vec for audio, BERT for text
    <li><strong>Freeze encoders initially:</strong> Train fusion transformer first
    <li><strong>Fine-tune end-to-end:</strong> Unfreeze all parameters later
    <li><strong>Modality dropout:</strong> Randomly drop modalities during training for robustness
    <li><strong>Temporal modeling:</strong> Add temporal attention for video sequences
</ol>

<p><strong>Key Insights:</strong></p>

<ul>
    <li>Multimodal fusion requires careful architecture design
    <li>Modality tokens enable flexible aggregation
    <li>Shared transformer enables cross-modal learning
    <li>Multi-task learning improves representation quality
    <li>Parameter count is reasonable for modern GPUs
    <li>Pre-trained encoders significantly improve performance
</ul>
</div>
        
        <div class="chapter-nav">
  <a href="chapter17_vision_transformers.html">‚Üê Chapter 17: Vision Transformers</a>
  <a href="../../deeptech.html">üìö Table of Contents</a>
  <a href="chapter19_long_context.html">Chapter 19: Long Context Handling ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
