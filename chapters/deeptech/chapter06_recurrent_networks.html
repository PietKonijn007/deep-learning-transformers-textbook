<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 6: Recurrent Neural Networks - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../../deeptech.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Recurrent Neural Networks</h1>

<h2>Chapter Overview</h2>

<p>Recurrent Neural Networks (RNNs) process sequential data by maintaining hidden states that capture information from previous time steps. This chapter develops RNNs from basic recurrence to modern architectures like LSTMs and GRUs, establishing foundations for understanding transformers.</p>

<h3>Learning Objectives</h3>

<ol>
    <li>Understand recurrent architectures for sequential data
    <li>Implement vanilla RNNs, LSTMs, and GRUs
    <li>Understand vanishing/exploding gradient problems
    <li>Apply RNNs to sequence modeling tasks
    <li>Understand bidirectional and multi-layer RNNs
</ol>

<h2>Vanilla RNNs</h2>

<div class="definition"><strong>Definition:</strong> 
An RNN processes sequence $\vx_1, \vx_2, \ldots, \vx_T$ by maintaining hidden state $\vh_t \in \R^h$:
<div class="equation">
$$\begin{align}
\vh_t &= \tanh(\mW_{hh} \vh_{t-1} + \mW_{xh} \vx_t + \vb_h) \\
\vy_t &= \mW_{hy} \vh_t + \vb_y
\end{align}$$
</div>
where:
<ul>
    <li>$\mW_{hh} \in \R^{h \times h}$: hidden-to-hidden weights
    <li>$\mW_{xh} \in \R^{h \times d}$: input-to-hidden weights
    <li>$\mW_{hy} \in \R^{k \times h}$: hidden-to-output weights
    <li>$\vh_0$ initialized (often zeros)

<div class="architecture-diagram">
<h3>RNN/LSTM Forward Pass with Memory</h3>
<pre class="mermaid">
graph LR
    subgraph TimeStep["Each Timestep t"]
        XT["Input x_t<br/> ‚àà ‚Ñù^d"] -->|"W_xh ‚àà ‚Ñù^h x d"| Z["z_t = W_hh*h_t-1<br/> + W_xh*x_t + b_h<br/> ‚àà ‚Ñù^h"]
        HT_prev["Hidden h_t-1<br/> ‚àà ‚Ñù^h"] -->|"W_hh ‚àà ‚Ñù^h x h"| Z
        Z -->|"tanh"| HT["h_t = tanh(z_t)<br/> ‚àà ‚Ñù^h<br/> STORED: z_t for tanh'<br/> STORED: h_t-1 for ‚àÇL/‚àÇW_hh<br/> STORED: x_t for ‚àÇL/‚àÇW_xh"]
        HT -->|"W_hy ‚àà ‚Ñù^k x h<br/> b_y ‚àà ‚Ñù^k"| YT["y_t = W_hy*h_t + b_y<br/> ‚àà ‚Ñù^k"]
    end

    subgraph LSTM_Gates["LSTM Gating (Alternative)"]
        FG["Forget Gate f_t<br/> œÉ(W_f*[h_t-1,x_t])<br/> ‚àà ‚Ñù^h -- STORED"]
        IG["Input Gate i_t<br/> œÉ(W_i*[h_t-1,x_t])<br/> ‚àà ‚Ñù^h -- STORED"]
        OG["Output Gate o_t<br/> œÉ(W_o*[h_t-1,x_t])<br/> ‚àà ‚Ñù^h -- STORED"]
        CS["Cell State c_t<br/> f_t . c_t-1 + i_t . c_tilde<br/> ‚àà ‚Ñù^h -- STORED"]
    end

    style XT fill:#e8f5e9,stroke:#4caf50,color:#000
    style Z fill:#fff3e0,stroke:#ff9800,color:#000
    style HT fill:#e3f2fd,stroke:#2196f3,color:#000
    style YT fill:#f3e5f5,stroke:#9c27b0,color:#000
    style CS fill:#fce4ec,stroke:#e91e63,color:#000
</pre>
<p class="diagram-caption">RNN/LSTM Forward Pass with Memory</p>
</div>

<p></ul>
</div>

<figure>
<div class="tikz-diagram"><img src="../diagrams/chapter06_recurrent_networks_b260b1fb6b03.svg" alt="TikZ Diagram" /></div>
<figcaption>RNN unrolled through time showing sequential dependencies. Each hidden state $\vh_t$ depends on both the current input $\vx_t$ (blue arrows) and the previous hidden state $\vh_{t-1}$ (red arrows). The same weight matrices are shared across all time steps. This sequential structure prevents parallelization: $\vh_2$ cannot be computed until $\vh_1$ is complete.</figcaption>
</figure>

<div class="example"><strong>Example:</strong> 
Character-level language model with vocabulary size $V=5$, hidden size $h=3$.

<p>Input sequence: "hello" encoded as one-hot vectors $\vx_1, \ldots, \vx_5 \in \R^5$</p>

<p>Initialize: $\vh_0 = [0, 0, 0]\transpose$</p>

<p><strong>Time step 1:</strong> Process 'h'
<div class="equation">
$$\begin{align}
\vh_1 &= \tanh(\mW_{hh}\vh_0 + \mW_{xh}\vx_1 + \vb_h) \in \R^3 \\
\vy_1 &= \mW_{hy}\vh_1 + \vb_y \in \R^5 \\
\hat{\mathbf{p}}_1 &= \text{softmax}(\vy_1) \quad \text{(predict next character)}
\end{align}$$
</div>

<p><strong>Time step 2:</strong> Process 'e' using $\vh_1$
<div class="equation">
$$
\vh_2 = \tanh(\mW_{hh}\vh_1 + \mW_{xh}\vx_2 + \vb_h)
$$
</div>

<p>Hidden state $\vh_t$ carries information from all previous time steps.
</div>

<h3>Backpropagation Through Time (BPTT)</h3>

<div class="algorithm"><div class="algorithm-title">Algorithm: Backpropagation Through Time</div>
<div class="algorithm-line"><strong>Input:</strong> Sequence $\{\vx_1, \ldots, \vx_T\}$, targets $\{\vy_1, \ldots, \vy_T\}$</div>
<div class="algorithm-line"><strong>Output:</strong> Gradients for all parameters</div>
<div class="algorithm-line"><span class="algorithm-comment">// Forward Pass</span></div>
<div class="algorithm-line"><strong>for</strong> $t = 1$ to $T$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">$\vh_t = \tanh(\mW_{hh}\vh_{t-1} + \mW_{xh}\vx_t + \vb_h)$</div>
<div class="algorithm-line">$\vy_t = \mW_{hy}\vh_t + \vb_y$</div>
<div class="algorithm-line">$L_t = \text{Loss}(\vy_t, \text{target}_t)$</div>
<div class="algorithm-line">}</p></div>
<div class="algorithm-line"><span class="algorithm-comment">// Backward Pass</span></div>
<div class="algorithm-line">Initialize $\frac{\partial L}{\partial \vh_{T+1}} = \mathbf{0}$</div>
<div class="algorithm-line"><strong>for</strong> $t = T$ to $1$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Compute $\frac{\partial L}{\partial \vh_t}$ (includes gradient from $t+1$)</div>
<div class="algorithm-line">Accumulate $\frac{\partial L}{\partial \mW_{hh}}, \frac{\partial L}{\partial \mW_{xh}}, \frac{\partial L}{\partial \mW_{hy}}$</div>
</div>
</div>
</div>

<h3>Vanishing and Exploding Gradients</h3>

<p>The fundamental challenge in training RNNs on long sequences arises from the multiplicative nature of gradient backpropagation through time. When computing gradients with respect to early hidden states, the chain rule requires multiplying Jacobian matrices across all intermediate time steps, leading to exponential growth or decay of gradient magnitudes.</p>

<p>The gradient of the loss with respect to an early hidden state $\vh_0$ involves the product of Jacobians across all time steps:
<div class="equation">
$$
\frac{\partial \vh_T}{\partial \vh_0} = \prod_{t=1}^{T} \frac{\partial \vh_t}{\partial \vh_{t-1}} = \prod_{t=1}^{T} \mW_{hh}\transpose \text{diag}(\tanh'(\vz_t))
$$
</div>
where $\vz_t$ is the pre-activation at time $t$. Each Jacobian $\frac{\partial \vh_t}{\partial \vh_{t-1}}$ has spectral norm bounded by $\norm{\mW_{hh}} \cdot \norm{\text{diag}(\tanh'(\vz_t))}$. Since $\tanh'(z) \in (0, 1]$ with maximum value 1 at $z = 0$, the derivative term is at most 1 and typically much smaller for saturated activations. This means the Jacobian norm is approximately $\norm{\mW_{hh}}$ in the best case.</p>

<p>For a sequence of length $T = 100$, if $\norm{\mW_{hh}} = 0.95$ (slightly less than 1), the gradient magnitude decays as $0.95^{100} \approx 0.006$, reducing gradients by a factor of 167. If $\norm{\mW_{hh}} = 0.9$, the decay is $0.9^{100} \approx 2.7 \times 10^{-5}$, reducing gradients by a factor of 37,000. This exponential decay makes it nearly impossible for the network to learn long-range dependencies: the gradient signal from time step 100 is effectively zero by the time it reaches time step 0. In practice, vanilla RNNs struggle to learn dependencies longer than 10-20 time steps due to vanishing gradients.</p>

<p>Conversely, if $\norm{\mW_{hh}} = 1.05$, the gradient magnitude grows as $1.05^{100} \approx 131.5$, amplifying gradients by a factor of 131. If $\norm{\mW_{hh}} = 1.1$, the growth is $1.1^{100} \approx 13{,}781$, causing gradients to explode. Exploding gradients lead to numerical overflow (NaN values) and training instability, where loss suddenly spikes to infinity. While gradient clipping (capping gradient norms at a threshold like 1.0) provides a practical solution for exploding gradients, it does not address the fundamental problem of vanishing gradients.</p>

<p>The vanishing gradient problem is particularly severe because the spectral norm of $\mW_{hh}$ must be precisely 1.0 to avoid both vanishing and exploding gradients, and maintaining this property during training is extremely difficult. Initialization schemes like orthogonal initialization set $\mW_{hh}$ to have spectral norm 1.0 initially, but gradient descent updates quickly perturb this property. Even with careful initialization, vanilla RNNs rarely learn dependencies beyond 20-30 time steps in practice.</p>

<h3>Quantitative Analysis of Gradient Decay</h3>

<p>To understand the severity of vanishing gradients, consider a concrete example with BERT-base dimensions. Suppose we have a vanilla RNN with hidden dimension $h = 768$ (matching BERT-base) and sequence length $n = 512$ (BERT's maximum sequence length). The recurrence matrix $\mW_{hh} \in \R^{768 \times 768}$ has 589,824 parameters. If we initialize $\mW_{hh}$ orthogonally (spectral norm exactly 1.0) and the $\tanh$ derivatives average 0.5 (typical for non-saturated activations), the effective Jacobian norm per time step is approximately $1.0 \times 0.5 = 0.5$.</p>

<p>Over 512 time steps, the gradient magnitude decays as $0.5^{512} \approx 10^{-154}$, which is far below machine precision for FP32 (approximately $10^{-38}$) or even FP64 (approximately $10^{-308}$). The gradient effectively becomes exactly zero after about 130 time steps in FP32 or 1,000 time steps in FP64. This means a vanilla RNN cannot learn any dependencies spanning more than 130 tokens when using FP32 arithmetic, regardless of optimization algorithm or learning rate. The mathematical structure of the recurrence fundamentally limits the learnable dependency length.</p>

<p>For comparison, consider the gradient flow in a transformer with the same dimensions. The self-attention mechanism computes attention scores $\mA = \text{softmax}(\frac{\mQ\mK\transpose}{\sqrt{d_k}})$ and outputs $\mO = \mA\mV$. The gradient $\frac{\partial L}{\partial \mV}$ flows directly from the output through the attention weights, without any multiplicative accumulation across time steps. The gradient magnitude remains approximately constant regardless of sequence length, enabling transformers to learn dependencies spanning thousands of tokens. This fundamental difference in gradient flow explains why transformers replaced RNNs for nearly all sequence modeling tasks: they solve the vanishing gradient problem by design.</p>

<p>The LSTM architecture addresses vanishing gradients through its cell state mechanism, which provides an additive path for gradient flow. The cell state update $\mathbf{c}_t = \vf_t \odot \mathbf{c}_{t-1} + \vi_t \odot \tilde{\mathbf{c}}_t$ includes an additive term rather than purely multiplicative updates. The gradient with respect to $\mathbf{c}_{t-1}$ is:
<div class="equation">
$$
\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}} = \text{diag}(\vf_t)
$$
</div>
which is a diagonal matrix with entries in $(0, 1)$ determined by the forget gate. If the forget gate learns to output values close to 1 for important information, the gradient can flow backward through many time steps without vanishing. However, this requires the network to learn appropriate forget gate values, and in practice, LSTMs still struggle with dependencies beyond 100-200 time steps. The cell state provides a highway for gradients, but it does not eliminate the vanishing gradient problem entirely.</p>

<h2>Long Short-Term Memory (LSTM)</h2>

<div class="definition"><strong>Definition:</strong> 
LSTM uses gating mechanisms to control information flow:
<div class="equation">
$$\begin{align}
\vf_t &= \sigma(\mW_f[\vh_{t-1}, \vx_t] + \vb_f) && \text{(forget gate)} \\
\vi_t &= \sigma(\mW_i[\vh_{t-1}, \vx_t] + \vb_i) && \text{(input gate)} \\
\tilde{\mathbf{c}}_t &= \tanh(\mW_c[\vh_{t-1}, \vx_t] + \vb_c) && \text{(candidate cell)} \\
\mathbf{c}_t &= \vf_t \odot \mathbf{c}_{t-1} + \vi_t \odot \tilde{\mathbf{c}}_t && \text{(cell state)} \\
\vo_t &= \sigma(\mW_o[\vh_{t-1}, \vx_t] + \vb_o) && \text{(output gate)} \\
\vh_t &= \vo_t \odot \tanh(\mathbf{c}_t) && \text{(hidden state)}
\end{align}$$
</div>
where $\sigma$ is sigmoid, $\odot$ is element-wise multiplication, and $[\cdot, \cdot]$ is concatenation.
</div>

<p><strong>Key components:</strong>
<ul>
    <li><strong>Cell state $\mathbf{c}_t$:</strong> Long-term memory, flows with minimal modification
    <li><strong>Forget gate $\vf_t$:</strong> What to remove from cell state
    <li><strong>Input gate $\vi_t$:</strong> What new information to store
    <li><strong>Output gate $\vo_t$:</strong> What to output from cell state
</ul>

<figure>
<div class="tikz-diagram"><img src="../diagrams/chapter06_recurrent_networks_70319ea06696.svg" alt="TikZ Diagram" /></div>
<figcaption>LSTM cell internal structure showing the four gates and cell state flow. The cell state $\mathbf{c}_t$ (red path) provides an additive "highway" for gradient flow, addressing the vanishing gradient problem. The forget gate $\vf_t$ controls what to keep from $\mathbf{c}_{t-1}$, the input gate $\vi_t$ and candidate $\tilde{\mathbf{c}}_t$ control what new information to add, and the output gate $\vo_t$ controls what to output. All gates receive the same input $[\vh_{t-1}, \vx_t]$ but learn different transformations.</figcaption>
</figure>

<div class="example"><strong>Example:</strong> 
For input dimension $d=512$ and hidden dimension $h=1024$:

<p>Each gate has weight matrix for $[\vh_{t-1}, \vx_t] \in \R^{h+d}$:
<div class="equation">
$$\begin{align}
\text{Single gate:} \quad &(h+d) \times h + h = (1024 + 512) \times 1024 + 1024 \\
&= 1{,}572{,}864 + 1{,}024 = 1{,}573{,}888
\end{align}$$
</div>

<p>LSTM has 4 gates (forget, input, cell, output):
<div class="equation">
$$
\text{Total:} \quad 4 \times 1{,}573{,}888 = 6{,}295{,}552 \text{ parameters}
$$
</div>

<p>Compare to transformer attention with same dimensions: often fewer parameters and better parallelization!
</div>

<h3>LSTM Computational Analysis</h3>

<p>Understanding the computational cost of LSTMs is essential for comparing them to transformers and explaining why transformers have become dominant despite LSTMs' theoretical advantages for sequential processing. The LSTM's gating mechanisms provide powerful modeling capabilities but come with significant computational overhead that limits their efficiency on modern hardware.</p>

<p>For an LSTM with input dimension $d$ and hidden dimension $h$, each time step requires computing four gates (forget, input, candidate, output), each involving a matrix multiplication with the concatenated input $[\vh_{t-1}, \vx_t] \in \R^{h+d}$. The computational cost per time step is:
<div class="equation">
$$
\text{FLOPs per step} = 4 \times 2h(h+d) = 8h(h+d)
$$
</div>
where the factor of 2 accounts for multiply-accumulate operations, and the factor of 4 accounts for the four gates. For BERT-base dimensions with $d = h = 768$, this gives $8 \times 768 \times (768 + 768) = 9{,}437{,}184$ FLOPs per time step. For a sequence of length $n = 512$, the total cost is $512 \times 9{,}437{,}184 = 4{,}831{,}838{,}208$ FLOPs, or approximately 4.8 GFLOPs.</p>

<p>This computational cost is deceptively modest compared to transformers. A single transformer layer with the same dimensions requires approximately 12.9 GFLOPs for self-attention (with $n = 512$) plus 9.4 GFLOPs for the feed-forward network, totaling 22.3 GFLOPs‚Äîabout 4.6√ó more than the LSTM. However, this comparison is misleading because it ignores the critical difference in parallelization: the transformer can process all 512 positions simultaneously, while the LSTM must process them sequentially.</p>

<p>The sequential nature of LSTMs means that the 4.8 GFLOPs cannot be parallelized across time steps. On an NVIDIA A100 GPU with peak throughput of 312 TFLOPS (FP16), the theoretical minimum time to process a sequence of length 512 is $\frac{4.8 \times 10^9}{312 \times 10^{12}} = 15.4$ microseconds if we could achieve perfect parallelization. However, the sequential dependency forces us to process one time step at a time, with each step taking approximately $\frac{9.4 \times 10^6}{312 \times 10^{12}} = 0.03$ microseconds at peak throughput. In practice, small matrix multiplications achieve only 1-5\% of peak throughput due to insufficient parallelism, so each time step actually takes approximately 1-3 microseconds, giving a total sequence processing time of 512-1,536 microseconds (0.5-1.5 milliseconds).</p>

<p>For comparison, a transformer layer can process the entire sequence in parallel. The self-attention computation requires three matrix multiplications ($\mQ = \mX\mW_Q$, $\mK = \mX\mW_K$, $\mV = \mX\mW_V$) with dimensions $512 \times 768 \times 768$, followed by the attention score computation $\mA = \text{softmax}(\frac{\mQ\mK\transpose}{\sqrt{d_k}})$ and output computation $\mO = \mA\mV$. These operations can be batched into large matrix multiplications that achieve 40-60\% of peak GPU throughput, completing in approximately 50-100 microseconds total. The transformer is 5-30√ó faster than the LSTM despite having more FLOPs, purely due to better parallelization.</p>

<p>The memory requirements for LSTM hidden states are modest compared to transformer attention matrices. For batch size $B$ and sequence length $n$, the LSTM must store hidden states $\vh_t \in \R^{B \times h}$ and cell states $\mathbf{c}_t \in \R^{B \times h}$ for each time step, requiring $2Bnh \times 4 = 8Bnh$ bytes in FP32. For BERT-base dimensions with $B = 32$, $n = 512$, $h = 768$, this totals $8 \times 32 \times 512 \times 768 = 100{,}663{,}296$ bytes, or approximately 96 MB. This is substantially less than the 384 MB required for transformer attention scores in a single layer, making LSTMs more memory-efficient for long sequences.</p>

<p>However, this memory advantage is offset by the sequential processing requirement. While transformers can trade memory for speed by using gradient checkpointing (recomputing activations during the backward pass rather than storing them), LSTMs cannot benefit from this technique as effectively because the sequential dependency prevents parallelization of the recomputation. Gradient checkpointing reduces transformer memory by 3-5√ó with only 20-30\% slowdown, but for LSTMs, the slowdown is 2-3√ó because the recomputation cannot be parallelized. This makes gradient checkpointing less attractive for LSTMs, limiting their ability to scale to very long sequences.</p>

<h2>Gated Recurrent Unit (GRU)</h2>

<div class="definition"><strong>Definition:</strong> 
GRU simplifies LSTM by merging cell and hidden states:
<div class="equation">
$$\begin{align}
\vz_t &= \sigma(\mW_z[\vh_{t-1}, \vx_t] + \vb_z) && \text{(update gate)} \\
\vr_t &= \sigma(\mW_r[\vh_{t-1}, \vx_t] + \vb_r) && \text{(reset gate)} \\
\tilde{\vh}_t &= \tanh(\mW_h[\vr_t \odot \vh_{t-1}, \vx_t] + \vb_h) && \text{(candidate)} \\
\vh_t &= (1 - \vz_t) \odot \vh_{t-1} + \vz_t \odot \tilde{\vh}_t && \text{(hidden state)}
\end{align}$$
</div>
</div>

<p><strong>Advantages over LSTM:</strong>
<ul>
    <li>Fewer parameters (3 gates vs 4)
    <li>Simpler architecture
    <li>Often similar performance
    <li>Faster training
</ul>

<h2>Bidirectional RNNs</h2>

<div class="definition"><strong>Definition:</strong> 
Process sequence in both directions:
<div class="equation">
$$\begin{align}
\overrightarrow{\vh}_t &= \text{RNN}_{\text{forward}}(\vx_t, \overrightarrow{\vh}_{t-1}) \\
\overleftarrow{\vh}_t &= \text{RNN}_{\text{backward}}(\vx_t, \overleftarrow{\vh}_{t+1}) \\
\vh_t &= [\overrightarrow{\vh}_t; \overleftarrow{\vh}_t]
\end{align}$$
</div>
</div>

<p>Bidirectional RNNs capture context from both past and future, useful when entire sequence is available (not for online/causal tasks).</p>

<p><strong>Example:</strong> BERT uses bidirectional transformers (attention, not RNN), capturing full context.</p>

<h2>RNN Applications</h2>

<p><strong>Sequence-to-Sequence:</strong>
<ul>
    <li>Machine translation: Encoder RNN $\to$ Decoder RNN
    <li>Text summarization
    <li>Speech recognition
</ul>

<p><strong>Sequence Labeling:</strong>
<ul>
    <li>Part-of-speech tagging
    <li>Named entity recognition
    <li>Output at each time step
</ul>

<p><strong>Sequence Generation:</strong>
<ul>
    <li>Language modeling
    <li>Music generation
    <li>Sample from output distribution
</ul>

<h2>RNNs vs Transformers: A Computational Comparison</h2>

<p>The transition from RNNs to transformers represents one of the most significant architectural shifts in deep learning. The following table summarizes the key computational differences:</p>

<div style="text-align: center;">

<table>
<tr><th><strong>Property</strong></th><th><strong>RNNs</strong></th><th><strong>Transformers</strong></th></tr>
<tr><td>Computation</td><td>Sequential ($n$ steps)</td><td>Parallel (constant depth)</td></tr>
<tr><td>Memory scaling</td><td>$O(nd)$</td><td>$O(n^2 + nd)$</td></tr>
<tr><td>GPU utilization</td><td>1--5\%</td><td>40--60\%</td></tr>
<tr><td>Memory bandwidth</td><td>Reload weights each step</td><td>Load weights once</td></tr>
<tr><td>Training time (BERT-scale)</td><td>Estimated 100--200 days</td><td>4 days</td></tr>
</table>

<p></div>

<p>The fundamental bottleneck of RNNs is sequential processing: each hidden state $\vh_t$ depends on $\vh_{t-1}$, preventing parallelization across time steps. On an A100 GPU with 6,912 CUDA cores, an LSTM processing batch size 32 utilizes only $\sim$0.5\% of parallel capacity. Transformers eliminate this bottleneck by computing all positions simultaneously via matrix multiplication, achieving 15$\times$ or greater speedup despite having more total FLOPs.</p>

<p>The parallelization advantage compounds with hardware efficiency: transformers achieve high arithmetic intensity through data reuse in matrix multiplications ($\sim$256 FLOPs/byte), while RNNs perform small matrix-vector products with low data reuse ($\sim$1--10 FLOPs/byte). The combined effect is 100--500$\times$ faster training for equivalent model capacity.</p>

<div class="keypoint">
Transformers dominate modern sequence modeling due to parallel computation (5--30$\times$ speedup), superior hardware utilization (40--60\% vs 1--5\%), and direct long-range gradient flow. RNNs retain advantages for online/streaming applications and extremely long sequences where $O(n^2)$ attention memory is prohibitive. Efficient attention mechanisms (Chapter~16) increasingly address this limitation.
</div>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> For vanilla RNN with input dim $d=128$, hidden dim $h=256$, and sequence length $T=50$: (1) Count total parameters in $\mW_{hh}$, $\mW_{xh}$, and $\mW_{hy}$, (2) Compute total FLOPs for forward pass through all time steps, (3) Estimate GPU utilization on an A100 (312 TFLOPS peak) with batch size 32, assuming each time step achieves 2\% of peak throughput. Why is utilization so low?
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Derive the gradient $\frac{\partial L}{\partial \mW_{hh}}$ for a 3-step sequence. Show how the gradient involves products of Jacobians $\frac{\partial \vh_t}{\partial \vh_{t-1}}$. If $\norm{\mW_{hh}} = 0.9$ and $\tanh'$ averages 0.5, compute the gradient magnitude decay factor from time step 3 to time step 0. At what sequence length would gradients vanish below FP32 precision ($10^{-38}$)?
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> Compare parameter counts and FLOPs per sequence for: (1) LSTM with $d=512$, $h=512$, $n=512$, (2) GRU with $d=512$, $h=512$, $n=512$, (3) Transformer attention layer with $d_{\text{model}}=512$, $d_k=64$, $h=8$ heads, $n=512$. Which architecture has the most parameters? Which has the most FLOPs? Which achieves the highest GPU utilization and why?
</div>

<div class="exercise" id="exercise-4"><strong>Exercise 4:</strong> Implement bidirectional LSTM in PyTorch for sequence "The cat sat on the mat" with vocabulary size 10, embedding dim 16, hidden dim 32. Process the sequence and show output dimensions. Compute the total memory required for hidden states and cell states in FP32. How does this compare to the memory required for attention scores in a transformer with the same dimensions?
</div>

<div class="exercise" id="exercise-5"><strong>Exercise 5:</strong> For BERT-base dimensions ($d=768$, $n=512$), compute: (1) Memory required for LSTM hidden and cell states across 12 layers with batch size 32, (2) Memory required for transformer attention scores across 12 layers with batch size 32 and 12 attention heads, (3) The sequence length at which LSTM memory equals transformer memory. Explain why transformers are memory-limited for long sequences while LSTMs are compute-limited.
</div>

<div class="exercise" id="exercise-6"><strong>Exercise 6:</strong> Estimate the training time for a 110M parameter LSTM on 16 billion tokens (sequence length 512) using 64 TPU cores with 2,880 TFLOPS total peak throughput. Assume the LSTM achieves 3\% of peak throughput due to sequential processing. Compare this to BERT-base training time of 4 days on the same hardware. What is the speedup factor? Explain the three main reasons for the difference: parallelization, memory bandwidth, and GPU utilization.
</div>

<h2>Solutions</h2>

<p>Full solutions for all exercises are available at \url{https://deeplearning.hofkensvermeulen.be}.</p>

<div class="solution"><strong>Solution:</strong> For vanilla RNN with $d=128$, $h=256$, $T=50$:

<p><strong>(1) Parameter count:</strong>
<ul>
    <li>$\mW_{xh} \in \R^{h \times d}$: $256 \times 128 = 32{,}768$ parameters
    <li>$\mW_{hh} \in \R^{h \times h}$: $256 \times 256 = 65{,}536$ parameters
    <li>$\mW_{hy} \in \R^{V \times h}$: Assuming vocabulary $V=10{,}000$: $10{,}000 \times 256 = 2{,}560{,}000$ parameters
    <li>Biases: $h + h + V = 256 + 256 + 10{,}000 = 10{,}512$ parameters
    <li>Total: $32{,}768 + 65{,}536 + 2{,}560{,}000 + 10{,}512 = 2{,}668{,}816$ parameters
</ul>

<p><strong>(2) FLOPs for forward pass:</strong>
Per time step:
<ul>
    <li>$\mW_{xh}\vx_t$: $2 \times h \times d = 2 \times 256 \times 128 = 65{,}536$ FLOPs
    <li>$\mW_{hh}\vh_{t-1}$: $2 \times h \times h = 2 \times 256 \times 256 = 131{,}072$ FLOPs
    <li>$\tanh$ activation: $h = 256$ FLOPs
    <li>Per time step total: $\approx 196{,}864$ FLOPs
</ul>

<p>For $T=50$ time steps: $50 \times 196{,}864 = 9{,}843{,}200 \approx 9.8$ MFLOPs</p>

<p><strong>(3) GPU utilization with batch size 32:</strong>
<ul>
    <li>Total FLOPs per batch: $32 \times 9.8 \text{ MFLOPs} = 313.6$ MFLOPs
    <li>At 2\% peak throughput: $0.02 \times 312 \text{ TFLOPS} = 6.24$ TFLOPS
    <li>Time per batch: $\frac{313.6 \text{ MFLOPs}}{6.24 \text{ TFLOPS}} = 0.05$ ms
</ul>

<p><strong>Why utilization is so low:</strong>
<ul>
    <li>Sequential dependency: Each time step depends on previous, preventing parallelization
    <li>Small matrix operations: $256\times256$ matrices don't saturate GPU
    <li>Memory-bound: Constantly loading/storing hidden states
    <li>Low arithmetic intensity: Few operations per memory access
    <li>Kernel launch overhead dominates for small operations
</ul>
</div>

<div class="solution"><strong>Solution:</strong> For 3-step RNN sequence, the gradient involves backpropagation through time (BPTT):

<p><strong>Forward pass:</strong>
<div class="equation">
$$\begin{align}
\vh_1 &= \tanh(\mW_{hh}\vh_0 + \mW_{xh}\vx_1) \\
\vh_2 &= \tanh(\mW_{hh}\vh_1 + \mW_{xh}\vx_2) \\
\vh_3 &= \tanh(\mW_{hh}\vh_2 + \mW_{xh}\vx_3)
\end{align}$$
</div>

<p><strong>Gradient derivation:</strong>
<div class="equation">
$$\begin{align}
\frac{\partial L}{\partial \mW_{hh}} &= \sum_{t=1}^{3} \frac{\partial L}{\partial \vh_t} \frac{\partial \vh_t}{\partial \mW_{hh}}
\end{align}$$
</div>

<p>The gradient involves products of Jacobians:
<div class="equation">
$$
\frac{\partial \vh_t}{\partial \vh_{t-1}} = \text{diag}(\tanh'(\vz_t)) \mW_{hh}
$$
</div>

<p><strong>Gradient magnitude decay:</strong>
With $\norm{\mW_{hh}} = 0.9$ and $\tanh'$ averaging 0.5:
<div class="equation">
$$
\norm{\frac{\partial \vh_t}{\partial \vh_{t-1}}} \approx 0.5 \times 0.9 = 0.45
$$
</div>

<p>From time step 3 to 0 (3 steps back):
<div class="equation">
$$
\text{Decay factor} = 0.45^3 \approx 0.091
$$
</div>

<p><strong>Vanishing gradient threshold:</strong>
For gradients to vanish below $10^{-38}$:
<div class="equation">
$$\begin{align}
0.45^T &< 10^{-38} \\
T &> \frac{-38 \log(10)}{\log(0.45)} \approx 110 \text{ steps}
\end{align}$$
</div>

<p>Gradients vanish below FP32 precision after approximately 110 time steps.
</div>

<div class="solution"><strong>Solution:</strong> For $d=512$, $h=512$, $n=512$:

<p><strong>(1) LSTM:</strong> $2{,}099{,}200$ parameters, $2.15$ GFLOPs</p>

<p><strong>(2) GRU:</strong> $1{,}574{,}400$ parameters, $1.61$ GFLOPs</p>

<p><strong>(3) Transformer:</strong> $1{,}048{,}576$ parameters, $1.61$ GFLOPs</p>

<p><strong>Most parameters:</strong> LSTM (2.1M)</p>

<p><strong>Most FLOPs:</strong> LSTM (2.15 GFLOPs)</p>

<p><strong>Highest GPU utilization:</strong> Transformer (40-60\% vs 2-5\% for RNNs) due to full parallelization across sequence length and large matrix operations.
</div>

<div class="solution"><strong>Solution:</strong> For bidirectional LSTM with 6 tokens, embedding dim 16, hidden dim 32:

<p><strong>Output dimensions:</strong> $6 \times 64$ (concatenated forward and backward)</p>

<p><strong>Memory for LSTM states:</strong>
<ul>
    <li>Forward hidden + cell: $2 \times 6 \times 32 \times 4 = 1{,}536$ bytes
    <li>Backward hidden + cell: $1{,}536$ bytes
    <li>Total: $3{,}072$ bytes $\approx 3$ KB
</ul>

<p><strong>Transformer attention scores (8 heads):</strong> $8 \times 6 \times 6 \times 4 = 1{,}152$ bytes</p>

<p>LSTM requires 2.7√ó more memory for this small sequence, but attention scales as $O(n^2)$ vs $O(n)$ for LSTM.
</div>

<div class="solution"><strong>Solution:</strong> For BERT-base ($d=768$, $n=512$), batch size 32:

<p><strong>(1) LSTM memory (12 layers):</strong> $\approx 1.13$ GB</p>

<p><strong>(2) Transformer attention (12 layers):</strong> $\approx 4.5$ GB</p>

<p><strong>(3) Equal memory at:</strong> $n = 128$ tokens</p>

<p>For $n>128$, transformers use more memory due to $O(n^2)$ attention scores. Transformers are memory-limited for long sequences, while LSTMs are compute-limited due to sequential processing.
</div>

<div class="solution"><strong>Solution:</strong> <strong>LSTM training time:</strong> $\approx 8.4$ days

<p><strong>BERT training time:</strong> 4 days</p>

<p><strong>Speedup:</strong> 2.1√ó (BERT is faster)</p>

<p><strong>Three main reasons:</strong>
<ol>
    <li>Parallelization: BERT processes all tokens in parallel ($\approx 10\times$ speedup)
    <li>Memory bandwidth: BERT has higher arithmetic intensity ($\approx 3\times$ better)
    <li>GPU utilization: BERT achieves 40-60\% vs 2-5\% for LSTM ($\approx 17\times$ better)
</ol>
</div>
        
        <div class="chapter-nav">
  <a href="chapter05_convolutional_networks.html">‚Üê Chapter 5: Convolutional Neural Networks</a>
  <a href="../../deeptech.html">üìö Table of Contents</a>
  <a href="chapter07_attention_fundamentals.html">Chapter 7: Attention Mechanisms: Fundamentals ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
