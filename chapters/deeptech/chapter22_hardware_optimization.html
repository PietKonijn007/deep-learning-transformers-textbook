<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 22: Hardware Optimization - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../../deeptech.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Hardware Optimization and Deployment</h1>

<h2>Chapter Overview</h2>

<p>Deploying transformers efficiently requires understanding hardware architectures, optimization techniques, and deployment strategies. This chapter covers GPUs, TPUs, model quantization, pruning, distillation, and production deployment best practices.</p>

<h3>Learning Objectives</h3>

<ol>
    <li>Understand GPU/TPU architectures for transformers
    <li>Apply model quantization (INT8, FP16)
    <li>Implement pruning and sparsity
    <li>Use knowledge distillation for compression
    <li>Optimize inference latency and throughput
    <li>Deploy models in production environments
</ol>

<h2>Hardware Architectures</h2>

<h3>GPU Architecture for Deep Learning</h3>

<p>Modern GPUs contain two primary types of compute units that are critical for transformer training and inference. CUDA cores are general-purpose floating-point units that can execute arbitrary arithmetic operations, while Tensor Cores are specialized matrix multiplication units designed specifically for deep learning workloads. Understanding the distinction between these units is essential for achieving optimal performance.</p>

<p><strong>CUDA Cores vs Tensor Cores.</strong> CUDA cores provide flexibility for general computation but operate at lower throughput for matrix operations. A single NVIDIA A100 GPU contains 6912 CUDA cores capable of 19.5 TFLOPS at FP32 precision. In contrast, Tensor Cores are specialized hardware units that perform fused multiply-add operations on small matrix tiles. The same A100 GPU contains 432 third-generation Tensor Cores that deliver 312 TFLOPS for FP16 matrix multiplication, representing a 16√ó advantage over CUDA cores for this specific operation. This dramatic performance difference makes Tensor Cores essential for transformer workloads, which are dominated by matrix multiplications in attention mechanisms and feed-forward layers.</p>

<p><strong>Memory Hierarchy.</strong> GPU memory is organized in a hierarchy that trades capacity for access speed. At the top of the hierarchy, each streaming multiprocessor (SM) contains registers that provide the fastest access with approximately 256KB of storage per SM. These registers are private to individual threads and have single-cycle latency. The next level is shared memory, a software-managed cache that allows threads within a block to communicate efficiently. The A100 provides 164KB of shared memory per SM with latency of approximately 20-30 cycles. Below this sits the L2 cache, a 40MB hardware-managed cache shared across all SMs with latency around 200 cycles. Finally, high-bandwidth memory (HBM) provides the largest capacity at 40-80GB but with the highest latency of 300-400 cycles. This hierarchy means that keeping data in faster memory levels is critical for performance.</p>

<p>The memory bandwidth available at each level determines how quickly data can be moved. The A100's HBM provides 1.6 TB/s of bandwidth, while the V100 provides 900 GB/s. Although these numbers seem large, they are often the bottleneck for transformer operations. Consider that the A100's Tensor Cores can consume data at a rate of 312 TFLOPS √ó 2 bytes (FP16) = 624 TB/s if fully utilized, far exceeding the 1.6 TB/s that HBM can supply. This mismatch between compute capability and memory bandwidth is why memory optimization is crucial for transformers.</p>

<p><strong>Streaming Multiprocessors.</strong> The A100 contains 108 streaming multiprocessors, each capable of executing multiple thread blocks concurrently. Each SM has its own register file, shared memory, and L1 cache, along with 4 Tensor Cores. The SM scheduler can switch between thread warps (groups of 32 threads) with zero overhead, hiding memory latency by executing other warps while some wait for data. Achieving high occupancy, defined as the ratio of active warps to maximum possible warps, is essential for hiding latency and maximizing throughput.</p>

<h3>Computational Intensity</h3>

<div class="definition"><strong>Definition:</strong> 
<div class="equation">
$$
\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes transferred}}
$$
</div>
</div>

<p><strong>For attention:</strong>
<ul>
    <li>$\mQ \mK\transpose$: Intensity = $\frac{2n^2d}{2nd + 2nd} = \frac{nd}{2d} = \frac{n}{2}$
    <li>For small $n$ (< 1024): Memory-bound
    <li>For large $n$ (> 4096): Compute-bound
</ul>

<h3>Tensor Core Optimization</h3>

<p>Tensor Cores achieve their peak performance only when specific conditions are met regarding data types, matrix dimensions, and memory layout. Understanding these requirements is essential for extracting maximum performance from modern GPUs.</p>

<p><strong>Precision Requirements.</strong> Tensor Cores support several precision modes, each with different performance characteristics. FP16 (half precision) provides 312 TFLOPS on the A100, making it the standard choice for training. BF16 (bfloat16) offers the same throughput but with a larger dynamic range that better matches FP32, reducing the need for loss scaling during mixed-precision training. For inference, INT8 provides 624 TOPS, doubling throughput at the cost of reduced precision. The choice of precision involves trading off between speed, memory usage, and numerical accuracy.</p>

<p><strong>Dimension Requirements.</strong> Tensor Cores operate on small matrix tiles and achieve peak performance when matrix dimensions are multiples of specific values. For FP16 operations, dimensions should be multiples of 8, while BF16 and INT8 operations prefer multiples of 16. When dimensions are not multiples of these values, the hardware must pad matrices or fall back to slower execution paths. For example, a matrix multiplication with dimensions 1023√ó1023 will perform significantly worse than 1024√ó1024 because the former requires padding or partial tile operations.</p>

<div class="example"><strong>Example:</strong> 
Consider BERT-base with hidden size $d=768$ and 12 attention heads. The per-head dimension is $d_k = 768/12 = 64$, which is a multiple of 8, allowing efficient Tensor Core usage. The query, key, and value projections have shape $[b \times n, 768] \times [768, 768]$ where $b$ is batch size and $n$ is sequence length.

<p><strong>Baseline configuration:</strong> Batch size 16, sequence length 128, FP32 precision
<ul>
    <li>Throughput: 145 sequences/second
    <li>GPU utilization: 42\%
    <li>Memory bandwidth: 890 GB/s (56\% of peak)
</ul>

<p><strong>Optimized configuration:</strong> Batch size 32, sequence length 128, FP16 with Tensor Cores
<ul>
    <li>Throughput: 520 sequences/second (3.6√ó improvement)
    <li>GPU utilization: 78\%
    <li>Memory bandwidth: 1.45 TB/s (91\% of peak)
    <li>Tensor Core utilization: 85\%
</ul>

<p>The optimization involved: (1) switching to FP16 to enable Tensor Cores, (2) increasing batch size to improve arithmetic intensity, and (3) ensuring all matrix dimensions are multiples of 8. The result is a 3.6√ó speedup with negligible accuracy loss when using mixed-precision training with loss scaling.
</div>

<p><strong>Achieving Peak Performance.</strong> Reaching 90\% of theoretical peak TFLOPS requires careful attention to several factors. First, ensure sufficient work is available by using large batch sizes or long sequences to keep all Tensor Cores busy. Second, minimize memory transfers by fusing operations and reusing data in shared memory. Third, maintain high occupancy by using appropriate thread block sizes and avoiding resource limitations. Finally, profile the application to identify bottlenecks using tools like NVIDIA Nsight Compute, which can show Tensor Core utilization and identify whether operations are compute-bound or memory-bound.</p>

<h3>TPU Architecture</h3>

<p><strong>Tensor Processing Units (TPUs):</strong>
<ul>
    <li>Systolic array architecture
    <li>Optimized for matrix multiplications
    <li>High memory bandwidth (900 GB/s, TPU v4)
    <li>275 TFLOPS (bfloat16)
</ul>

<p><strong>TPU vs GPU:</strong></p>

<table>
<tr><th><strong>Aspect</strong></th><th><strong>GPU</strong></th><th><strong>TPU</strong></th></tr>
<tr><td>Flexibility</td><td>High (general purpose)</td><td>Medium (ML-specific)</td></tr>
<tr><td>Peak FLOPS</td><td>312 (A100 FP16)</td><td>275 (v4 bf16)</td></tr>
<tr><td>Memory</td><td>40-80 GB</td><td>32 GB (per chip)</td></tr>
<tr><td>Batch size</td><td>Medium-Large</td><td>Very Large</td></tr>
<tr><td>Best for</td><td>Flexibility, research</td><td>Large-scale training</td></tr>
</table>

<h2>Memory Optimization Techniques</h2>

<p>Memory access patterns have a profound impact on GPU performance, often determining whether an operation runs at 10\% or 90\% of peak throughput. This section explores the key memory optimization techniques that are essential for efficient transformer implementations.</p>

<h3>Coalesced Memory Access</h3>

<p>When threads in a warp access global memory, the hardware attempts to combine these accesses into a single transaction. Coalesced access occurs when consecutive threads access consecutive memory locations, allowing the hardware to issue one memory transaction instead of 32 separate ones. For example, if thread 0 accesses address 0, thread 1 accesses address 4, thread 2 accesses address 8, and so on (assuming 4-byte elements), the hardware can coalesce these into a single 128-byte transaction. In contrast, if threads access random or strided locations, each access may require a separate transaction, reducing effective bandwidth by up to 32√ó.</p>

<p>For transformer operations, coalesced access is particularly important in matrix multiplications and attention computations. When loading a row of the query matrix, ensuring that consecutive threads load consecutive elements allows full utilization of memory bandwidth. This often requires careful consideration of matrix layout (row-major vs column-major) and access patterns in custom CUDA kernels.</p>

<h3>Shared Memory and Bank Conflicts</h3>

<p>Shared memory is divided into 32 banks that can be accessed simultaneously. When multiple threads in a warp access the same bank but different addresses, a bank conflict occurs, serializing the accesses and reducing throughput. The A100's shared memory has 32 banks with 4-byte bank width, meaning addresses that differ by 128 bytes map to the same bank.</p>

<p>In attention implementations, shared memory is commonly used to cache tiles of the query, key, and value matrices. Careful padding of these tiles can eliminate bank conflicts. For example, if each thread block loads a 64√ó64 tile of FP16 values, adding 8 elements of padding to each row ensures that consecutive rows start at different banks, eliminating conflicts when threads access columns.</p>

<div class="example"><strong>Example:</strong> 
Consider computing attention scores $\mA = \mQ\mK\transpose$ where $\mQ, \mK \in \mathbb{R}^{n \times d}$. A naive implementation loads tiles of $\mQ$ and $\mK$ into shared memory and computes partial results.

<p><strong>Unoptimized approach:</strong>
<ul>
    <li>Tile size: 64√ó64 FP16 values
    <li>Shared memory per tile: $64 \times 64 \times 2 = 8192$ bytes
    <li>Bank conflicts when accessing columns: 32-way conflicts
    <li>Effective bandwidth: 50 GB/s (3\% of peak)
</ul>

<p><strong>Optimized approach with padding:</strong>
<ul>
    <li>Tile size: 64√ó72 FP16 values (8 elements padding per row)
    <li>Shared memory per tile: $64 \times 72 \times 2 = 9216$ bytes
    <li>No bank conflicts: consecutive rows in different banks
    <li>Effective bandwidth: 1.4 TB/s (88\% of peak)
</ul>

<p>The 12.5\% increase in shared memory usage eliminates bank conflicts and increases bandwidth by 28√ó, demonstrating the critical importance of memory access patterns.
</div>

<h3>Memory Bandwidth Utilization</h3>

<p>Maximizing memory bandwidth requires ensuring that memory operations are large enough to amortize transaction overhead and that the GPU has sufficient concurrent operations to hide latency. Small transfers are inefficient because they cannot fully utilize the 32-byte or 128-byte cache line sizes. Additionally, launching enough thread blocks to keep all memory controllers busy is essential for achieving peak bandwidth.</p>

<p>For transformers, memory bandwidth is often the limiting factor during attention computation with short sequences. When sequence length $n < 1024$, the arithmetic intensity of attention is low, meaning each floating-point operation requires loading relatively more data from memory. Techniques like Flash Attention address this by restructuring computations to maximize data reuse in shared memory, reducing the number of global memory accesses.</p>

<h2>Kernel Fusion and Operation Optimization</h2>

<p>Kernel fusion combines multiple operations into a single GPU kernel, reducing memory traffic and kernel launch overhead. This technique is particularly effective for transformers, where many operations are memory-bound and benefit from data reuse.</p>

<h3>Fusion Opportunities in Transformers</h3>

<p>Standard transformer implementations launch separate kernels for each operation, requiring intermediate results to be written to and read from global memory. For example, computing layer normalization followed by dropout requires writing the normalized values to memory, then reading them back for the dropout operation. Fusing these operations allows the normalized values to remain in registers or shared memory, eliminating the round-trip to global memory.</p>

<p><strong>Common fusion patterns:</strong>
<ol>
    <li><strong>Layer norm + dropout:</strong> Normalize activations and apply dropout in a single pass, keeping intermediate values in registers.
    <li><strong>GELU + bias:</strong> Compute the GELU activation and add bias without storing intermediate results.
    <li><strong>Attention score + softmax + dropout:</strong> Compute $\text{softmax}(\mQ\mK\transpose/\sqrt{d_k})$ and apply dropout in one kernel.
    <li><strong>Residual + layer norm:</strong> Add residual connection and normalize in a single operation.
</ol>

<div class="example"><strong>Example:</strong> 
Consider layer normalization followed by dropout on a tensor of shape $[32, 128, 768]$ (batch size 32, sequence length 128, hidden size 768).

<p><strong>Unfused implementation:</strong>
<ul>
    <li>Layer norm kernel: Read 3.1M elements, write 3.1M elements
    <li>Dropout kernel: Read 3.1M elements, write 3.1M elements
    <li>Total memory traffic: 12.4M elements = 24.8 MB (FP16)
    <li>Execution time: 0.18 ms
</ul>

<p><strong>Fused implementation:</strong>
<ul>
    <li>Single kernel: Read 3.1M elements, write 3.1M elements
    <li>Total memory traffic: 6.2M elements = 12.4 MB (FP16)
    <li>Execution time: 0.10 ms
</ul>

<p>The fused kernel achieves 1.8√ó speedup by halving memory traffic and eliminating kernel launch overhead. For a 12-layer transformer, this fusion appears 24 times per forward pass (twice per layer), providing substantial cumulative savings.
</div>

<h3>Flash Attention: Fused Attention Implementation</h3>

<p>Flash Attention represents a sophisticated application of kernel fusion to the attention mechanism. Standard attention implementations compute $\mA = \mQ\mK\transpose$, write $\mA$ to memory, read it back for softmax, write the result, read it again for multiplication with $\mV$, and finally write the output. This results in $O(n^2)$ memory reads and writes where $n$ is sequence length.</p>

<p>Flash Attention restructures the computation to work on tiles that fit in shared memory. It computes attention for one tile at a time, keeping intermediate results in fast memory and only writing the final output. This reduces memory traffic from $O(n^2)$ to $O(n)$, providing dramatic speedups for long sequences.</p>

<div class="example"><strong>Example:</strong> 
BERT-base with sequence length 512, batch size 16, on A100 GPU:

<p><strong>Standard attention:</strong>
<ul>
    <li>Memory traffic: 48 GB per forward pass
    <li>Execution time: 8.2 ms
    <li>Memory bandwidth utilization: 45\%
</ul>

<p><strong>Flash Attention:</strong>
<ul>
    <li>Memory traffic: 12 GB per forward pass (4√ó reduction)
    <li>Execution time: 3.8 ms (2.2√ó speedup)
    <li>Memory bandwidth utilization: 82\%
</ul>

<p>For longer sequences, the benefits are even more pronounced. At sequence length 2048, Flash Attention provides 3.5√ó speedup, and at 8192, it provides 5.2√ó speedup while also enabling sequences that would otherwise exceed memory capacity.
</div>

<h3>Implementing Fused Kernels</h3>

<p>Creating fused kernels requires careful consideration of register usage, shared memory capacity, and thread block dimensions. The goal is to maximize data reuse while maintaining high occupancy. Modern deep learning frameworks provide tools for kernel fusion, including PyTorch's JIT compiler and TensorRT's graph optimizer, which can automatically fuse compatible operations. For custom fusion patterns, libraries like CUTLASS provide templates for efficient CUDA implementations.</p>

<h2>Model Quantization</h2>

<h3>Quantization Fundamentals</h3>

<div class="definition"><strong>Definition:</strong> 
Map FP32 weights to lower precision (INT8, FP16):
<div class="equation">
$$
w_{\text{quant}} = \text{round}\left(\frac{w_{\text{float}}}{s}\right) + z
$$
</div>
where $s$ is scale factor, $z$ is zero-point.
</div>

<p><strong>Precision options:</strong>
<ul>
    <li><strong>FP32:</strong> 32 bits, full precision (baseline)
    <li><strong>FP16:</strong> 16 bits, 2√ó compression
    <li><strong>BF16:</strong> 16 bits, better range than FP16
    <li><strong>INT8:</strong> 8 bits, 4√ó compression
    <li><strong>INT4:</strong> 4 bits, 8√ó compression (extreme)
</ul>

<h3>Post-Training Quantization (PTQ)</h3>

<p><strong>Procedure:</strong>
<ol>
    <li>Train model in FP32
    <li>Collect activation statistics on calibration set
    <li>Determine scale factors
    <li>Convert weights and activations to INT8
</ol>

<div class="example"><strong>Example:</strong> 
<strong>FP32 weight:</strong> $w = 0.137$

<p><strong>Determine range:</strong> $w \in [-1.0, 1.0]$</p>

<p><strong>Scale:</strong> $s = \frac{2.0}{256} = 0.0078125$</p>

<p><strong>Quantize:</strong>
<div class="equation">
$$
w_{\text{INT8}} = \text{round}\left(\frac{0.137}{0.0078125}\right) = \text{round}(17.54) = 18
$$
</div>

<p><strong>Dequantize:</strong> $w' = 18 \times 0.0078125 = 0.1406$</p>

<p><strong>Error:</strong> $|0.137 - 0.1406| = 0.0036$ (2.6\% relative)
</div>

<h3>Quantization-Aware Training (QAT)</h3>

<p><strong>Simulate quantization during training:</strong>
<ol>
    <li>Forward pass: Quantize weights/activations
    <li>Compute loss with quantized values
    <li>Backward pass: FP32 gradients
    <li>Update FP32 weights
</ol>

<p><strong>Benefits:</strong>
<ul>
    <li>Model learns to be robust to quantization
    <li>Better accuracy than PTQ
    <li>Minimal accuracy loss with INT8
</ul>

<div class="example"><strong>Example:</strong>

<div style="text-align: center;">

<table>
<tr><th><strong>Precision</strong></th><th><strong>GLUE Score</strong></th><th><strong>Speedup</strong></th></tr>
<tr><td>FP32 (baseline)</td><td>84.5</td><td>1.0√ó</td></tr>
<tr><td>FP16</td><td>84.4</td><td>1.8√ó</td></tr>
<tr><td>INT8 (PTQ)</td><td>82.1</td><td>2.9√ó</td></tr>
<tr><td>INT8 (QAT)</td><td>84.2</td><td>2.9√ó</td></tr>
</table>

<p></div>

<p>QAT recovers most accuracy lost in PTQ!
</div>

<h2>Model Pruning</h2>

<h3>Pruning Strategies</h3>

<p><strong>Magnitude-based pruning:</strong>
<div class="equation">
$$
\text{Prune if } |w_{ij}| < \tau
$$
</div>

<p><strong>Structured pruning:</strong>
<ul>
    <li>Remove entire neurons, heads, layers
    <li>Easier to deploy (no sparse kernels needed)
    <li>Less aggressive compression
</ul>

<p><strong>Unstructured pruning:</strong>
<ul>
    <li>Remove individual weights
    <li>Higher compression ratios
    <li>Requires sparse matrix operations
</ul>

<h3>Iterative Pruning</h3>

<div class="algorithm"><div class="algorithm-title">Algorithm: Iterative Magnitude Pruning</div>
<div class="algorithm-line"><p><strong>Input:</strong> Model, sparsity target $s_{\text{target}}$</p></div>
<div class="algorithm-line"><p>\For{sparsity $s = 0$ \KwTo $s_{\text{target}}$ by steps}{</div>
<div class="algorithm-line">Train model to convergence</div>
<div class="algorithm-line">Prune $\Delta s$ lowest-magnitude weights</div>
<div class="algorithm-line">Fine-tune model</div>
</div>

<div class="example"><strong>Example:</strong> 
BERT-base: 12 layers √ó 12 heads = 144 heads

<p><strong>Finding:</strong> Can remove 50\% of heads with minimal impact!</p>

<p><strong>Procedure:</strong>
<ol>
    <li>Compute importance score per head
    <li>Rank heads by importance
    <li>Prune lowest 50\% (72 heads)
    <li>Fine-tune remaining model
</ol>

<p><strong>Result:</strong>
<ul>
    <li>50\% fewer attention operations
    <li>GLUE score: 84.5 $\to$ 83.8 (0.7 point drop)
    <li>1.5√ó faster inference
</ul>
</div>

<h2>Knowledge Distillation</h2>

<h3>Distillation Loss</h3>

<div class="definition"><strong>Definition:</strong> 
<div class="equation">
$$
\mathcal{L} = \alpha \mathcal{L}_{\text{CE}}(y, y_{\text{student}}) + (1-\alpha) \mathcal{L}_{\text{KD}}(y_{\text{teacher}}, y_{\text{student}})
$$
</div>

<p>where:
<div class="equation">
$$
\mathcal{L}_{\text{KD}} = \text{KL}\left(\frac{\exp(z_t/T)}{\sum \exp(z_t/T)} \Big\| \frac{\exp(z_s/T)}{\sum \exp(z_s/T)}\right)
$$
</div>

<p>$T$ = temperature (typically 2-5), higher = softer probabilities
</div>

<h3>DistilBERT Approach</h3>

<p><strong>Student architecture:</strong>
<ul>
    <li>6 layers (vs 12 in BERT)
    <li>Same hidden size (768)
    <li>Initialize from teacher's even layers
</ul>

<p><strong>Training:</strong>
<ul>
    <li>Distillation loss from teacher
    <li>Masked language modeling loss
    <li>Cosine distance between hidden states
</ul>

<p><strong>Results:</strong>
<ul>
    <li>40\% smaller (66M vs 110M params)
    <li>60\% faster
    <li>Retains 97\% of BERT performance
</ul>

<h2>Multi-GPU Training and Optimization</h2>

<p>Training large transformer models requires distributing computation across multiple GPUs. The efficiency of multi-GPU training depends critically on communication bandwidth, parallelization strategy, and the balance between computation and communication.</p>

<h3>Interconnect Technologies</h3>

<p>The bandwidth between GPUs determines how quickly gradients, activations, and parameters can be exchanged during distributed training. PCIe provides 16-32 GB/s bidirectional bandwidth per GPU, which is adequate for small models but becomes a bottleneck for large transformers. NVLink, NVIDIA's proprietary interconnect, provides 600 GB/s bidirectional bandwidth on A100 systems, enabling much more efficient multi-GPU training. For comparison, the bandwidth within a single GPU (HBM) is 1600 GB/s, so NVLink provides roughly 40\% of intra-GPU bandwidth for inter-GPU communication.</p>

<p>The impact of interconnect bandwidth is most visible during gradient synchronization in data parallel training. After computing gradients on each GPU's local batch, all GPUs must exchange and average their gradients through an all-reduce operation. With PCIe, this communication can take longer than the backward pass itself for models with hundreds of millions of parameters. With NVLink, communication overhead is typically 10-20\% of total training time.</p>

<h3>Data Parallelism and Gradient Synchronization</h3>

<p>Data parallelism replicates the model on each GPU and processes different batches on each device. After the backward pass, gradients are averaged across all GPUs using an all-reduce collective operation. The communication volume is equal to the model size, independent of batch size, making data parallelism most efficient for large batch sizes where computation time dominates communication time.</p>

<div class="example"><strong>Example:</strong> 
Training BERT-large (340M parameters) with batch size 32 per GPU on A100 GPUs:

<p><strong>Single GPU:</strong>
<ul>
    <li>Forward + backward time: 145 ms
    <li>Throughput: 221 sequences/second
</ul>

<p><strong>4 GPUs with NVLink:</strong>
<ul>
    <li>Forward + backward time: 145 ms (unchanged)
    <li>Gradient all-reduce time: 18 ms
    <li>Total time per step: 163 ms
    <li>Throughput: 785 sequences/second
    <li>Scaling efficiency: 89\% (ideal would be 884 seq/s)
</ul>

<p><strong>8 GPUs with NVLink:</strong>
<ul>
    <li>Forward + backward time: 145 ms
    <li>Gradient all-reduce time: 22 ms
    <li>Total time per step: 167 ms
    <li>Throughput: 1533 sequences/second
    <li>Scaling efficiency: 87\%
</ul>

<p>The high scaling efficiency demonstrates that NVLink bandwidth is sufficient for data parallel training of BERT-large. With PCIe, the all-reduce time would be approximately 85 ms, reducing scaling efficiency to 63\% for 8 GPUs.
</div>

<h3>Pipeline and Tensor Parallelism</h3>

<p>For models too large to fit on a single GPU, pipeline parallelism splits the model across GPUs by layers, while tensor parallelism splits individual layers across GPUs. Pipeline parallelism has lower communication requirements but suffers from pipeline bubbles where some GPUs are idle. Tensor parallelism requires more communication (activations must be exchanged between layers) but maintains better GPU utilization.</p>

<p>Modern training frameworks like Megatron-LM combine data, pipeline, and tensor parallelism to train models with hundreds of billions of parameters. For example, training a 175B parameter model might use 8-way tensor parallelism, 8-way pipeline parallelism, and 32-way data parallelism across 2048 GPUs.</p>

<h3>Overlapping Communication and Computation</h3>

<p>Advanced implementations overlap gradient communication with backward pass computation. As soon as gradients for one layer are computed, they can begin synchronizing while the backward pass continues on earlier layers. This technique, called gradient bucketing, can hide most communication overhead when sufficient computation is available to overlap.</p>

<p>PyTorch's DistributedDataParallel automatically implements gradient bucketing, grouping parameters into buckets of approximately 25MB and launching all-reduce operations as soon as each bucket's gradients are ready. This optimization is particularly effective for large models where the backward pass takes much longer than gradient synchronization.</p>

<h2>Inference Optimization</h2>

<h3>ONNX Runtime</h3>

<p><strong>ONNX (Open Neural Network Exchange):</strong>
<ul>
    <li>Framework-agnostic model format
    <li>Optimized inference engine
    <li>Supports quantization, pruning
</ul>

<p><strong>Optimizations:</strong>
<ul>
    <li>Operator fusion (combine LayerNorm + Add)
    <li>Constant folding
    <li>Dead code elimination
    <li>Graph optimization
</ul>

<h3>TensorRT</h3>

<p><strong>NVIDIA TensorRT:</strong>
<ul>
    <li>Deep learning inference optimizer
    <li>Layer fusion
    <li>Kernel auto-tuning
    <li>INT8 calibration
</ul>

<p><strong>Typical speedups:</strong>
<ul>
    <li>BERT-base: 2-3√ó over PyTorch
    <li>With INT8: 4-5√ó over PyTorch FP32
</ul>

<h3>Batching Strategies</h3>

<p><strong>Static batching:</strong>
<ul>
    <li>Fixed batch size
    <li>Pad to max length
    <li>Simple but wasteful
</ul>

<p><strong>Dynamic batching:</strong>
<ul>
    <li>Accumulate requests until batch full or timeout
    <li>Reduces latency while maintaining throughput
</ul>

<p><strong>Continuous batching:</strong>
<ul>
    <li>For autoregressive generation
    <li>Add new sequences as others finish
    <li>Maximizes GPU utilization
</ul>

<h2>Production Deployment</h2>

<h3>Serving Frameworks</h3>

<p><strong>TorchServe:</strong>
<ul>
    <li>PyTorch native serving
    <li>REST/gRPC APIs
    <li>Batching, versioning, monitoring
</ul>

<p><strong>Triton Inference Server:</strong>
<ul>
    <li>Multi-framework (PyTorch, TensorFlow, ONNX)
    <li>Concurrent model execution
    <li>Dynamic batching
    <li>Model ensembles
</ul>

<p><strong>FastAPI + Custom:</strong>
<ul>
    <li>Lightweight, flexible
    <li>Full control over serving logic
    <li>Easy integration with existing systems
</ul>

<h3>Deployment Checklist</h3>

<p><strong>Performance:</strong>
<ul>
    <li>Quantize to INT8/FP16
    <li>Export to ONNX/TensorRT
    <li>Optimize batch size for latency/throughput
    <li>Enable KV caching for generation
</ul>

<p><strong>Reliability:</strong>
<ul>
    <li>Graceful degradation on errors
    <li>Request timeouts
    <li>Health checks
    <li>Model versioning
</ul>

<p><strong>Monitoring:</strong>
<ul>
    <li>Latency (p50, p95, p99)
    <li>Throughput (requests/second)
    <li>GPU utilization
    <li>Error rates
</ul>

<h2>Hardware Selection and Cost Analysis</h2>

<p>Selecting appropriate hardware for transformer workloads requires balancing performance, cost, and operational requirements. This section provides guidance for different use cases and scales.</p>

<h3>CPU vs GPU Trade-offs</h3>

<p>The choice between CPU and GPU depends on model size, batch size, latency requirements, and cost constraints. CPUs excel at low-latency inference with small batch sizes, while GPUs provide superior throughput for larger batches and are essential for training.</p>

<p><strong>When to use CPUs:</strong>
<ul>
    <li>Small models (< 100M parameters) with batch size 1-4
    <li>Latency-critical applications requiring < 10ms response time
    <li>Cost-sensitive deployments with low throughput requirements
    <li>Edge deployment where GPU hardware is unavailable
</ul>

<p>For example, a distilled BERT model with 66M parameters can achieve 15ms latency on a modern CPU (Intel Xeon or AMD EPYC) with batch size 1. The same model on a T4 GPU achieves 8ms latency but requires batching to amortize GPU overhead, making it less suitable for single-request scenarios.</p>

<p><strong>When to use GPUs:</strong>
<ul>
    <li>Training any transformer model
    <li>Large models (> 100M parameters)
    <li>Batch inference with batch size > 8
    <li>Throughput-oriented applications
</ul>

<p>A BERT-large model (340M parameters) achieves 2.5 sequences/second on CPU but 45 sequences/second on a T4 GPU with batch size 16, demonstrating the GPU's 18√ó throughput advantage for larger models.</p>

<p><strong>Cost analysis.</strong> Cloud GPU instances cost approximately 3-5√ó more than equivalent CPU instances. However, the higher throughput often makes GPUs more cost-effective per inference. For BERT-large inference, a T4 GPU instance costs \$0.35/hour and processes 162,000 sequences/hour, yielding \$0.0000022 per sequence. A CPU instance costs \$0.10/hour and processes 9,000 sequences/hour, yielding \$0.000011 per sequence, making the GPU 5√ó more cost-effective despite higher instance cost.</p>

<p><strong>Energy efficiency.</strong> GPUs also provide better energy efficiency for large models. The T4 GPU consumes 70W and processes 45 sequences/second, yielding 0.64 sequences/second/watt. A high-end CPU consumes 200W and processes 2.5 sequences/second, yielding 0.0125 sequences/second/watt, making the GPU 51√ó more energy-efficient.</p>

<h3>Training Hardware Selection</h3>

<p>Training requirements scale dramatically with model size, from single GPUs for small models to thousands of GPUs for the largest models.</p>

<p><strong>Small models (< 1B parameters):</strong>
<ul>
    <li>Hardware: Single V100 (32GB) or A100 (40GB)
    <li>Training time: BERT-base on 16GB dataset trains in 3 days on V100
    <li>Cost: \$2.50/hour (V100) to \$4.00/hour (A100) on cloud
    <li>Use case: Research, fine-tuning, domain-specific models
</ul>

<p><strong>Medium models (1-10B parameters):</strong>
<ul>
    <li>Hardware: 4-8√ó A100 (40GB or 80GB) with NVLink
    <li>Training time: GPT-2 (1.5B) trains in 2 weeks on 8√ó A100
    <li>Cost: \$32/hour for 8√ó A100 on cloud
    <li>Use case: Production models, large-scale fine-tuning
</ul>

<p>For these models, NVLink is essential for efficient data parallelism. The 600 GB/s NVLink bandwidth enables 85-90\% scaling efficiency, while PCIe would reduce efficiency to 60-70\%.</p>

<p><strong>Large models (10-100B parameters):</strong>
<ul>
    <li>Hardware: 16-64√ó A100 (80GB) with NVLink and InfiniBand
    <li>Training time: GPT-3 (175B) trained on 10,000√ó V100-days
    <li>Cost: \$128-256/hour for 32-64√ó A100 on cloud
    <li>Use case: Foundation models, large-scale pretraining
</ul>

<p>These models require pipeline and tensor parallelism in addition to data parallelism. InfiniBand provides 200-400 Gb/s bandwidth between nodes, enabling efficient multi-node training.</p>

<p><strong>Extreme models (100B+ parameters):</strong>
<ul>
    <li>Hardware: 100s-1000s of A100 GPUs across multiple nodes
    <li>Training time: Months on large clusters
    <li>Cost: Millions of dollars for full pretraining
    <li>Use case: State-of-the-art foundation models (GPT-4, PaLM)
</ul>

<h3>Inference Hardware Selection</h3>

<p>Inference requirements vary widely based on latency, throughput, and cost constraints.</p>

<p><strong>Batch inference (throughput-oriented):</strong>
<ul>
    <li>Hardware: A100 or A10 GPUs
    <li>Characteristics: Process large batches (32-128), optimize for throughput
    <li>Use case: Offline processing, data pipelines, batch predictions
    <li>Example: BERT-large processes 520 sequences/second on A100 with batch size 64
</ul>

<p><strong>Low-latency inference:</strong>
<ul>
    <li>Hardware: T4 or A10 GPUs with TensorRT
    <li>Characteristics: Small batches (1-8), optimize for latency
    <li>Use case: Real-time applications, interactive systems
    <li>Example: BERT-base achieves 5ms latency on T4 with batch size 1 using INT8
</ul>

<p><strong>Cost-optimized inference:</strong>
<ul>
    <li>Hardware: CPU or small GPUs (T4)
    <li>Characteristics: Quantized models, efficient batching
    <li>Use case: High-volume, cost-sensitive applications
    <li>Example: Distilled BERT (66M params) on CPU costs \$0.000005 per inference
</ul>

<p><strong>Edge deployment:</strong>
<ul>
    <li>Hardware: Mobile CPUs, edge TPUs, or NVIDIA Jetson
    <li>Characteristics: Heavily quantized (INT8/INT4), pruned models
    <li>Use case: On-device inference, privacy-sensitive applications
    <li>Example: MobileBERT (25M params) runs at 30ms on mobile CPU
</ul>

<h3>Hardware Selection Decision Tree</h3>

<p>The following decision tree provides guidance for hardware selection:</p>

<ol>
    <li><strong>Training or inference?</strong>
    <ul>
        <li>Training: Proceed to step 2
        <li>Inference: Proceed to step 4
    </ul>
    
    <li><strong>Model size?</strong>
    <ul>
        <li>< 1B params: Single V100/A100
        <li>1-10B params: 4-8√ó A100 with NVLink
        <li>10-100B params: 16-64√ó A100 with InfiniBand
        <li>> 100B params: 100s-1000s of GPUs
    </ul>
    
    <li><strong>Budget constraints?</strong>
    <ul>
        <li>Research/limited budget: V100 or A10
        <li>Production/performance-critical: A100
    </ul>
    
    <li><strong>Latency requirements?</strong>
    <ul>
        <li>< 10ms: T4 GPU with TensorRT or CPU for small models
        <li>10-50ms: T4 or A10 GPU
        <li>> 50ms: Any GPU or CPU based on cost
    </ul>
    
    <li><strong>Throughput requirements?</strong>
    <ul>
        <li>< 10 seq/s: CPU
        <li>10-100 seq/s: T4 GPU
        <li>> 100 seq/s: A10 or A100 GPU
    </ul>
</ol>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> Quantize BERT-base to INT8:
<ol>
    <li>Use PyTorch quantization APIs
    <li>Calibrate on 1000 examples
    <li>Measure: (a) Model size, (b) Inference speed, (c) GLUE accuracy
    <li>Compare PTQ vs QAT
</ol>
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Implement attention head pruning:
<ol>
    <li>Compute importance scores for all heads
    <li>Prune 25\%, 50\%, 75\% of heads
    <li>Fine-tune after pruning
    <li>Plot accuracy vs sparsity
</ol>
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> Optimize inference pipeline:
<ol>
    <li>Baseline: PyTorch FP32
    <li>Convert to ONNX, measure speedup
    <li>Apply INT8 quantization
    <li>Implement dynamic batching
    <li>Report final throughput improvement
</ol>
</div>

<div class="exercise" id="exercise-4"><strong>Exercise 4:</strong> Analyze Tensor Core utilization:
<ol>
    <li>Profile BERT-base training with FP32 and FP16
    <li>Measure Tensor Core utilization using NVIDIA Nsight Compute
    <li>Experiment with different batch sizes and sequence lengths
    <li>Identify which operations benefit most from Tensor Cores
    <li>Calculate achieved TFLOPS as percentage of theoretical peak
</ol>
</div>

<div class="exercise" id="exercise-5"><strong>Exercise 5:</strong> Implement and benchmark kernel fusion:
<ol>
    <li>Create separate kernels for layer norm and dropout
    <li>Implement a fused layer norm + dropout kernel
    <li>Measure memory bandwidth utilization for both approaches
    <li>Compare execution time across different tensor sizes
    <li>Analyze the speedup and explain the performance difference
</ol>
</div>

<div class="exercise" id="exercise-6"><strong>Exercise 6:</strong> Optimize memory access patterns:
<ol>
    <li>Implement matrix multiplication with and without coalesced access
    <li>Add padding to eliminate shared memory bank conflicts
    <li>Profile both implementations using Nsight Compute
    <li>Measure effective memory bandwidth for each version
    <li>Document the impact of access patterns on performance
</ol>
</div>

<div class="exercise" id="exercise-7"><strong>Exercise 7:</strong> Multi-GPU scaling analysis:
<ol>
    <li>Train BERT-base on 1, 2, 4, and 8 GPUs
    <li>Measure training time and throughput for each configuration
    <li>Calculate scaling efficiency relative to single GPU
    <li>Profile communication overhead using NVIDIA Nsight Systems
    <li>Compare PCIe vs NVLink if available
</ol>
</div>

<div class="exercise" id="exercise-8"><strong>Exercise 8:</strong> Hardware selection analysis:
<ol>
    <li>Choose a transformer model and deployment scenario
    <li>Estimate throughput requirements and latency constraints
    <li>Compare cost per inference for CPU, T4, and A100
    <li>Calculate break-even point where GPU becomes cost-effective
    <li>Recommend hardware configuration with justification
</ol>
</div>

<div class="exercise" id="exercise-9"><strong>Exercise 9:</strong> Flash Attention implementation study:
<ol>
    <li>Implement standard attention with separate kernels
    <li>Analyze memory traffic for different sequence lengths
    <li>Study Flash Attention paper and implementation
    <li>Benchmark Flash Attention vs standard attention
    <li>Plot speedup as a function of sequence length
</ol>
</div>

<h2>Solutions</h2>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 1: Quantize BERT-base to INT8</strong>

<p><strong>Results:</strong></p>

<table>
<tr><th><strong>Method</strong></th><th><strong>Size (MB)</strong></th><th><strong>Speed (ms)</strong></th><th><strong>GLUE Acc</strong></th></tr>
<tr><td>FP32 Baseline</td><td>438</td><td>45.2</td><td>84.5\%</td></tr>
<tr><td>PTQ INT8</td><td>110</td><td>18.3</td><td>83.8\%</td></tr>
<tr><td>QAT INT8</td><td>110</td><td>18.1</td><td>84.2\%</td></tr>
</table>

<p><strong>Analysis:</strong>
<ul>
    <li><strong>Model size:</strong> 4x reduction (438 MB $\to$ 110 MB)
    <li><strong>Speed:</strong> 2.5x faster inference
    <li><strong>PTQ:</strong> 0.7\% accuracy drop, no retraining
    <li><strong>QAT:</strong> 0.3\% accuracy drop, requires fine-tuning
</ul>

<p><strong>Recommendation:</strong> Use QAT for production (better accuracy with same speed/size)
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 2: Attention Head Pruning</strong>

<p><strong>Results:</strong></p>

<table>
<tr><th><strong>Pruning \%</strong></th><th><strong>Params Remaining</strong></th><th><strong>Accuracy</strong></th></tr>
<tr><td>0\% (baseline)</td><td>110M</td><td>84.5\%</td></tr>
<tr><td>25\%</td><td>82.5M</td><td>84.1\%</td></tr>
<tr><td>50\%</td><td>55M</td><td>82.8\%</td></tr>
<tr><td>75\%</td><td>27.5M</td><td>78.3\%</td></tr>
</table>

<p><strong>Key Insight:</strong> Can prune 25-50\% of heads with minimal accuracy loss (<2\%), but 75\% pruning causes significant degradation.
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 3: Optimize Inference Pipeline</strong>

<p><strong>Progressive Optimization:</strong></p>

<table>
<tr><th><strong>Stage</strong></th><th><strong>Latency (ms)</strong></th><th><strong>Throughput (samples/s)</strong></th></tr>
<tr><td>PyTorch FP32</td><td>45.2</td><td>22.1</td></tr>
<tr><td>+ ONNX</td><td>32.8</td><td>30.5</td></tr>
<tr><td>+ INT8 Quant</td><td>14.7</td><td>68.0</td></tr>
<tr><td>+ Dynamic Batch</td><td>12.3</td><td>162.4</td></tr>
</table>

<p><strong>Final Improvement:</strong> 7.3x throughput increase (22.1 $\to$ 162.4 samples/s)
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 4: Tensor Core Utilization</strong>

<p><strong>Results (A100 GPU):</strong></p>

<table>
<tr><th><strong>Config</strong></th><th><strong>TC Util</strong></th><th><strong>TFLOPS</strong></th><th><strong>\% Peak</strong></th></tr>
<tr><td>FP32, batch=16</td><td>0\%</td><td>8.2</td><td>5.3\%</td></tr>
<tr><td>FP16, batch=16</td><td>45\%</td><td>89.4</td><td>28.7\%</td></tr>
<tr><td>FP16, batch=64</td><td>78\%</td><td>187.3</td><td>60.1\%</td></tr>
<tr><td>FP16, batch=128</td><td>85\%</td><td>214.6</td><td>68.9\%</td></tr>
</table>

<p><strong>Key Findings:</strong>
<ul>
    <li>Attention and MLP benefit most from Tensor Cores
    <li>Larger batches improve utilization (more parallelism)
    <li>FP16 essential for Tensor Core activation
    <li>Achieved 68.9\% of theoretical peak (good for transformers)
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 5: Kernel Fusion</strong>

<p><strong>Results:</strong></p>

<table>
<tr><th><strong>Implementation</strong></th><th><strong>Time (Œºs)</strong></th><th><strong>Bandwidth (GB/s)</strong></th><th><strong>Speedup</strong></th></tr>
<tr><td>Separate Kernels</td><td>142</td><td>385</td><td>1.0x</td></tr>
<tr><td>Fused Kernel</td><td>58</td><td>943</td><td>2.45x</td></tr>
</table>

<p><strong>Analysis:</strong>
<ul>
    <li>Fused kernel eliminates intermediate memory writes
    <li>2.45x speedup from reduced memory traffic
    <li>Bandwidth utilization improves from 385 to 943 GB/s
    <li>Most beneficial for memory-bound operations
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 6: Memory Access Patterns</strong>

<p><strong>Results:</strong></p>

<table>
<tr><th><strong>Implementation</strong></th><th><strong>Bandwidth (GB/s)</strong></th><th><strong>Speedup</strong></th></tr>
<tr><td>Uncoalesced</td><td>287</td><td>1.0x</td></tr>
<tr><td>Coalesced</td><td>823</td><td>2.87x</td></tr>
<tr><td>+ No Bank Conflicts</td><td>1,142</td><td>3.98x</td></tr>
</table>

<p><strong>Key Insight:</strong> Proper memory access patterns provide 4x speedup through coalescing and eliminating bank conflicts.
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 7: Multi-GPU Scaling</strong>

<p><strong>Results:</strong></p>

<table>
<tr><th><strong>GPUs</strong></th><th><strong>Time (min)</strong></th><th><strong>Throughput</strong></th><th><strong>Efficiency</strong></th></tr>
<tr><td>1</td><td>240</td><td>1.0x</td><td>100\%</td></tr>
<tr><td>2</td><td>126</td><td>1.90x</td><td>95.2\%</td></tr>
<tr><td>4</td><td>67</td><td>3.58x</td><td>89.6\%</td></tr>
<tr><td>8</td><td>36</td><td>6.67x</td><td>83.3\%</td></tr>
</table>

<p><strong>Communication Overhead:</strong>
<ul>
    <li>PCIe: 12-15\% overhead
    <li>NVLink: 5-8\% overhead (2x better)
</ul>

<p><strong>Recommendation:</strong> NVLink for multi-GPU training (better scaling)
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 8: Hardware Selection</strong>

<p><strong>Scenario:</strong> BERT-base inference, 1M requests/day, <50ms latency</p>

<p><strong>Cost Analysis:</strong></p>

<table>
<tr><th><strong>Hardware</strong></th><th><strong>Throughput</strong></th><th><strong>Cost/hour</strong></th><th><strong>Cost/1M inferences</strong></th></tr>
<tr><td>CPU (32 cores)</td><td>45 req/s</td><td>\$1.20</td><td>\$7.41</td></tr>
<tr><td>T4 GPU</td><td>180 req/s</td><td>\$0.35</td><td>\$0.54</td></tr>
<tr><td>A100 GPU</td><td>650 req/s</td><td>\$2.50</td><td>\$1.07</td></tr>
</table>

<p><strong>Break-even:</strong> T4 becomes cost-effective at >10k requests/day</p>

<p><strong>Recommendation:</strong> T4 GPU (best cost/performance for this workload)
</div>
        
        <div class="chapter-nav">
  <a href="chapter21_pytorch_implementation.html">‚Üê Chapter 21: PyTorch Implementation</a>
  <a href="../../deeptech.html">üìö Table of Contents</a>
  <a href="chapter23_best_practices.html">Chapter 23: Best Practices ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
