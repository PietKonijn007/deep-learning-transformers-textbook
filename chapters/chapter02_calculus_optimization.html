<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Calculus and Optimization - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../css/style.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
    </nav>

    <main>
        <h1>Calculus and Optimization</h1>

<h2>Chapter Overview</h2>

<p>Training deep learning models requires optimizing complex, high-dimensional functions. This chapter develops the calculus and optimization theory necessary to understand how neural networks learn from data. We cover multivariable calculus, gradient computation, and the optimization algorithms that power modern deep learning.</p>

<p>The centerpiece of this chapter is backpropagation, the algorithm that efficiently computes gradients in neural networks. We derive backpropagation from first principles, showing how the chain rule enables gradient computation through arbitrarily deep computational graphs. We then explore gradient descent and its variants, which use these gradients to iteratively improve model parameters.</p>

<h3>Learning Objectives</h3>

<p>After completing this chapter, you will be able to:</p>

<ol>
    <li>Compute gradients and Jacobians for multivariable functions
    <li>Apply the chain rule to composite functions
    <li>Understand and implement the backpropagation algorithm
    <li>Implement gradient descent and its variants (SGD, momentum, Adam)
    <li>Analyze convergence properties of optimization algorithms
    <li>Apply learning rate schedules and regularization techniques
</ol>

<h2>Multivariable Calculus</h2>

<h3>Partial Derivatives</h3>

<div class="definition"><strong>Definition:</strong> 
For function $f: \R^n \to \R$, the <strong>partial derivative</strong> with respect to $x_i$ is:
<div class="equation">
$$
\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i + h, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}
$$
</div>
</div>

<div class="example"><strong>Example:</strong> 
For $f(x_1, x_2) = x_1^2 + 3x_1 x_2 + x_2^2$:
<div class="equation">
$$\begin{align}
\frac{\partial f}{\partial x_1} &= 2x_1 + 3x_2 \\
\frac{\partial f}{\partial x_2} &= 3x_1 + 2x_2
\end{align}$$
</div>

<p>At point $(x_1, x_2) = (1, 2)$:
<div class="equation">
$$\begin{align}
\frac{\partial f}{\partial x_1}\bigg|_{(1,2)} &= 2(1) + 3(2) = 8 \\
\frac{\partial f}{\partial x_2}\bigg|_{(1,2)} &= 3(1) + 2(2) = 7
\end{align}$$
</div>
</div>

<h3>Gradients</h3>

<div class="definition"><strong>Definition:</strong> 
For function $f: \R^n \to \R$, the <strong>gradient</strong> is the vector of partial derivatives:
<div class="equation">
$$
\nabla f(\vx) = \begin{bmatrix} 
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix} \in \R^n
$$
</div>
</div>

<p>The gradient points in the direction of steepest ascent of the function.</p>

<div class="example"><strong>Example:</strong> 
For mean squared error loss:
<div class="equation">
$$
L(\vw) = \frac{1}{N} \sum_{i=1}^N (y_i - \vw\transpose \vx^{(i)})^2
$$
</div>

<p>The gradient with respect to $\vw$ is:
<div class="equation">
$$
\nabla_{\vw} L = -\frac{2}{N} \sum_{i=1}^N (y_i - \vw\transpose \vx^{(i)}) \vx^{(i)}
$$
</div>

<p>For $N=1$, $\vw = [w_1, w_2]\transpose$, $\vx = [1, 2]\transpose$, $y = 5$, and current prediction $\hat{y} = \vw\transpose \vx = 3$:
<div class="equation">
$$
\nabla_{\vw} L = -2(5 - 3) \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} -4 \\ -8 \end{bmatrix}
$$
</div>

<p>The negative gradient $-\nabla_{\vw} L = [4, 8]\transpose$ points toward better parameters.
</div>

<h3>The Chain Rule</h3>

<div class="theorem"><strong>Theorem:</strong> 
For composite function $h(\vx) = f(g(\vx))$ where $g: \R^n \to \R^m$ and $f: \R^m \to \R$:
<div class="equation">
$$
\frac{\partial h}{\partial x_i} = \sum_{j=1}^m \frac{\partial f}{\partial g_j} \frac{\partial g_j}{\partial x_i}
$$
</div>

<p>In vector form:
<div class="equation">
$$
\nabla_{\vx} h = \mJ_g\transpose \nabla_{\vz} f
$$
</div>
where $\vz = g(\vx)$ and $\mJ_g \in \R^{m \times n}$ is the Jacobian of $g$.
</div>

<div class="example"><strong>Example:</strong> 
For neural network layer: $\vy = \sigma(\mW\vx + \vb)$ where $\sigma$ is applied element-wise.

<p>Let $\vz = \mW\vx + \vb$ (pre-activation). Then:
<div class="equation">
$$
\frac{\partial L}{\partial \vx} = \mW\transpose \left( \frac{\partial L}{\partial \vy} \odot \sigma'(\vz) \right)
$$
</div>

<p>where $\odot$ denotes element-wise multiplication.</p>

<p><strong>Concrete example:</strong> For ReLU activation $\sigma(z) = \max(0, z)$:
<div class="equation">
$$
\sigma'(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}
$$
</div>

<p>If $\vz = [2.0, -1.0, 0.5]\transpose$, then $\sigma'(\vz) = [1, 0, 1]\transpose$.
</div>

<h3>Jacobian and Hessian Matrices</h3>

<div class="definition"><strong>Definition:</strong> 
For function $\mathbf{f}: \R^n \to \R^m$, the <strong>Jacobian matrix</strong> is:
<div class="equation">
$$
\mJ_{\mathbf{f}}(\vx) = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix} \in \R^{m \times n}
$$
</div>
</div>

<div class="definition"><strong>Definition:</strong> 
For function $f: \R^n \to \R$, the <strong>Hessian matrix</strong> contains second derivatives:
<div class="equation">
$$
\mH_f(\vx) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots \\
\vdots & \vdots & \ddots
\end{bmatrix} \in \R^{n \times n}
$$
</div>
</div>

<p>The Hessian describes the local curvature of the function. For smooth functions, $\mH$ is symmetric.</p>

<h2>Gradient Descent</h2>

<h3>The Gradient Descent Algorithm</h3>

<p>Gradient descent iteratively moves parameters in the direction opposite to the gradient:</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: Gradient Descent</div>
<div class="algorithm-line"><strong>Input:</strong> Objective function $f(\vw)$, initial parameters $\vw^{(0)}$, learning rate $\eta$, iterations $T$</div>
<div class="algorithm-line"><strong>Output:</strong> Optimized parameters $\vw^{(T)}$</div>
<div class="algorithm-line"><strong>for</strong> $t = 0$ to $T-1$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Compute gradient: $\mathbf{g}^{(t)} = \nabla f(\vw^{(t)})$</div>
<div class="algorithm-line">Update parameters: $\vw^{(t+1)} = \vw^{(t)} - \eta \mathbf{g}^{(t)}$</div>
</div>
<div class="algorithm-line"><strong>return</strong> $\vw^{(T)</div>
</div>

<div class="keypoint">
The learning rate $\eta$ controls the step size. Too large: divergence. Too small: slow convergence.
</div>

<div class="example"><strong>Example:</strong> 
Minimize $f(w) = w^2$ starting from $w^{(0)} = 3$ with $\eta = 0.1$:
<div class="equation">
$$\begin{align}
t=0:& \quad w^{(0)} = 3, \quad g^{(0)} = 2w^{(0)} = 6, \quad w^{(1)} = 3 - 0.1(6) = 2.4 \\
t=1:& \quad w^{(1)} = 2.4, \quad g^{(1)} = 4.8, \quad w^{(2)} = 2.4 - 0.1(4.8) = 1.92 \\
t=2:& \quad w^{(2)} = 1.92, \quad g^{(2)} = 3.84, \quad w^{(3)} = 1.92 - 0.1(3.84) = 1.536
\end{align}$$
</div>

<p>The parameters converge to $w^* = 0$ (the minimum).
</div>

<h3>Stochastic Gradient Descent (SGD)</h3>

<p>For large datasets, computing the full gradient is expensive. SGD approximates the gradient using mini-batches.</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: Stochastic Gradient Descent (SGD)</div>
<div class="algorithm-line"><strong>Input:</strong> Dataset $\mathcal{D} = \{(\vx^{(i)}, y^{(i)})\}_{i=1}^N$, batch size $B$, learning rate $\eta$, epochs $E$</div>
<div class="algorithm-line"><strong>Output:</strong> Optimized parameters $\vw$</div>
<div class="algorithm-line">Initialize $\vw$ randomly</div>
<div class="algorithm-line"><strong>for</strong> epoch $e = 1$ to $E$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Shuffle dataset $\mathcal{D}$</div>
<div class="algorithm-line">Compute mini-batch gradient: $\mathbf{g} = \frac{1}{B} \sum_{(\vx, y) \in \mathcal{B}} \nabla_{\vw} L(\vw; \vx, y)$</div>
<div class="algorithm-line">Update: $\vw \leftarrow \vw - \eta \mathbf{g}$</div>
</div>
<div class="algorithm-line"><strong>return</strong> $\vw$</div>
</div>

<div class="implementation">
PyTorch SGD implementation:
<pre><code>import torch
import torch.nn as nn

<p># Model and loss
model = nn.Linear(10, 1)
criterion = nn.MSELoss()</p>

<p># SGD optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)</p>

<p># Training loop
for epoch in range(100):
    for x_batch, y_batch in dataloader:
        # Forward pass
        y_pred = model(x_batch)
        loss = criterion(y_pred, y_batch)</p>

<p># Backward pass
        optimizer.zero_grad()  # Clear previous gradients
        loss.backward()         # Compute gradients
        optimizer.step()        # Update parameters
</code></pre>
</div>

<h3>Momentum</h3>

<p>Momentum accelerates SGD by accumulating a velocity vector:</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: SGD with Momentum</div>
<div class="algorithm-line"><strong>Input:</strong> Learning rate $\eta$, momentum coefficient $\beta$ (typically 0.9)</div>
<div class="algorithm-line">Initialize velocity $\mathbf{v} = \mathbf{0}$</div>
<div class="algorithm-line"><strong>for</strong> each iteration <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Compute gradient $\mathbf{g} = \nabla_{\vw} L(\vw)$</div>
<div class="algorithm-line">Update velocity: $\mathbf{v} \leftarrow \beta \mathbf{v} + \mathbf{g}$</div>
<div class="algorithm-line">Update parameters: $\vw \leftarrow \vw - \eta \mathbf{v}$</div>
</div>
</div>

<p>Momentum helps navigate ravines and accelerates convergence in relevant directions.</p>

<h3>Adam Optimizer</h3>

<p>Adam (Adaptive Moment Estimation) combines momentum with adaptive learning rates:</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: Adam Optimizer</div>
<div class="algorithm-line"><strong>Input:</strong> Learning rate $\alpha$ (default 0.001), $\beta_1$ = 0.9, $\beta_2$ = 0.999, $\epsilon$ = $10^{-8}$</div>
<div class="algorithm-line">Initialize $\mathbf{m}_0 = \mathbf{0}$ (first moment), $\mathbf{v}_0 = \mathbf{0}$ (second moment), $t = 0$</div>
<div class="algorithm-line"><strong>while</strong> not converged <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">$t \leftarrow t + 1$</div>
<div class="algorithm-line">Compute gradient: $\mathbf{g}_t = \nabla_{\vw} L(\vw_{t-1})$</div>
<div class="algorithm-line">Update biased first moment: $\mathbf{m}_t = \beta_1 \mathbf{m}_{t-1} + (1 - \beta_1) \mathbf{g}_t$</div>
<div class="algorithm-line">Update biased second moment: $\mathbf{v}_t = \beta_2 \mathbf{v}_{t-1} + (1 - \beta_2) \mathbf{g}_t^2$</div>
<div class="algorithm-line">Bias-corrected first moment: $\hat{\mathbf{m}}_t = \mathbf{m}_t / (1 - \beta_1^t)$</div>
<div class="algorithm-line">Bias-corrected second moment: $\hat{\mathbf{v}}_t = \mathbf{v}_t / (1 - \beta_2^t)$</div>
<div class="algorithm-line">Update parameters: $\vw_t = \vw_{t-1} - \alpha \hat{\mathbf{m}}_t / (\sqrt{\hat{\mathbf{v}}_t} + \epsilon)$</div>
</div>
</div>

<p>Adam is the most commonly used optimizer for training transformers and large language models.</p>

<h2>Gradient Computation Complexity</h2>

<p>Understanding the computational and memory costs of gradient computation is essential for training large models efficiently.</p>

<h3>FLOPs for Gradient Computation</h3>

<div class="keypoint">
Computing gradients via backpropagation requires approximately 2√ó the FLOPs of the forward pass: 1√ó for the backward pass itself, plus the original 1√ó forward pass.
</div>

<div class="example"><strong>Example:</strong> 
For BERT-base (110M parameters, 12 layers, $d_{\text{model}} = 768$) processing sequence length $n = 512$:

<p><strong>Forward pass:</strong>
<ul>
    <li>Self-attention: $12 \times 4n^2d = 12 \times 4(512)^2(768) \approx 48$ GFLOPs
    <li>Feed-forward: $12 \times 2nd(4d) = 12 \times 2(512)(768)(3072) \approx 36$ GFLOPs
    <li>Other operations: $\approx 12$ GFLOPs
    <li><strong>Total forward: $\approx 96$ GFLOPs</strong>
</ul>

<p><strong>Backward pass:</strong>
<ul>
    <li>Gradient computation through each layer: $\approx 96$ GFLOPs
    <li>Gradient accumulation for weight updates: $\approx 97$ GFLOPs
    <li><strong>Total backward: $\approx 193$ GFLOPs</strong>
</ul>

<p><strong>Total per training step: $\approx 289$ GFLOPs</strong></p>

<p>For batch size $B = 32$: $289 \times 32 \approx 9.2$ TFLOPs per batch.
</div>

<h3>Memory Requirements for Activations</h3>

<p>During backpropagation, intermediate activations must be stored for gradient computation.</p>

<div class="definition"><strong>Definition:</strong> 
For a network with $L$ layers processing batch size $B$, activation memory is:
<div class="equation">
$$
M_{\text{act}} = B \sum_{\ell=1}^L d_\ell
$$
</div>
where $d_\ell$ is the dimension of layer $\ell$'s output.
</div>

<div class="example"><strong>Example:</strong> 
For BERT-base with batch size $B = 32$, sequence length $n = 512$, $d = 768$:

<p><strong>Per transformer layer:</strong>
<ul>
    <li>Query, Key, Value projections: $3 \times Bnd = 3 \times 32 \times 512 \times 768 \times 4 \text{ bytes} \approx 113$ MB
    <li>Attention scores: $B \times h \times n \times n = 32 \times 12 \times 512 \times 512 \times 4 \text{ bytes} \approx 402$ MB
    <li>Attention output: $Bnd \approx 38$ MB
    <li>Feed-forward intermediate: $B \times n \times 4d \approx 151$ MB
    <li><strong>Per layer total: $\approx 704$ MB</strong>
</ul>

<p><strong>For 12 layers: $704 \times 12 \approx 8.4$ GB</strong></p>

<p>This excludes gradients and optimizer states!
</div>

<h3>Automatic Differentiation: Forward vs Reverse Mode</h3>

<div class="definition"><strong>Definition:</strong> 
Forward mode computes derivatives by propagating tangent vectors forward through the computational graph. For function $f: \R^n \to \R^m$, computing $\nabla f$ requires $n$ forward passes.
</div>

<div class="definition"><strong>Definition:</strong> 
Reverse mode (backpropagation) computes derivatives by propagating adjoints backward. Computing $\nabla f$ requires 1 forward pass + 1 backward pass, regardless of $n$.
</div>

<div class="keypoint">
For neural networks where $n \gg m$ (millions of parameters, one loss), reverse mode is vastly more efficient: $O(1)$ passes vs $O(n)$ passes.
</div>

<div class="example"><strong>Example:</strong> 
For a network with $n = 10^8$ parameters (100M) and scalar loss ($m = 1$):

<p><strong>Forward mode:</strong>
<ul>
    <li>Requires $10^8$ forward passes
    <li>Each pass: $\approx 100$ GFLOPs
    <li>Total: $10^{10}$ GFLOPs $\approx 10$ PFLOPs
    <li>Time on A100 GPU (312 TFLOPS): $\approx 32,000$ seconds $\approx 9$ hours
</ul>

<p><strong>Reverse mode (backpropagation):</strong>
<ul>
    <li>Requires 1 forward + 1 backward pass
    <li>Total: $\approx 300$ GFLOPs
    <li>Time on A100 GPU: $\approx 0.001$ seconds
    <li><strong>Speedup: $\approx 32$ million√ó</strong>
</ul>
</div>

<h3>Gradient Checkpointing</h3>

<p>Gradient checkpointing trades computation for memory by recomputing activations during the backward pass.</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: Gradient Checkpointing</div>
<div class="algorithm-line"><strong>Input:</strong> Network with $L$ layers, checkpoint every $k$ layers</div>
<div class="algorithm-line"><span class="algorithm-comment">// Forward Pass</span></div>
<div class="algorithm-line"><strong>for</strong> $\ell = 1$ to $L$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Compute $\vh^{(\ell)} = f^{(\ell)}(\vh^{(\ell-1)})$</div>
<div class="algorithm-line"><strong>if</strong> $\ell \bmod k = 0$ <strong>then</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Save $\vh^{(\ell)}$ to memory (checkpoint)</div>
</div>
</div>
<div class="algorithm-line"><span class="algorithm-comment">// Backward Pass</span></div>
<div class="algorithm-line"><strong>for</strong> $\ell = L$ to $1$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Recompute forward from last checkpoint to layer $\ell$</div>
</div>
<div class="algorithm-line">Compute gradient $\nabla_{\vh^{(\ell-1)}} L$ using $\vh^{(\ell)}$</div>
</div>

<div class="example"><strong>Example:</strong> 
For BERT-base (12 layers) with checkpointing every 3 layers:

<p><strong>Without checkpointing:</strong>
<ul>
    <li>Memory: $8.4$ GB (all activations)
    <li>Computation: $289$ GFLOPs (1 forward + 1 backward)
</ul>

<p><strong>With checkpointing (every 3 layers):</strong>
<ul>
    <li>Memory: $8.4 / 3 \approx 2.8$ GB (only checkpoints)
    <li>Computation: $96 + 193 + 72 = 361$ GFLOPs (1 forward + 1 backward + 0.75 forward recompute)
    <li><strong>Memory reduction: 3√ó, Computation increase: 1.25√ó</strong>
</ul>

<p>For GPT-3 (175B parameters), checkpointing is essential to fit in GPU memory.
</div>

<h2>Backpropagation</h2>

<p>Backpropagation efficiently computes gradients in neural networks using the chain rule.</p>

<h3>Computational Graphs</h3>

<p>A computational graph represents the sequence of operations in a neural network. Each node is an operation, and edges carry values/gradients.</p>

<div class="example"><strong>Example:</strong> 
For $L = (y - \hat{y})^2$ where $\hat{y} = w_2 \sigma(w_1 x + b_1) + b_2$:

<p><strong>Forward pass:</strong>
<div class="equation">
$$\begin{align}
z_1 &= w_1 x + b_1 = 2.0(1.0) + 0.5 = 2.5 \\
a_1 &= \sigma(z_1) = \sigma(2.5) = 0.924 \quad \text{(sigmoid)} \\
z_2 &= w_2 a_1 + b_2 = 1.5(0.924) + 0.3 = 1.686 \\
L &= (y - z_2)^2 = (3.0 - 1.686)^2 = 1.726
\end{align}$$
</div>

<p><strong>Backward pass:</strong>
<div class="equation">
$$\begin{align}
\frac{\partial L}{\partial z_2} &= 2(z_2 - y) = 2(1.686 - 3.0) = -2.628 \\
\frac{\partial L}{\partial w_2} &= \frac{\partial L}{\partial z_2} \cdot a_1 = -2.628(0.924) = -2.428 \\
\frac{\partial L}{\partial a_1} &= \frac{\partial L}{\partial z_2} \cdot w_2 = -2.628(1.5) = -3.942 \\
\frac{\partial L}{\partial z_1} &= \frac{\partial L}{\partial a_1} \cdot \sigma'(z_1) = -3.942(0.070) = -0.276 \\
\frac{\partial L}{\partial w_1} &= \frac{\partial L}{\partial z_1} \cdot x = -0.276(1.0) = -0.276
\end{align}$$
</div>
</div>

<h3>Backpropagation Algorithm</h3>

<div class="algorithm"><div class="algorithm-title">Algorithm: Backpropagation</div>
<div class="algorithm-line"><strong>Input:</strong> Training example $(\vx, y)$, network with $L$ layers</div>
<div class="algorithm-line"><strong>Output:</strong> Gradients $\{\nabla_{\mW^{(\ell)}} L, \nabla_{\vb^{(\ell)}} L\}_{\ell=1}^L$</div>
<div class="algorithm-line"><span class="algorithm-comment">// Forward Pass</span></div>
<div class="algorithm-line">$\vh^{(0)} = \vx$</div>
<div class="algorithm-line"><strong>for</strong> $\ell = 1$ to $L$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">$\vz^{(\ell)} = \mW^{(\ell)} \vh^{(\ell-1)} + \vb^{(\ell)}$</div>
<div class="algorithm-line">$\vh^{(\ell)} = \sigma^{(\ell)}(\vz^{(\ell)})$</div>
</div>
<div class="algorithm-line">$\hat{y} = \vh^{(L)}$</div>
<div class="algorithm-line">Compute loss: $L = \text{Loss}(y, \hat{y})$</p></div>
<div class="algorithm-line"><span class="algorithm-comment">// Backward Pass</span></div>
<div class="algorithm-line">$\boldsymbol{\delta}^{(L)} = \nabla_{\vh^{(L)}} L \odot \sigma'^{(L)}(\vz^{(L)})$</div>
<div class="algorithm-line"><strong>for</strong> $\ell = L$ to $1$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">$\nabla_{\mW^{(\ell)}} L = \boldsymbol{\delta}^{(\ell)} (\vh^{(\ell-1)})\transpose$</div>
<div class="algorithm-line">$\nabla_{\vb^{(\ell)}} L = \boldsymbol{\delta}^{(\ell)}$</div>
<div class="algorithm-line"><strong>if</strong> $\ell > 1$ <strong>then</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">$\boldsymbol{\delta}^{(\ell-1)} = (\mW^{(\ell)})\transpose \boldsymbol{\delta}^{(\ell)} \odot \sigma'^{(\ell-1)}(\vz^{(\ell-1)})$</div>
</div>
</div>
<div class="algorithm-line"><strong>return</strong> All gradients</div>
</div>

<div class="keypoint">
Backpropagation computes gradients in $O(n)$ time where $n$ is the number of parameters, compared to $O(n^2)$ for naive methods. This efficiency enables training of billion-parameter models.
</div>

<h3>Why Backpropagation is $O(n)$ Not $O(n^2)$</h3>

<div class="theorem"><strong>Theorem:</strong> 
For a neural network with $n$ parameters and $m$ operations, backpropagation computes all gradients in $O(m)$ time, where typically $m = O(n)$.
</div>

<div class="proof"><strong>Proof (Intuition):</strong> 
Each operation in the forward pass corresponds to one gradient computation in the backward pass. The chain rule allows us to reuse intermediate gradients:
<div class="equation">
$$
\frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial z_j} \cdot \frac{\partial z_j}{\partial w_i}
$$
</div>

<p>We compute $\frac{\partial L}{\partial z_j}$ once and reuse it for all parameters that affect $z_j$. This sharing prevents the $O(n^2)$ cost of computing each gradient independently.
</div>

<div class="example"><strong>Example:</strong> 
For a network with $n = 10^8$ parameters:

<p><strong>Naive finite differences:</strong>
<div class="equation">
$$
\frac{\partial L}{\partial w_i} \approx \frac{L(w_i + \epsilon) - L(w_i)}{\epsilon}
$$
</div>
Requires $n$ forward passes: $O(n \cdot m) = O(n^2)$ operations.</p>

<p><strong>Backpropagation:</strong>
<ul>
    <li>Forward pass: $O(m) = O(n)$ operations
    <li>Backward pass: $O(m) = O(n)$ operations
    <li>Total: $O(n)$ operations
</ul>

<p><strong>Speedup: $O(n) = 10^8$√ó</strong>
</div>

<h2>Optimizer Memory Requirements</h2>

<p>Different optimizers have vastly different memory requirements, which becomes critical for large models.</p>

<h3>Memory Comparison by Optimizer</h3>

<table>
<tr><th><strong>Optimizer</strong></th><th><strong>Memory per Parameter</strong></th><th><strong>Total Memory Factor</strong></th></tr>
<tr><td>SGD (no momentum)</td><td>4 bytes (fp32)</td><td>1√ó</td></tr>
<tr><td>SGD with momentum</td><td>8 bytes (param + velocity)</td><td>2√ó</td></tr>
<tr><td>Adam</td><td>16 bytes (param + 2 moments)</td><td>4√ó</td></tr>
<tr><td>Adam (mixed precision)</td><td>10 bytes (fp16 param + fp32 master + 2 moments)</td><td>2.5√ó</td></tr>
</table>

<div class="example"><strong>Example:</strong> 
For BERT-base with 110M parameters:

<p><strong>Model parameters:</strong>
<ul>
    <li>FP32: $110 \times 10^6 \times 4 \text{ bytes} = 440$ MB
    <li>FP16: $110 \times 10^6 \times 2 \text{ bytes} = 220$ MB
</ul>

<p><strong>SGD with momentum:</strong>
<ul>
    <li>Parameters: 440 MB
    <li>Momentum buffer: 440 MB
    <li><strong>Total: 880 MB</strong>
</ul>

<p><strong>Adam optimizer:</strong>
<ul>
    <li>Parameters: 440 MB
    <li>First moment ($\mathbf{m}$): 440 MB
    <li>Second moment ($\mathbf{v}$): 440 MB
    <li>Gradients: 440 MB
    <li><strong>Total: 1,760 MB $\approx 1.7$ GB</strong>
</ul>

<p><strong>Adam with mixed precision:</strong>
<ul>
    <li>FP16 parameters: 220 MB
    <li>FP32 master copy: 440 MB
    <li>FP32 first moment: 440 MB
    <li>FP32 second moment: 440 MB
    <li>FP16 gradients: 220 MB
    <li><strong>Total: 1,760 MB $\approx 1.7$ GB</strong>
</ul>

<p>Note: Mixed precision doesn't reduce optimizer memory, but enables larger batch sizes.
</div>

<div class="example"><strong>Example:</strong> 
For GPT-3 (175B parameters) with Adam optimizer:

<p><strong>Model + optimizer states:</strong>
<ul>
    <li>Parameters (FP16): $175 \times 10^9 \times 2 = 350$ GB
    <li>Master copy (FP32): $175 \times 10^9 \times 4 = 700$ GB
    <li>First moment (FP32): 700 GB
    <li>Second moment (FP32): 700 GB
    <li>Gradients (FP16): 350 GB
    <li><strong>Total: 2,800 GB $\approx 2.8$ TB</strong>
</ul>

<p>This requires distributed training across multiple GPUs. With 8√ó A100 GPUs (80 GB each = 640 GB total), we need model parallelism and optimizer state sharding (e.g., ZeRO optimizer).
</div>

<h3>Impact on GPU Memory Budget</h3>

<div class="keypoint">
For large models, optimizer states often consume more memory than the model itself. Adam uses 4√ó parameter memory, leaving less room for batch size and activations.
</div>

<div class="example"><strong>Example:</strong> 
Training BERT-base on A100 GPU (80 GB memory):

<p><strong>Memory allocation:</strong>
<ul>
    <li>Model parameters: 0.44 GB
    <li>Optimizer states (Adam): 1.32 GB
    <li>Activations (batch size 32): 8.4 GB
    <li>Gradients: 0.44 GB
    <li>Framework overhead: $\approx 2$ GB
    <li><strong>Total: $\approx 12.6$ GB</strong>
</ul>

<p><strong>Remaining: 67.4 GB</strong> available for larger batch sizes or longer sequences.</p>

<p>With batch size 256: Activations $\approx 67$ GB, total $\approx 71$ GB (fits comfortably).
</div>

<h2>Learning Rate Schedules</h2>

<p>Learning rate schedules adjust $\eta$ during training to improve convergence.</p>

<h3>Learning Rate Impact on Convergence and GPU Utilization</h3>

<div class="keypoint">
Learning rate affects both convergence speed and hardware efficiency. Larger learning rates enable larger batch sizes, improving GPU utilization.
</div>

<div class="example"><strong>Example:</strong> 
Training BERT-base on 1M examples:

<table>
<tr><th><strong>Learning Rate</strong></th><th><strong>Steps to Converge</strong></th><th><strong>Wall Time</strong></th><th><strong>Final Loss</strong></th></tr>
<tr><td>$1 \times 10^{-5}$</td><td>100,000</td><td>12 hours</td><td>1.85</td></tr>
<tr><td>$1 \times 10^{-4}$</td><td>30,000</td><td>3.6 hours</td><td>1.82</td></tr>
<tr><td>$5 \times 10^{-4}$</td><td>15,000</td><td>1.8 hours</td><td>1.81</td></tr>
<tr><td>$1 \times 10^{-3}$</td><td>20,000</td><td>2.4 hours</td><td>1.83</td></tr>
<tr><td>$5 \times 10^{-3}$</td><td>Diverges</td><td>-</td><td>-</td></tr>
</table>

<p>Optimal learning rate ($5 \times 10^{-4}$) achieves 6.7√ó faster convergence than conservative rate.
</div>

<h3>Learning Rate Scaling with Batch Size</h3>

<div class="theorem"><strong>Theorem:</strong> 
When increasing batch size by factor $k$, scale learning rate by $k$ to maintain convergence behavior:
<div class="equation">
$$
\eta_{\text{new}} = k \cdot \eta_{\text{base}}
$$
</div>

<p>This holds approximately for $k \leq 8$. For larger $k$, use gradual warmup.
</div>

<div class="example"><strong>Example:</strong> 
Training BERT-base with different batch sizes:

<table>
<tr><th><strong>Batch Size</strong></th><th><strong>Learning Rate</strong></th><th><strong>GPU Util.</strong></th><th><strong>Steps/sec</strong></th><th><strong>Samples/sec</strong></th></tr>
<tr><td>32</td><td>$5 \times 10^{-4}$</td><td>45\%</td><td>2.1</td><td>67</td></tr>
<tr><td>64</td><td>$1 \times 10^{-3}$</td><td>68\%</td><td>1.8</td><td>115</td></tr>
<tr><td>128</td><td>$2 \times 10^{-3}$</td><td>85\%</td><td>1.4</td><td>179</td></tr>
<tr><td>256</td><td>$4 \times 10^{-3}$</td><td>92\%</td><td>1.0</td><td>256</td></tr>
<tr><td>512</td><td>$8 \times 10^{-3}$</td><td>95\%</td><td>0.6</td><td>307</td></tr>
</table>

<p>Larger batches improve GPU utilization but require proportionally larger learning rates. Throughput increases 4.6√ó from batch 32 to 512.
</div>

<h3>Practical Learning Rates for Transformers</h3>

<table>
<tr><th><strong>Model</strong></th><th><strong>Batch Size</strong></th><th><strong>Peak Learning Rate</strong></th></tr>
<tr><td>BERT-base</td><td>256</td><td>$1 \times 10^{-4}$</td></tr>
<tr><td>BERT-large</td><td>256</td><td>$5 \times 10^{-5}$</td></tr>
<tr><td>GPT-2 (117M)</td><td>512</td><td>$2.5 \times 10^{-4}$</td></tr>
<tr><td>GPT-2 (1.5B)</td><td>512</td><td>$1.5 \times 10^{-4}$</td></tr>
<tr><td>GPT-3 (175B)</td><td>3.2M</td><td>$6 \times 10^{-5}$</td></tr>
<tr><td>T5-base</td><td>128</td><td>$1 \times 10^{-4}$</td></tr>
<tr><td>T5-11B</td><td>2048</td><td>$1 \times 10^{-4}$</td></tr>
</table>

<div class="keypoint">
Larger models generally require smaller learning rates for stability. GPT-3 uses $6 \times 10^{-5}$ despite massive batch size of 3.2M tokens.
</div>

<h3>Common Schedules</h3>

<p><strong>Step Decay:</strong>
<div class="equation">
$$
\eta_t = \eta_0 \gamma^{\lfloor t/s \rfloor}
$$
</div>
where $\gamma < 1$ (e.g., 0.1) and $s$ is step size (e.g., every 10 epochs).</p>

<p><strong>Exponential Decay:</strong>
<div class="equation">
$$
\eta_t = \eta_0 e^{-\lambda t}
$$
</div>

<p><strong>Cosine Annealing:</strong>
<div class="equation">
$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t\pi}{T}\right)\right)
$$
</div>

<p><strong>Warmup + Decay (Transformers):</strong>
<div class="equation">
$$
\eta_t = \frac{d_{\text{model}}^{-0.5}}{\max(t, \text{warmup\_steps}^{-0.5})} \cdot \min(t^{-0.5}, t \cdot \text{warmup\_steps}^{-1.5})
$$
</div>

<p>The warmup phase prevents instability in early training of transformers.</p>

<h2>Hardware Considerations for Gradient Computation</h2>

<p>Modern deep learning relies on specialized hardware for efficient gradient computation.</p>

<h3>Gradient Computation on GPUs</h3>

<p>GPUs excel at gradient computation due to massive parallelism in matrix operations.</p>

<div class="example"><strong>Example:</strong> 
Computing gradients for BERT-base (110M parameters) on one training batch:

<p><strong>NVIDIA A100 GPU:</strong>
<ul>
    <li>Forward pass: 0.31 ms (96 GFLOPs √∑ 312 TFLOPS)
    <li>Backward pass: 0.62 ms (193 GFLOPs √∑ 312 TFLOPS)
    <li><strong>Total: 0.93 ms per batch</strong>
    <li>Throughput: 1,075 batches/second
</ul>

<p><strong>Intel Xeon CPU (32 cores):</strong>
<ul>
    <li>Forward pass: 45 ms (96 GFLOPs √∑ 2.1 TFLOPS)
    <li>Backward pass: 90 ms (193 GFLOPs √∑ 2.1 TFLOPS)
    <li><strong>Total: 135 ms per batch</strong>
    <li>Throughput: 7.4 batches/second
</ul>

<p><strong>GPU speedup: 145√ó</strong>
</div>

<h3>Mixed Precision Training</h3>

<p>Mixed precision uses FP16 for computation and FP32 for accumulation, reducing memory and increasing speed.</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: Mixed Precision Training</div>
<div class="algorithm-line"><strong>Input:</strong> Model parameters $\vw$ (FP32 master copy)</div>
<div class="algorithm-line"><strong>for</strong> each training step <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Convert $\vw$ to FP16: $\vw_{16} = \text{FP16}(\vw)$</div>
<div class="algorithm-line">Forward pass in FP16: $\hat{y} = f(\vx; \vw_{16})$</div>
<div class="algorithm-line">Compute loss: $L = \text{Loss}(y, \hat{y})$</div>
<div class="algorithm-line">Scale loss: $L_{\text{scaled}} = s \cdot L$ (prevent underflow)</div>
<div class="algorithm-line">Backward pass in FP16: $\mathbf{g}_{16} = \nabla_{\vw_{16}} L_{\text{scaled}}$</div>
<div class="algorithm-line">Unscale gradients: $\mathbf{g}_{16} = \mathbf{g}_{16} / s$</div>
<div class="algorithm-line">Convert to FP32: $\mathbf{g} = \text{FP32}(\mathbf{g}_{16})$</div>
<div class="algorithm-line">Update FP32 master: $\vw \leftarrow \vw - \eta \mathbf{g}$</div>
</div>
</div>

<div class="example"><strong>Example:</strong> 
Training BERT-base on A100 GPU:

<p><strong>FP32 training:</strong>
<ul>
    <li>Forward + backward: 0.93 ms
    <li>Memory: 12.6 GB
    <li>Max batch size: 32
    <li>Throughput: 34,400 samples/sec
</ul>

<p><strong>Mixed precision (FP16) training:</strong>
<ul>
    <li>Forward + backward: 0.48 ms (1.94√ó faster)
    <li>Memory: 8.2 GB (35\% reduction)
    <li>Max batch size: 64
    <li>Throughput: 133,300 samples/sec (3.87√ó faster)
</ul>

<p>Mixed precision provides 1.94√ó computational speedup and enables 2√ó larger batches, yielding 3.87√ó total throughput improvement.
</div>

<h3>Gradient Accumulation</h3>

<p>Gradient accumulation simulates large batch sizes by accumulating gradients over multiple forward-backward passes.</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: Gradient Accumulation</div>
<div class="algorithm-line"><strong>Input:</strong> Desired batch size $B$, physical batch size $b$, accumulation steps $k = B/b$</div>
<div class="algorithm-line">Initialize gradients: $\mathbf{g}_{\text{acc}} = \mathbf{0}$</div>
<div class="algorithm-line"><strong>for</strong> $i = 1$ to $k$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Sample mini-batch $\mathcal{B}_i$ of size $b$</div>
<div class="algorithm-line">Forward pass: $L_i = \text{Loss}(\mathcal{B}_i)$</div>
<div class="algorithm-line">Backward pass: $\mathbf{g}_i = \nabla L_i$</div>
<div class="algorithm-line">Accumulate: $\mathbf{g}_{\text{acc}} \leftarrow \mathbf{g}_{\text{acc}} + \mathbf{g}_i$</div>
</div>
<div class="algorithm-line">Average: $\mathbf{g}_{\text{acc}} \leftarrow \mathbf{g}_{\text{acc}} / k$</div>
<div class="algorithm-line">Update parameters: $\vw \leftarrow \vw - \eta \mathbf{g}_{\text{acc}}$</div>
<div class="algorithm-line">Clear gradients: $\mathbf{g}_{\text{acc}} = \mathbf{0}$</div>
</div>

<div class="example"><strong>Example:</strong> 
Training GPT-2 (1.5B parameters) on single A100 GPU (80 GB):

<p><strong>Without accumulation:</strong>
<ul>
    <li>Max batch size: 4 (memory limit)
    <li>Update frequency: every 4 samples
    <li>Training unstable (batch too small)
</ul>

<p><strong>With gradient accumulation (32 steps):</strong>
<ul>
    <li>Physical batch size: 4
    <li>Effective batch size: $4 \times 32 = 128$
    <li>Update frequency: every 128 samples
    <li>Memory: same as batch size 4
    <li>Training stable and efficient
</ul>

<p>Trade-off: 32√ó more forward-backward passes per update, but enables training large models on limited hardware.
</div>

<h3>Distributed Gradient Synchronization</h3>

<p>For multi-GPU training, gradients must be synchronized across devices.</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: Data Parallel Training with Gradient Synchronization</div>
<div class="algorithm-line"><strong>Input:</strong> $N$ GPUs, global batch size $B$, local batch size $b = B/N$</div>
<div class="algorithm-line"><strong>for</strong> each GPU $i = 1, \ldots, N$ in parallel <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Sample local mini-batch $\mathcal{B}_i$ of size $b$</div>
<div class="algorithm-line">Forward pass: $L_i = \text{Loss}(\mathcal{B}_i)$</div>
<div class="algorithm-line">Backward pass: $\mathbf{g}_i = \nabla L_i$</div>
</div>
<div class="algorithm-line">All-reduce gradients: $\mathbf{g} = \frac{1}{N} \sum_{i=1}^N \mathbf{g}_i$</div>
<div class="algorithm-line"><strong>for</strong> each GPU $i = 1, \ldots, N$ in parallel <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Update local parameters: $\vw_i \leftarrow \vw_i - \eta \mathbf{g}$</div>
</div>
</div>

<div class="example"><strong>Example:</strong> 
Training BERT-base on 8√ó A100 GPUs with NVLink:

<p><strong>Single GPU baseline:</strong>
<ul>
    <li>Batch size: 32
    <li>Time per step: 0.93 ms
    <li>Throughput: 34,400 samples/sec
</ul>

<p><strong>8 GPUs (data parallel):</strong>
<ul>
    <li>Global batch size: 256
    <li>Time per step: 0.93 ms (computation) + 0.12 ms (communication)
    <li>Total: 1.05 ms
    <li>Throughput: 243,800 samples/sec
    <li><strong>Scaling efficiency: 243,800 / (8 √ó 34,400) = 88.6\%</strong>
</ul>

<p>Communication overhead is 11.4\% due to gradient all-reduce. NVLink (600 GB/s) enables efficient synchronization.
</div>

<div class="keypoint">
For large models, gradient synchronization can become a bottleneck. Techniques like gradient compression, ZeRO optimizer, and pipeline parallelism reduce communication overhead.
</div>

<h2>Exercises</h2>

<div class="exercise"><strong>Exercise:</strong> Compute the gradient of $f(\vw) = \vw\transpose \mA \vw + \vb\transpose \vw + c$ where $\mA \in \R^{n \times n}$ is symmetric, $\vw, \vb \in \R^n$, and $c \in \R$.
</div>

<div class="exercise"><strong>Exercise:</strong> Implement backpropagation for a 2-layer network with ReLU activation. Given input $\vx = [1.0, 0.5]\transpose$, weights $\mW^{(1)} \in \R^{3 \times 2}$, $\mW^{(2)} \in \R^{1 \times 3}$, and target $y = 2.0$, compute all gradients.
</div>

<div class="exercise"><strong>Exercise:</strong> For Adam optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\alpha = 0.001$:
<ol>
    <li>Why is bias correction necessary?
    <li>What are the effective learning rates after steps $t = 1, 10, 100, 1000$?
    <li>How does Adam handle sparse gradients compared to SGD?
</ol>
</div>

<div class="exercise"><strong>Exercise:</strong> A transformer is trained with learning rate warmup over 4000 steps, then inverse square root decay. If $d_{\text{model}} = 512$:
<ol>
    <li>Plot the learning rate schedule for 100,000 steps
    <li>What is the learning rate at step 1, 4000, and 10,000?
    <li>Why is warmup beneficial for transformer training?
</ol>
</div>

<div class="exercise"><strong>Exercise:</strong> Calculate the memory requirements for training GPT-2 (1.5B parameters) with Adam optimizer:
<ol>
    <li>Model parameters in FP16
    <li>Optimizer states (FP32 master copy + 2 moments)
    <li>Gradients in FP16
    <li>Total memory for model + optimizer
    <li>How many A100 GPUs (80 GB each) are needed?
</ol>
</div>

<div class="exercise"><strong>Exercise:</strong> For BERT-base processing sequence length 512 with batch size 64:
<ol>
    <li>Calculate total FLOPs for one training step (forward + backward)
    <li>Estimate time per step on A100 GPU (312 TFLOPS)
    <li>How does mixed precision (FP16) affect throughput?
    <li>What is the maximum batch size that fits in 80 GB memory?
</ol>
</div>

<div class="exercise"><strong>Exercise:</strong> Compare gradient computation methods for a network with $10^7$ parameters:
<ol>
    <li>How many forward passes does finite differences require?
    <li>How many passes does backpropagation require?
    <li>If one forward pass takes 10 ms, compare total time
    <li>Why is reverse mode AD preferred over forward mode?
</ol>
</div>

<div class="exercise"><strong>Exercise:</strong> Implement gradient checkpointing for a 24-layer transformer:
<ol>
    <li>Without checkpointing, how much activation memory is needed?
    <li>With checkpointing every 6 layers, what is the memory reduction?
    <li>What is the computational overhead (extra forward passes)?
    <li>At what model size does checkpointing become necessary?
</ol>
</div>

<div class="exercise"><strong>Exercise:</strong> Analyze distributed training efficiency for 8 GPUs:
<ol>
    <li>If gradient all-reduce takes 15 ms and computation takes 100 ms, what is the scaling efficiency?
    <li>How does batch size affect communication overhead?
    <li>Compare ring all-reduce vs tree all-reduce for 64 GPUs
    <li>When does gradient compression become beneficial?
</ol>
</div>

<h2>Solutions</h2>

<div class="solution"><strong>Solution:</strong> For $f(\vw) = \vw\transpose \mA \vw + \vb\transpose \vw + c$ where $\mA$ is symmetric:

<p>Using the gradient rules:
<ul>
    <li>$\nabla_{\vw}(\vw\transpose \mA \vw) = 2\mA\vw$ (since $\mA$ is symmetric)
    <li>$\nabla_{\vw}(\vb\transpose \vw) = \vb$
    <li>$\nabla_{\vw}(c) = \mathbf{0}$
</ul>

<p>Therefore:
<div class="equation">
$$
\nabla_{\vw} f = 2\mA\vw + \vb
$$
</div>
</div>

<div class="solution"><strong>Solution:</strong> Given: $\vx = [1.0, 0.5]\transpose$, target $y = 2.0$, ReLU activation.

<p>Let's use specific weights:
<div class="equation">
$$
\mW^{(1)} = \begin{bmatrix} 0.5 & -0.3 \\ 0.2 & 0.6 \\ -0.4 & 0.8 \end{bmatrix}, \quad \mW^{(2)} = \begin{bmatrix} 1.0 & -0.5 & 0.7 \end{bmatrix}
$$
</div>

<p><strong>Forward pass:</strong>
<div class="equation">
$$\begin{align}
\vz^{(1)} &= \mW^{(1)}\vx = \begin{bmatrix} 0.5(1.0) - 0.3(0.5) \\ 0.2(1.0) + 0.6(0.5) \\ -0.4(1.0) + 0.8(0.5) \end{bmatrix} = \begin{bmatrix} 0.35 \\ 0.50 \\ 0.00 \end{bmatrix} \\
\vh^{(1)} &= \text{ReLU}(\vz^{(1)}) = \begin{bmatrix} 0.35 \\ 0.50 \\ 0.00 \end{bmatrix} \\
z^{(2)} &= \mW^{(2)}\vh^{(1)} = 1.0(0.35) - 0.5(0.50) + 0.7(0.00) = 0.10 \\
L &= \frac{1}{2}(y - z^{(2)})^2 = \frac{1}{2}(2.0 - 0.10)^2 = 1.805
\end{align}$$
</div>

<p><strong>Backward pass:</strong>
<div class="equation">
$$\begin{align}
\frac{\partial L}{\partial z^{(2)}} &= -(y - z^{(2)}) = -(2.0 - 0.10) = -1.90 \\
\frac{\partial L}{\partial \mW^{(2)}} &= \frac{\partial L}{\partial z^{(2)}} \vh^{(1)\transpose} = -1.90 \begin{bmatrix} 0.35 & 0.50 & 0.00 \end{bmatrix} = \begin{bmatrix} -0.665 & -0.950 & 0.000 \end{bmatrix} \\
\frac{\partial L}{\partial \vh^{(1)}} &= \mW^{(2)\transpose} \frac{\partial L}{\partial z^{(2)}} = \begin{bmatrix} 1.0 \\ -0.5 \\ 0.7 \end{bmatrix}(-1.90) = \begin{bmatrix} -1.90 \\ 0.95 \\ -1.33 \end{bmatrix} \\
\frac{\partial L}{\partial \vz^{(1)}} &= \frac{\partial L}{\partial \vh^{(1)}} \odot \text{ReLU}'(\vz^{(1)}) = \begin{bmatrix} -1.90 \\ 0.95 \\ -1.33 \end{bmatrix} \odot \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} -1.90 \\ 0.95 \\ 0.00 \end{bmatrix} \\
\frac{\partial L}{\partial \mW^{(1)}} &= \frac{\partial L}{\partial \vz^{(1)}} \vx\transpose = \begin{bmatrix} -1.90 \\ 0.95 \\ 0.00 \end{bmatrix} \begin{bmatrix} 1.0 & 0.5 \end{bmatrix} = \begin{bmatrix} -1.90 & -0.95 \\ 0.95 & 0.475 \\ 0.00 & 0.00 \end{bmatrix}
\end{align}$$
</div>
</div>

<div class="solution"><strong>Solution:</strong> For Adam optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\alpha = 0.001$:

<p><strong>(1) Why bias correction is necessary:</strong>
The first and second moment estimates are initialized to zero, creating a bias toward zero in early iterations. Without correction, the effective learning rate would be too small initially. Bias correction factors $\frac{1}{1-\beta_1^t}$ and $\frac{1}{1-\beta_2^t}$ compensate for this initialization bias.</p>

<p><strong>(2) Effective learning rates:</strong>
The effective learning rate is $\alpha_{\text{eff}} = \alpha \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}$:</p>

<ul>
    <li>$t=1$: $\alpha_{\text{eff}} = 0.001 \times \frac{\sqrt{1-0.999}}{1-0.9} = 0.001 \times \frac{0.0316}{0.1} \approx 0.000316$
    <li>$t=10$: $\alpha_{\text{eff}} = 0.001 \times \frac{\sqrt{1-0.999^{10}}}{1-0.9^{10}} \approx 0.001 \times \frac{0.0998}{0.651} \approx 0.000153$
    <li>$t=100$: $\alpha_{\text{eff}} = 0.001 \times \frac{\sqrt{1-0.999^{100}}}{1-0.9^{100}} \approx 0.001 \times \frac{0.302}{1.000} \approx 0.000302$
    <li>$t=1000$: $\alpha_{\text{eff}} \approx 0.001$ (bias correction negligible)
</ul>

<p><strong>(3) Handling sparse gradients:</strong>
Adam maintains separate adaptive learning rates for each parameter through the second moment estimate $\mathbf{v}$. For sparse gradients, parameters with infrequent updates have smaller $v_i$ values, resulting in larger effective learning rates. This allows Adam to make larger updates to rarely-updated parameters, unlike SGD which treats all parameters equally. This is particularly beneficial for embedding layers and natural language processing tasks.
</div>

<div class="solution"><strong>Solution:</strong> For transformer with $d_{\text{model}} = 512$ and warmup over 4000 steps:

<p>The learning rate schedule is:
<div class="equation">
$$
\eta(t) = d_{\text{model}}^{-0.5} \cdot \min(t^{-0.5}, t \cdot \text{warmup}^{-1.5})
$$
</div>

<p><strong>(1) Plot description:</strong>
The schedule has two phases:
<ul>
    <li>Warmup ($t \leq 4000$): Linear increase $\eta(t) = \frac{t}{4000} \cdot 512^{-0.5} \approx 0.0011 \cdot t$
    <li>Decay ($t > 4000$): Inverse square root $\eta(t) = 512^{-0.5} \cdot t^{-0.5} \approx \frac{1.414}{\sqrt{t}}$
</ul>

<p><strong>(2) Learning rates at specific steps:</strong>
<ul>
    <li>$t=1$: $\eta = 512^{-0.5} \cdot 1 \cdot 4000^{-1.5} \approx 0.0000111$
    <li>$t=4000$: $\eta = 512^{-0.5} \cdot 4000^{-0.5} \approx 0.0222$ (peak)
    <li>$t=10000$: $\eta = 512^{-0.5} \cdot 10000^{-0.5} \approx 0.0141$
</ul>

<p><strong>(3) Why warmup is beneficial:</strong>
<ul>
    <li>Prevents instability from large gradients in early training when parameters are randomly initialized
    <li>Allows the optimizer's momentum statistics to stabilize
    <li>Particularly important for Adam, where the second moment estimate needs time to accumulate
    <li>Without warmup, large initial learning rates can cause divergence or poor local minima
</ul>
</div>

<div class="solution"><strong>Solution:</strong> For GPT-2 with 1.5B parameters and Adam optimizer:

<p><strong>(1) Model parameters in FP16:</strong>
<div class="equation">
$$
1.5 \times 10^9 \times 2 \text{ bytes} = 3 \times 10^9 \text{ bytes} = 3 \text{ GB}
$$
</div>

<p><strong>(2) Optimizer states:</strong>
<ul>
    <li>FP32 master copy: $1.5 \times 10^9 \times 4 = 6$ GB
    <li>First moment $\mathbf{m}$ (FP32): $1.5 \times 10^9 \times 4 = 6$ GB
    <li>Second moment $\mathbf{v}$ (FP32): $1.5 \times 10^9 \times 4 = 6$ GB
    <li>Total optimizer states: $18$ GB
</ul>

<p><strong>(3) Gradients in FP16:</strong>
<div class="equation">
$$
1.5 \times 10^9 \times 2 \text{ bytes} = 3 \text{ GB}
$$
</div>

<p><strong>(4) Total memory:</strong>
<div class="equation">
$$
\text{Model (FP16)} + \text{Optimizer states} + \text{Gradients} = 3 + 18 + 3 = 24 \text{ GB}
$$
</div>

<p><strong>(5) Number of A100 GPUs needed:</strong>
<div class="equation">
$$
\frac{24 \text{ GB}}{80 \text{ GB per GPU}} = 0.3 \text{ GPUs}
$$
</div>

<p>One A100 GPU is sufficient for the model and optimizer states alone. However, activations during training require additional memory, so 1-2 GPUs would be needed in practice depending on batch size.
</div>

<div class="solution"><strong>Solution:</strong> For BERT-base with sequence length 512 and batch size 64:

<p><strong>(1) Total FLOPs per training step:</strong>
From Example~[ref]:
<ul>
    <li>Forward pass: $\approx 96$ GFLOPs per sample
    <li>Backward pass: $\approx 193$ GFLOPs per sample
    <li>Total per sample: $289$ GFLOPs
    <li>For batch of 64: $289 \times 64 = 18{,}496$ GFLOPs $\approx 18.5$ TFLOPs
</ul>

<p><strong>(2) Time per step on A100:</strong>
<div class="equation">
$$
\frac{18.5 \text{ TFLOPs}}{312 \text{ TFLOPs}} \approx 59 \text{ ms}
$$
</div>

<p>In practice, memory bandwidth and kernel launch overhead increase this to $\approx 80$-$100$ ms.</p>

<p><strong>(3) Mixed precision impact:</strong>
<ul>
    <li>FP16 Tensor Cores provide 2√ó speedup: $\approx 30$ ms theoretical
    <li>Reduced memory traffic (2√ó less bandwidth): enables larger batches
    <li>Practical speedup: 1.8-2.2√ó including overhead
    <li>Throughput increase: $\approx 3.5$-$4$√ó due to larger batch sizes
</ul>

<p><strong>(4) Maximum batch size in 80 GB:</strong>
Memory breakdown:
<ul>
    <li>Model + optimizer: $\approx 1.7$ GB
    <li>Activations per sample: $\approx 130$ MB
    <li>Gradients: $\approx 0.44$ GB
    <li>Framework overhead: $\approx 2$ GB
</ul>

<p>Available for activations: $80 - 1.7 - 0.44 - 2 = 75.86$ GB</p>

<p>Maximum batch size: $\frac{75{,}860 \text{ MB}}{130 \text{ MB/sample}} \approx 583$ samples
</div>

<div class="solution"><strong>Solution:</strong> For network with $10^7$ parameters:

<p><strong>(1) Finite differences forward passes:</strong>
Requires one forward pass per parameter: $10^7$ forward passes</p>

<p><strong>(2) Backpropagation passes:</strong>
Requires 1 forward pass + 1 backward pass = 2 passes total</p>

<p><strong>(3) Time comparison:</strong>
<ul>
    <li>Finite differences: $10^7 \times 10 \text{ ms} = 10^8 \text{ ms} = 100{,}000 \text{ seconds} \approx 27.8 \text{ hours}$
    <li>Backpropagation: $2 \times 10 \text{ ms} = 20 \text{ ms}$
    <li>Speedup: $\frac{10^8}{20} = 5 \times 10^6 = 5$ million√ó
</ul>

<p><strong>(4) Why reverse mode AD is preferred:</strong>
<ul>
    <li>For $n$ parameters and scalar loss, forward mode requires $O(n)$ passes while reverse mode requires $O(1)$ passes
    <li>Reverse mode exploits the structure of neural networks: many parameters, one loss
    <li>Memory cost is higher (must store activations) but computational savings are enormous
    <li>Forward mode would be preferred only if we had many outputs and few inputs (rare in deep learning)
</ul>
</div>

<div class="solution"><strong>Solution:</strong> For 24-layer transformer with gradient checkpointing:

<p><strong>(1) Activation memory without checkpointing:</strong>
Assuming $\approx 700$ MB per layer (from Example~[ref]):
<div class="equation">
$$
24 \times 700 \text{ MB} = 16{,}800 \text{ MB} \approx 16.4 \text{ GB}
$$
</div>

<p><strong>(2) Memory reduction with checkpointing every 6 layers:</strong>
We save only 4 checkpoints (layers 6, 12, 18, 24):
<div class="equation">
$$
\text{Memory} = 4 \times 700 \text{ MB} = 2{,}800 \text{ MB} \approx 2.7 \text{ GB}
$$
</div>

<p>Reduction factor: $\frac{16.4}{2.7} \approx 6\times$</p>

<p><strong>(3) Computational overhead:</strong>
For each checkpoint interval, we recompute the forward pass once during backward:
<ul>
    <li>Original: 1 forward + 1 backward
    <li>With checkpointing: 1 forward + 1 backward + 0.75 forward (recompute 18 of 24 layers)
    <li>Overhead: $\frac{1.75}{2} = 87.5\%$ increase, or 1.875√ó total time
</ul>

<p><strong>(4) When checkpointing becomes necessary:</strong>
Checkpointing is essential when:
<ul>
    <li>Activation memory exceeds available GPU memory
    <li>For GPT-3 scale (175B parameters), activations can exceed 100 GB
    <li>Rule of thumb: Use checkpointing when activations $>$ 50\% of GPU memory
    <li>Trade-off: 1.5-2√ó slower training for 4-8√ó memory reduction
</ul>
</div>

<div class="solution"><strong>Solution:</strong> For distributed training with 8 GPUs:

<p><strong>(1) Scaling efficiency:</strong>
<ul>
    <li>Time per step (single GPU): 100 ms
    <li>Time per step (8 GPUs): $\frac{100}{8} + 15 = 12.5 + 15 = 27.5$ ms
    <li>Ideal time (perfect scaling): $\frac{100}{8} = 12.5$ ms
    <li>Scaling efficiency: $\frac{12.5}{27.5} \approx 45.5\%$
</ul>

<p><strong>(2) Batch size effect on communication:</strong>
<ul>
    <li>Communication time is independent of batch size (same gradient size)
    <li>Larger batches increase computation time, reducing communication overhead percentage
    <li>For batch size $B$: efficiency $\approx \frac{100B/8}{100B/8 + 15}$
    <li>Doubling batch size: $\frac{25}{40} = 62.5\%$ efficiency
    <li>4√ó batch size: $\frac{50}{65} = 76.9\%$ efficiency
</ul>

<p><strong>(3) Ring vs tree all-reduce for 64 GPUs:</strong>
<ul>
    <li>Ring all-reduce: $O(N)$ communication steps, bandwidth-optimal
    <li>Tree all-reduce: $O(\log N)$ communication steps, latency-optimal
    <li>For 64 GPUs: Ring has 64 steps, tree has $\log_2(64) = 6$ steps
    <li>Ring is better for large messages (bandwidth-bound)
    <li>Tree is better for small messages (latency-bound)
    <li>Typical gradient sizes favor ring all-reduce
</ul>

<p><strong>(4) When gradient compression is beneficial:</strong>
<ul>
    <li>When communication time $>$ compression time
    <li>For slow networks (inter-node communication)
    <li>Typical compression: 8-bit quantization or top-k sparsification
    <li>Compression ratio: 4√ó (FP32 to 8-bit)
    <li>Beneficial when: $\frac{\text{gradient size}}{\text{bandwidth}} > \frac{\text{gradient size}}{\text{compression throughput}} + \frac{\text{compressed size}}{\text{bandwidth}}$
    <li>Usually beneficial for $>$8 GPUs across multiple nodes
</ul>
</div>
        
        <div class="chapter-nav">
  <a href="chapter01_linear_algebra.html">‚Üê Chapter 1: Linear Algebra for Deep Learning</a>
  <a href="../index.html">üìö Table of Contents</a>
  <a href="chapter03_probability_information.html">Chapter 3: Probability and Information Theory ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
