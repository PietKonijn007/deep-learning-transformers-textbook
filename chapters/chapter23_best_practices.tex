\chapter{Best Practices and Production Case Studies}
\label{chap:best_practices}

\section*{Chapter Overview}

This final chapter synthesizes practical wisdom from deploying transformers at scale. We cover debugging strategies, hyperparameter tuning, common pitfalls, and real-world case studies from industry deployments of BERT, GPT, and other transformer models.

\subsection*{Learning Objectives}

\begin{enumerate}
    \item Apply systematic debugging for transformer training
    \item Tune hyperparameters effectively
    \item Avoid common pitfalls in architecture and training
    \item Learn from real-world deployment case studies
    \item Design robust production systems
    \item Plan future-proof transformer architectures
\end{enumerate}

\section{Model Selection}
\label{sec:model_selection}

Choosing the right transformer architecture is a critical decision that impacts both performance and resource requirements. This section provides a systematic framework for selecting among the major transformer variants based on task requirements, data availability, and computational constraints.

\subsection{Architecture Selection Framework}

The choice between BERT, GPT, T5, and other architectures depends fundamentally on the nature of your task. BERT and its variants excel at understanding tasks where bidirectional context is crucial, such as classification, named entity recognition, and question answering. The bidirectional attention mechanism allows BERT to build rich representations by attending to both past and future tokens simultaneously, making it particularly effective when the entire input is available at once.

GPT models, in contrast, are designed for generation tasks where autoregressive decoding is required. The unidirectional attention pattern makes GPT natural for text generation, code completion, and any task where outputs must be produced sequentially. While GPT can be adapted for understanding tasks through careful prompting, this is generally less efficient than using a bidirectional model designed for the purpose.

T5 represents a unified approach that frames all tasks as sequence-to-sequence problems. This architecture provides flexibility across both understanding and generation tasks, making it an excellent choice when you need a single model to handle diverse task types. The encoder-decoder structure allows T5 to leverage bidirectional attention in the encoder while maintaining autoregressive generation in the decoder.

\subsection{Model Size Selection}

Selecting the appropriate model size requires balancing performance requirements against computational constraints. The relationship between model size and performance generally follows a power law, with diminishing returns as models grow larger. For most practical applications, the base-sized models provide an excellent balance between capability and efficiency.

BERT-base with 110 million parameters serves as the standard choice for most understanding tasks. It provides strong performance across a wide range of benchmarks while remaining tractable for fine-tuning on a single GPU. BERT-large with 340 million parameters offers modest improvements, typically 1-3 percentage points on downstream tasks, but requires significantly more memory and computation. The large variant is justified primarily when you need to extract maximum performance and have sufficient computational resources.

For GPT models, the size selection depends heavily on the complexity of the generation task. GPT-2 small (117M parameters) suffices for simple completion tasks and domain-specific generation after fine-tuning. GPT-2 medium (345M parameters) provides better coherence for longer generations and more complex tasks. The larger variants (GPT-2 large at 774M and GPT-2 XL at 1.5B parameters) are necessary primarily when working with limited task-specific data, as their stronger pre-trained representations enable better few-shot performance.

\subsection{Pre-trained versus Training from Scratch}

The decision to use pre-trained models versus training from scratch depends on data availability, domain specificity, and computational budget. In nearly all cases, starting from pre-trained weights is the correct choice. Pre-training on large corpora provides general language understanding that transfers effectively to downstream tasks, and the computational cost of pre-training from scratch is prohibitive for most organizations.

Training from scratch becomes viable only in specific circumstances. When working with highly specialized domains where general language models perform poorly, such as medical text with extensive jargon or programming languages not well-represented in pre-training data, domain-specific pre-training may be justified. However, even in these cases, continued pre-training from existing checkpoints is typically more efficient than starting from random initialization.

The computational cost difference is substantial. Pre-training BERT-base from scratch requires approximately 64 TPU days or equivalent GPU time, representing tens of thousands of dollars in compute costs. Fine-tuning the same model on a downstream task typically requires only hours on a single GPU, costing tens of dollars. This thousand-fold difference in cost makes pre-trained models the default choice for nearly all applications.

\subsection{Cost-Benefit Analysis}

A systematic cost-benefit analysis should consider both direct computational costs and opportunity costs. For a typical classification task with 10,000 labeled examples, fine-tuning BERT-base requires approximately 2-4 hours on a single V100 GPU, costing roughly \$10-20 in cloud compute. This investment typically yields performance improvements of 5-15 percentage points over traditional methods like logistic regression on TF-IDF features.

Training a smaller model from scratch on the same data might require 8-16 hours and cost \$40-80, while likely achieving inferior performance due to the lack of pre-trained representations. The pre-trained approach thus provides both better performance and lower cost, a rare combination that explains the dominance of transfer learning in modern NLP.

For generation tasks, the cost analysis shifts somewhat. Fine-tuning GPT-2 medium on a specific generation task requires 4-8 hours on a V100, costing \$20-40. However, inference costs become more significant for generation, as producing each token requires a full forward pass through the model. For applications requiring high-throughput generation, the ongoing inference costs may exceed training costs within weeks or months of deployment, making inference optimization critical.

\section{Training Best Practices}
\label{sec:training_best_practices}

Effective training of transformer models requires careful attention to hyperparameter selection, monitoring, and debugging. This section provides comprehensive guidance on the key decisions that impact training success.

\subsection{Learning Rate Selection}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Scenario} & \textbf{Range} & \textbf{Notes} \\
\midrule
BERT fine-tuning & $1$--$5 \times 10^{-5}$ & Lower end for small datasets \\
GPT fine-tuning & $2 \times 10^{-5}$--$10^{-4}$ & Autoregressive is more stable \\
Pre-training from scratch & $10^{-4}$--$6 \times 10^{-4}$ & Requires longer warmup \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Rules of thumb:} Scale LR $\sim$linearly with batch size (use LAMB for very large batches). Warmup: 5--10\% of steps for fine-tuning, 10K--50K steps for pre-training.

\subsection{Batch Size Selection}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Scenario} & \textbf{Typical Range} & \textbf{Notes} \\
\midrule
Fine-tuning (single GPU) & 16--32 & Gradient accumulation for larger effective batch \\
Fine-tuning (multi-GPU) & 32--256 & Scale LR accordingly \\
Pre-training & 256--4096 & Requires LR warmup + LAMB \\
\bottomrule
\end{tabular}
\end{center}

If memory-limited, use gradient accumulation: $B_{\text{eff}} = B_{\text{micro}} \times N_{\text{accum}}$ (see Chapter~\ref{chap:pytorchimplementation} for implementation).

\subsection{Checkpointing and Monitoring Strategy}

Effective monitoring is essential for detecting problems early and understanding training dynamics. At minimum, you should track training loss, validation loss, and task-specific metrics at regular intervals. Logging every 100-500 steps provides sufficient granularity to detect issues without generating excessive data.

Checkpointing strategy depends on training duration and stability. For short fine-tuning runs of a few hours, saving checkpoints every epoch is sufficient. For longer training runs, save checkpoints every few thousand steps to protect against hardware failures and enable recovery from divergence. Always keep at least the three most recent checkpoints, as the most recent checkpoint may be corrupted or represent a point after training has diverged.

Beyond basic loss monitoring, tracking gradient norms provides early warning of training instability. Gradient norms should remain relatively stable throughout training, typically in the range of 0.1 to 10.0. Sudden spikes in gradient norm often precede loss divergence and indicate that gradient clipping or learning rate reduction may be necessary. Similarly, monitoring the ratio of update magnitude to parameter magnitude helps ensure that learning rates are appropriate.

\section{Memory Management}
\label{sec:memory_management}

Memory is often the primary constraint in transformer training. Rather than repeating the detailed memory analysis from Chapter~21, this section provides a quick-reference decision guide.

\subsection{Out-of-Memory Decision Checklist}

When encountering memory errors, apply these steps in order:

\begin{enumerate}
    \item \textbf{Reduce sequence length} (if task permits). Attention memory scales quadratically with sequence length---truncating from 512 to 256 tokens saves $\sim$4× attention memory. Many classification tasks work well at 128 tokens.
    \item \textbf{Enable mixed precision} (\texttt{torch.cuda.amp}). Halves activation and gradient memory with 2--3× speedup on tensor-core GPUs. Minimal code changes required. See Chapter~21 for implementation.
    \item \textbf{Enable gradient checkpointing.} Trades 20--30\% additional compute time for 40--50\% activation memory reduction. Apply via \texttt{torch.utils.checkpoint}.
    \item \textbf{Reduce batch size and use gradient accumulation.} Maintain effective batch size $B_{\text{eff}} = B_{\text{micro}} \times N_{\text{accum}}$ while fitting in memory. Linear memory savings.
    \item \textbf{Consider model parallelism.} When the model itself exceeds single-GPU memory, use pipeline parallelism (split by layers) or tensor parallelism (split within layers). Frameworks: DeepSpeed, Megatron-LM. See Chapter~22 for multi-GPU strategies.
\end{enumerate}

\subsection{Memory Estimation Rule of Thumb}

Total training memory (GB) $\approx$ (Parameters $\times$ 16 bytes) + (Batch $\times$ SeqLen $\times$ Hidden $\times$ Layers $\times$ 40 bytes). The first term covers parameters, gradients, and optimizer states; the second covers activations. For BERT-base (110M params, batch 32, seq 512): $\sim$8 GB.

\section{Debugging Transformers}
\label{sec:debugging_transformers}

\subsection{Systematic Debugging Workflow}

\textbf{Level 1: Data sanity checks}
\begin{enumerate}
    \item Visualize input samples
    \item Verify labels are correct
    \item Check for data leakage
    \item Validate preprocessing
\end{enumerate}

\textbf{Level 2: Model sanity checks}
\begin{enumerate}
    \item Overfit single batch (should reach near-zero loss)
    \item Check gradient flow (no dead neurons)
    \item Verify shapes at each layer
    \item Test with minimal model first
\end{enumerate}

\textbf{Level 3: Training dynamics}
\begin{enumerate}
    \item Monitor loss curves (training + validation)
    \item Track gradient norms
    \item Visualize attention weights
    \item Check learning rate schedule
\end{enumerate}

\begin{example}[Debugging Checklist]
\label{ex:debugging_checklist}
\textbf{Symptom:} Loss not decreasing

\textbf{Diagnose:}
\begin{itemize}
    \item Learning rate too low? Try 10× higher
    \item Frozen layers? Check requires\_grad
    \item Optimizer issue? Try SGD as baseline
    \item Bad initialization? Re-initialize
    \item Data issue? Manually inspect batches
\end{itemize}

\textbf{Symptom:} NaN loss

\textbf{Diagnose:}
\begin{itemize}
    \item Gradient explosion? Add clipping
    \item Numerical instability? Check mask values ($-\infty$ vs $-1e9$)
    \item Learning rate too high? Reduce 10×
    \item Mixed precision issue? Check loss scaling
\end{itemize}
\end{example}

\subsection{Gradient Analysis}

Monitor per-layer gradient norms throughout training (see Chapter~\ref{chap:pytorchimplementation} for PyTorch profiling tools). Healthy gradients have norms between $10^{-4}$ and $10^{1}$, are similar across layers, and are non-zero for all layers. Sudden spikes precede divergence; vanishing gradients indicate dead layers.

\subsection{Common Training Issues: Quick Reference}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Symptom} & \textbf{Likely Cause} & \textbf{Fix} \\
\midrule
Out of memory & Batch/sequence too large & See memory checklist (Section~\ref{sec:memory_management}) \\
Loss not decreasing & Learning rate too low & Increase LR 3--10$\times$; verify overfit on 1 batch \\
Loss diverges / NaN & LR too high or no clipping & Reduce LR; clip gradients to norm 1.0 \\
Slow training & Low GPU utilization & Increase batch size; add DataLoader workers \\
Train/val gap growing & Overfitting & More dropout; data augmentation; smaller model \\
\bottomrule
\end{tabular}
\end{center}

\section{Inference Optimization}
\label{sec:inference_optimization}

Inference costs often exceed training costs over a model's lifetime. This section provides decision tables for choosing optimization strategies; see Chapters~21 and~22 for detailed implementations.

\subsection{Optimizing for Latency}

\begin{center}
\begin{tabular}{llll}
\toprule
\textbf{Technique} & \textbf{Speedup} & \textbf{Accuracy Cost} & \textbf{Effort} \\
\midrule
FP16 inference & 1.5--2$\times$ & $<$0.1\% & Minimal \\
INT8 quantization (PTQ) & 2--4$\times$ & 0.5--2\% & Low (calibration) \\
INT8 quantization (QAT) & 2--4$\times$ & $<$0.5\% & Medium (retraining) \\
KV caching (autoregressive) & 5--10$\times$ & None & Low \\
TorchScript / torch.compile & 1.2--1.5$\times$ & None & Low \\
TensorRT compilation & 2--5$\times$ & $<$0.5\% & Medium \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Optimizing for Throughput}

\begin{itemize}
    \item \textbf{Dynamic batching:} Accumulate requests for 10--50\,ms, process together. Improves GPU utilization from 20--30\% to 70--90\%.
    \item \textbf{ONNX Runtime / TensorRT:} Graph-level optimizations provide 1.5--5$\times$ throughput gains via operator fusion and kernel selection.
    \item \textbf{Model distillation:} Train a smaller student (e.g., 6-layer DistilBERT retains 97\% of BERT-base accuracy at 1.6$\times$ speed). Combine with quantization for 5--10$\times$ cumulative speedup.
    \item \textbf{Continuous batching (vLLM):} For autoregressive generation, allow new requests to join in-flight batches as others complete. See Chapter~22.
\end{itemize}

\subsection{Hardware Selection Summary}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Scenario} & \textbf{Recommended Hardware} & \textbf{Rationale} \\
\midrule
$<$10 req/s, small model & CPU & Lower cost, sufficient throughput \\
10--100 req/s & T4 GPU + TensorRT & Good latency/cost balance \\
$>$100 req/s & A10/A100 GPU & Maximum throughput \\
Edge / mobile & INT8/INT4 on device & Memory and power constrained \\
\bottomrule
\end{tabular}
\end{center}

See Chapter~22 for detailed hardware analysis and cost breakdowns.

\section{Cost Optimization}
\label{sec:cost_optimization}

Understanding and optimizing costs is essential for sustainable deployment of transformer models. This section provides detailed analysis of training and inference costs with concrete examples.

\subsection{Training Cost Analysis}

Training costs depend on model size, dataset size, and hardware selection. For BERT-base pre-training on 16 GB of text, the original paper reports using 16 TPU chips for 4 days, equivalent to approximately 64 TPU days. At current Google Cloud pricing of roughly \$8 per TPU hour, this amounts to approximately \$12,000 for pre-training. Using equivalent GPU resources (64 V100 GPUs for 4 days) would cost approximately \$15,000 at on-demand rates.

Fine-tuning costs are much more modest. Training BERT-base on a typical classification task with 10,000 examples requires 2-4 hours on a single V100 GPU. At AWS on-demand pricing of approximately \$3 per hour for a p3.2xlarge instance, this amounts to \$6-12 per fine-tuning run. Even with extensive hyperparameter search involving 20-30 runs, total costs remain under \$300.

Cloud versus on-premise costs depend heavily on utilization. For continuous training workloads, purchasing GPUs becomes cost-effective after 12-18 months of use. A DGX A100 system costs approximately \$200,000 but provides compute equivalent to \$15,000 per month at cloud on-demand rates. For intermittent workloads or experimentation, cloud computing is more economical due to the flexibility to scale up and down.

Spot instances provide substantial savings for training workloads that can tolerate interruptions. AWS spot instances for p3.2xlarge typically cost 50-70\% less than on-demand rates, reducing fine-tuning costs to \$2-4 per run. Implementing checkpointing and automatic restart logic allows training to resume after spot instance interruptions, making this an attractive option for cost-conscious training.

\subsection{Training Time Estimation}

Estimating training time helps with planning and cost prediction. For fine-tuning, a useful rule of thumb is that BERT-base processes approximately 100-150 examples per second on a V100 GPU with batch size 32 and sequence length 128. For a dataset of 100,000 examples trained for 3 epochs, this translates to 2,000-3,000 seconds or roughly 1 hour of training time.

Pre-training time scales with dataset size and model size. BERT-base pre-training on 16 GB of text requires approximately 1 million training steps with batch size 256, processing roughly 4 billion tokens. At 1,000 tokens per second per V100 GPU, this requires 4 million GPU-seconds or approximately 1,100 GPU-hours. With 16 GPUs, this translates to roughly 70 hours or 3 days of training.

Larger models scale approximately linearly with parameter count for training time. GPT-2 medium with 345 million parameters takes roughly 3× longer to train than GPT-2 small with 117 million parameters, assuming the same dataset and batch size. However, larger models often benefit from larger batch sizes, which can partially offset the increased time per step.

\subsection{Inference Cost Analysis}

Inference costs depend on request volume, latency requirements, and model size. For a BERT-base classification service processing 1 million requests per day with average latency requirements of 100ms, a single V100 GPU can handle approximately 100 requests per second with dynamic batching, or 8.6 million requests per day. This suggests that a single GPU is sufficient, costing approximately \$200-300 per month for a cloud GPU instance.

For generation tasks, costs are higher due to the sequential nature of autoregressive decoding. GPT-2 medium generating 100 tokens per request can process approximately 10-20 requests per second per GPU, depending on batch size and sequence length. For 1 million requests per day, this requires 1-2 GPUs, costing \$400-600 per month. The cost per million tokens is approximately \$5-10 for self-hosted inference.

Comparing self-hosted to API costs reveals significant differences at scale. OpenAI's GPT-3.5 API costs approximately \$2 per million tokens for input and output combined. For applications processing 100 million tokens per month, this amounts to \$200 per month. Self-hosting a comparable model would require 4-8 GPUs costing \$1,600-3,200 per month, making the API more economical at this scale. However, at 1 billion tokens per month, self-hosting becomes competitive, and at 10 billion tokens per month, self-hosting is clearly more economical.

\subsection{Cost Optimization Strategies}

Several strategies can substantially reduce both training and inference costs. For training, using mixed precision reduces training time by 2-3×, directly reducing costs by the same factor. Gradient accumulation allows using smaller, cheaper GPU instances by simulating larger batch sizes. Spot instances reduce costs by 50-70\% for workloads that can tolerate interruptions.

For inference, quantization and distillation reduce both latency and cost. A distilled and quantized model may achieve 5-10× higher throughput than the original model, allowing a single GPU to handle the load that previously required 5-10 GPUs. This directly translates to 5-10× cost reduction. Dynamic batching improves GPU utilization from 20-30\% to 70-90\%, effectively tripling throughput without additional hardware.

Caching can dramatically reduce inference costs for applications with repeated queries. If 30\% of requests are duplicates or near-duplicates, caching responses eliminates 30\% of inference costs. Semantic caching using embedding similarity can extend this to near-duplicate queries, potentially caching 50-70\% of requests in some applications.

Autoscaling based on demand prevents paying for idle resources during low-traffic periods. For applications with diurnal traffic patterns, autoscaling can reduce costs by 40-60\% compared to provisioning for peak load. Kubernetes and cloud-native deployment platforms make autoscaling straightforward to implement.

\section{Production Deployment}
\label{sec:production_deployment}

Production deployment involves serving infrastructure, monitoring, and safe rollout practices. Chapter~22 covers serving frameworks (TorchServe, Triton, vLLM), deployment architectures (Ray Serve, Kubernetes), and KV cache management in detail. Here we summarize the key decision points.

\subsection{Deployment Checklist}

\textbf{Before deployment:}
\begin{enumerate}
    \item Optimize model: quantize (INT8/FP16), export to ONNX or TensorRT, enable KV caching for generation.
    \item Benchmark under realistic conditions (expected batch sizes, sequence lengths, peak load).
    \item Set up monitoring: latency percentiles (p50, p95, p99), throughput, error rate, GPU utilization.
    \item Plan scaling: autoscaling rules, load balancing, maximum replica count.
\end{enumerate}

\textbf{Safe rollout:} Use canary deployment (1--5\% traffic) or shadow mode (run new model alongside production without serving results) before full rollout. Monitor key metrics for several hours before increasing traffic. Maintain instant rollback capability.

\section{Practical Checklists}
\label{sec:practical_checklists}

These checklists provide systematic guidance for common transformer workflows, helping ensure that critical steps are not overlooked.

\subsection{Before Training Checklist}

Before beginning training, verify that you have made appropriate decisions about resources and configuration. Estimate memory requirements using the formulas provided earlier, ensuring that your chosen batch size and sequence length will fit in available GPU memory with some margin for safety. Select hardware appropriate for your model size and training duration, considering the trade-offs between cost and training time.

Choose batch size and sequence length based on your task requirements and memory constraints. Remember that sequence length has a quadratic effect on memory, so reducing it provides substantial savings if your task permits. Set up monitoring and logging infrastructure before training begins, as debugging issues after the fact is much more difficult than catching them in real-time.

Estimate training time and cost using the guidelines provided earlier. This helps with planning and ensures that you have allocated sufficient budget and time for the training run. For long training runs, verify that checkpointing is configured correctly and test recovery from checkpoints before committing to the full training run.

\subsection{During Training Checklist}

While training is in progress, monitor loss and metrics regularly to detect issues early. Training loss should decrease steadily, though not necessarily monotonically. Validation loss should track training loss initially, with some divergence expected as training progresses. If validation loss increases while training loss decreases, you may be overfitting.

Check GPU utilization to ensure that you are using resources efficiently. Utilization should be consistently above 80\% during training. Lower utilization suggests that batch size is too small, data loading is a bottleneck, or there are inefficiencies in the training loop. Monitor memory usage to ensure you are not close to OOM errors, which can cause training to fail unexpectedly.

Save checkpoints regularly according to your checkpointing strategy. Verify that checkpoints are being saved successfully and that you can load them for recovery. Validate periodically on a held-out set to track generalization performance. The frequency of validation depends on training duration, but every few hundred steps or every epoch is typical.

\subsection{Before Deployment Checklist}

Before deploying a model to production, optimize it for inference using the techniques described earlier. Apply quantization if accuracy permits, as the performance benefits are substantial. Consider distillation if you need further speedup and have time for the additional training. Export the model to an optimized format like ONNX or TensorRT if using those serving frameworks.

Benchmark latency and throughput under realistic conditions, including the batch sizes and sequence lengths you expect in production. Test with both average-case and worst-case inputs to understand performance variability. Estimate serving costs based on expected request volume and the hardware required to meet latency requirements.

Set up monitoring and alerting for the production deployment. Ensure that you can track request rate, latency, error rate, and resource utilization. Configure alerts for anomalies in these metrics. Plan your scaling strategy, including autoscaling rules if using dynamic scaling.

Test the deployment pipeline end-to-end, including model loading, preprocessing, inference, and postprocessing. Verify that error handling works correctly and that failures are logged appropriately. Conduct load testing to ensure the system can handle expected traffic with appropriate margins for spikes.

\section{Hyperparameter Tuning}
\label{sec:hyperparameter_tuning}

\subsection{Critical Hyperparameters (Ordered by Impact)}

\textbf{1. Learning Rate (highest impact)}
\begin{itemize}
    \item Typical range: $[10^{-5}, 10^{-3}]$
    \item BERT: $1-5 \times 10^{-5}$
    \item GPT: $2-6 \times 10^{-4}$
    \item Rule: Larger models need smaller LR
\end{itemize}

\textbf{2. Batch Size}
\begin{itemize}
    \item Trade-off: Speed vs generalization
    \item Typical: 32-512 for fine-tuning, 256-2048 for pre-training
    \item Scale LR linearly with batch size
\end{itemize}

\textbf{3. Warmup Steps}
\begin{itemize}
    \item Typical: 5-10\% of total training steps
    \item BERT: 10,000 steps
    \item GPT-3: 375M tokens (out of 300B)
\end{itemize}

\textbf{4. Weight Decay}
\begin{itemize}
    \item Typical: $0.01$ to $0.1$
    \item AdamW: Decouple from learning rate
\end{itemize}

\textbf{5. Dropout}
\begin{itemize}
    \item Standard: $0.1$
    \item Larger models: Lower dropout (0.05 or none)
    \item Apply uniformly (attention, FFN, embeddings)
\end{itemize}

\subsection{Tuning Strategy}

\textbf{Phase 1: Coarse search}
\begin{itemize}
    \item Grid/random search over wide ranges
    \item Short runs (10\% of full training)
    \item Focus on learning rate first
\end{itemize}

\textbf{Phase 2: Fine search}
\begin{itemize}
    \item Narrow ranges around best from Phase 1
    \item Longer runs (50\% of full training)
    \item Tune other hyperparameters
\end{itemize}

\textbf{Phase 3: Validation}
\begin{itemize}
    \item Full training with best settings
    \item Multiple seeds for robustness
    \item Final evaluation on test set
\end{itemize}

\begin{example}[Learning Rate Search]
\label{ex:lr_search}
\textbf{Task:} Fine-tune BERT on classification

\textbf{Coarse search:}
\begin{itemize}
    \item Try: $[10^{-5}, 3 \times 10^{-5}, 10^{-4}, 3 \times 10^{-4}]$
    \item Train 1 epoch each
    \item Best: $3 \times 10^{-5}$ (85.2\% dev accuracy)
\end{itemize}

\textbf{Fine search:}
\begin{itemize}
    \item Try: $[2 \times 10^{-5}, 3 \times 10^{-5}, 4 \times 10^{-5}]$
    \item Train 3 epochs each
    \item Best: $3 \times 10^{-5}$ (86.1\% dev accuracy)
\end{itemize}

\textbf{Final:}
\begin{itemize}
    \item Train with $\text{LR} = 3 \times 10^{-5}$, 5 epochs
    \item Test accuracy: 85.8\%
\end{itemize}
\end{example}

\section{Common Pitfalls and Solutions}
\label{sec:common_pitfalls}

\subsection{Architecture Pitfalls}

\textbf{Pitfall 1: Forgetting positional information}
\begin{itemize}
    \item Symptom: Model treats sequence as bag-of-words
    \item Solution: Verify position encoding is added
\end{itemize}

\textbf{Pitfall 2: Incorrect masking}
\begin{itemize}
    \item Symptom: Information leakage or blocked attention
    \item Solution: Visualize attention matrices, verify mask shape
\end{itemize}

\textbf{Pitfall 3: Not sharing embeddings}
\begin{itemize}
    \item Symptom: Twice as many parameters as expected
    \item Solution: Weight tying between input/output embeddings
\end{itemize}

\subsection{Training Pitfalls}

\textbf{Pitfall 4: Insufficient warmup}
\begin{itemize}
    \item Symptom: Training unstable early, doesn't recover
    \item Solution: Increase warmup to 10\% of training
\end{itemize}

\textbf{Pitfall 5: Wrong learning rate scale}
\begin{itemize}
    \item Symptom: Loss not decreasing or diverging
    \item Solution: Learning rate finder, try 10× up/down
\end{itemize}

\textbf{Pitfall 6: Overfitting small datasets}
\begin{itemize}
    \item Symptom: Large train/val gap
    \item Solution: More dropout, data augmentation, smaller model
\end{itemize}

\subsection{Deployment Pitfalls}

\textbf{Pitfall 7: Batch size 1 in production}
\begin{itemize}
    \item Symptom: Poor GPU utilization
    \item Solution: Dynamic batching, accumulate requests
\end{itemize}

\textbf{Pitfall 8: Not using mixed precision}
\begin{itemize}
    \item Symptom: Slow inference, high memory
    \item Solution: FP16 inference, quantization
\end{itemize}

\textbf{Pitfall 9: No KV caching for generation}
\begin{itemize}
    \item Symptom: Slow text generation (quadratic in length)
    \item Solution: Cache key/value tensors
\end{itemize}

\section{Case Study: BERT for Search Ranking}
\label{sec:case_study_search}

\subsection{Problem Setup}

\textbf{Task:} Rank search results by relevance

\textbf{Input:} Query + Document pairs

\textbf{Output:} Relevance score [0, 1]

\subsection{Architecture Decisions}

\textbf{Model:} BERT-base with regression head

\textbf{Input format:}
\begin{verbatim}
[CLS] query tokens [SEP] document tokens [SEP]
\end{verbatim}

\textbf{Output:} $\text{score} = \sigma(\mW \vh_{\text{[CLS]}} + b)$

\subsection{Training Strategy}

\textbf{Data:}
\begin{itemize}
    \item 10M query-document pairs
    \item Labels: Click-through rate (0-1)
    \item Hard negatives: Top results without clicks
\end{itemize}

\textbf{Loss:} Mean squared error on CTR prediction

\textbf{Optimization:}
\begin{itemize}
    \item Learning rate: $2 \times 10^{-5}$
    \item Batch size: 256
    \item Warmup: 10,000 steps
    \item Total: 100,000 steps
\end{itemize}

\subsection{Production Deployment}

\textbf{Optimizations:}
\begin{enumerate}
    \item Quantize to INT8 (3× speedup)
    \item Distill to 6-layer model (2× speedup)
    \item Deploy with ONNX Runtime
    \item Dynamic batching (avg batch size 32)
\end{enumerate}

\textbf{Results:}
\begin{itemize}
    \item Latency: 15ms p99 (vs 200ms baseline)
    \item Throughput: 2000 QPS per GPU
    \item Relevance: +8\% improvement over TF-IDF
\end{itemize}

\section{Case Study: GPT for Code Generation}
\label{sec:case_study_code}

\subsection{Problem Setup}

\textbf{Task:} Generate Python code from natural language

\textbf{Example:}
\begin{verbatim}
Input: "Function to reverse a string"
Output: 
def reverse_string(s):
    return s[::-1]
\end{verbatim}

\subsection{Model and Data}

\textbf{Model:} GPT-2 medium (345M params)

\textbf{Data:}
\begin{itemize}
    \item GitHub public repositories (Python)
    \item Filtered: Only files with docstrings
    \item Format: Docstring $\to$ Implementation
    \item Total: 50GB, 10B tokens
\end{itemize}

\subsection{Training}

\textbf{Pre-training:} Start from GPT-2 checkpoint

\textbf{Fine-tuning:}
\begin{itemize}
    \item 100,000 steps
    \item Learning rate: $5 \times 10^{-5}$
    \item Context: 1024 tokens
    \item Batch: 128 sequences
\end{itemize}

\subsection{Evaluation}

\textbf{Metrics:}
\begin{itemize}
    \item Pass@k: \% correct in top-k samples
    \item BLEU: Token overlap with reference
    \item Human evaluation: Correctness + readability
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item Pass@1: 42\%
    \item Pass@10: 71\%
    \item Human preferred over baseline: 78\%
\end{itemize}

\section{Future Directions}
\label{sec:future_directions}

\subsection{Architectural Innovations}

\textbf{1. Efficient attention}
\begin{itemize}
    \item Linear complexity methods
    \item State space models (S4, Mamba)
    \item Hybrid CNN-attention architectures
\end{itemize}

\textbf{2. Multimodal integration}
\begin{itemize}
    \item Unified text-image-audio models
    \item Better cross-modal alignment
    \item Efficient fusion strategies
\end{itemize}

\textbf{3. Long context}
\begin{itemize}
    \item Million-token contexts
    \item Hierarchical memory
    \item Retrieval-augmented transformers
\end{itemize}

\subsection{Training Innovations}

\textbf{1. Sample efficiency}
\begin{itemize}
    \item Better pre-training objectives
    \item Curriculum learning
    \item Few-shot and zero-shot learning
\end{itemize}

\textbf{2. Scaling}
\begin{itemize}
    \item Mixture of experts
    \item Conditional computation
    \item Efficient parallelism strategies
\end{itemize}

\textbf{3. Alignment}
\begin{itemize}
    \item Better RLHF techniques
    \item Constitutional AI
    \item Value alignment
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

\subsection{Key Takeaways}

\textbf{Architecture:}
\begin{itemize}
    \item Attention is powerful and flexible
    \item Position encodings crucial for sequences
    \item Residuals + normalization enable depth
\end{itemize}

\textbf{Training:}
\begin{itemize}
    \item Pre-training + fine-tuning is dominant paradigm
    \item Warmup is critical for stability
    \item Scale requires careful optimization
\end{itemize}

\textbf{Deployment:}
\begin{itemize}
    \item Quantization and distillation for efficiency
    \item Batching crucial for throughput
    \item Monitor performance in production
\end{itemize}

\subsection{Final Advice}

\textbf{For practitioners:}
\begin{enumerate}
    \item Start simple: Use pre-trained models
    \item Debug systematically: Data, model, training
    \item Optimize iteratively: Accuracy first, then speed
    \item Monitor continuously: Metrics, errors, drift
\end{enumerate}

\textbf{For researchers:}
\begin{enumerate}
    \item Understand fundamentals deeply
    \item Question assumptions: Why does this work?
    \item Experiment rigorously: Ablations, multiple seeds
    \item Share knowledge: Open source, papers, blogs
\end{enumerate}

This concludes our comprehensive journey through deep learning and transformers. You now have the mathematical foundations, practical implementations, and real-world insights to build state-of-the-art transformer models!

\section{Exercises}

\begin{exercise}
Reproduce DistilBERT:
\begin{enumerate}
    \item Train 6-layer student on BERT-base teacher
    \item Use distillation + MLM + cosine losses
    \item Evaluate on GLUE
    \item Measure compression ratio and speedup
\end{enumerate}
\end{exercise}

\begin{exercise}
Debug broken transformer (provided):
\begin{enumerate}
    \item Model trains but poor performance
    \item Find 3 subtle bugs (architecture, training, data)
    \item Fix and verify improvements
\end{enumerate}
\end{exercise}

\begin{exercise}
Deploy BERT for production:
\begin{enumerate}
    \item Fine-tune on classification task
    \item Quantize to INT8
    \item Export to ONNX
    \item Create REST API with FastAPI
    \item Load test and optimize
\end{enumerate}
\end{exercise}



\section{Solutions}

Full solutions for all exercises are available at \url{https://deeplearning.hofkensvermeulen.be}.

\begin{solution}
\textbf{Exercise 1: Reproduce DistilBERT}

\textbf{Training Configuration:}
\begin{itemize}
    \item Student: 6 layers, 768 hidden, 12 heads (66M params)
    \item Teacher: BERT-base (110M params)
    \item Loss: $\mathcal{L} = \alpha \mathcal{L}_{\text{distill}} + \beta \mathcal{L}_{\text{MLM}} + \gamma \mathcal{L}_{\text{cosine}}$
    \item Weights: $\alpha=0.5$, $\beta=0.25$, $\gamma=0.25$
\end{itemize}

\textbf{Results on GLUE:}

\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Model} & \textbf{Params} & \textbf{GLUE Score} & \textbf{Speed} \\
\hline
BERT-base & 110M & 84.5 & 1.0x \\
DistilBERT & 66M & 82.8 & 1.6x \\
\hline
\end{tabular}

\textbf{Compression Analysis:}
\begin{itemize}
    \item \textbf{Parameters:} 40\% reduction (110M $\to$ 66M)
    \item \textbf{Inference speed:} 60\% faster
    \item \textbf{Accuracy:} 98\% of teacher performance (2\% drop)
    \item \textbf{Memory:} 40\% less
\end{itemize}

\textbf{Key Insights:}
\begin{enumerate}
    \item Distillation preserves 98\% of teacher's knowledge
    \item Triple loss (distill + MLM + cosine) crucial for quality
    \item 6 layers sufficient for most understanding tasks
    \item Excellent trade-off for production deployment
\end{enumerate}

\textbf{When to use DistilBERT:}
\begin{itemize}
    \item Latency-sensitive applications (<50ms)
    \item Resource-constrained environments
    \item Mobile/edge deployment
    \item High-throughput serving
\end{itemize}
\end{solution}



\begin{solution}
\textbf{Exercise 2: Debug Broken Transformer}

\textbf{Common Bugs Found:}

\textbf{Bug 1 (Architecture):} Missing dropout in attention

\textbf{Symptom:} Model overfits quickly, poor generalization

\textbf{Fix:}
\begin{lstlisting}[language=Python]
# Before (broken)
attn_weights = F.softmax(scores, dim=-1)
output = torch.matmul(attn_weights, V)

# After (fixed)
attn_weights = F.softmax(scores, dim=-1)
attn_weights = F.dropout(attn_weights, p=0.1, training=self.training)
output = torch.matmul(attn_weights, V)
\end{lstlisting}

\textbf{Impact:} Validation accuracy improves from 72\% to 84\%

\textbf{Bug 2 (Training):} Learning rate too high

\textbf{Symptom:} Loss oscillates, doesn't converge

\textbf{Fix:}
\begin{lstlisting}[language=Python]
# Before (broken)
optimizer = AdamW(model.parameters(), lr=1e-3)  # Too high!

# After (fixed)
optimizer = AdamW(model.parameters(), lr=5e-5)  # Appropriate for BERT
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=1000)
\end{lstlisting}

\textbf{Impact:} Loss converges smoothly, final accuracy 84\% $\to$ 87\%

\textbf{Bug 3 (Data):} Incorrect padding token handling

\textbf{Symptom:} Model attends to padding, poor performance on variable-length sequences

\textbf{Fix:}
\begin{lstlisting}[language=Python]
# Before (broken)
attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
attn_weights = F.softmax(attn_scores, dim=-1)

# After (fixed)
attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
# Mask padding tokens
attn_scores = attn_scores.masked_fill(attention_mask == 0, float('-inf'))
attn_weights = F.softmax(attn_scores, dim=-1)
\end{lstlisting}

\textbf{Impact:} Accuracy on variable-length sequences improves from 79\% to 87\%

\textbf{Final Results:}

\begin{tabular}{|l|c|}
\hline
\textbf{Version} & \textbf{Accuracy} \\
\hline
Original (broken) & 72\% \\
After Bug 1 fix & 84\% \\
After Bug 2 fix & 87\% \\
After Bug 3 fix & 87\% (robust) \\
\hline
\end{tabular}

\textbf{Debugging Lessons:}
\begin{enumerate}
    \item Always include dropout in attention
    \item Use appropriate learning rates (5e-5 for BERT-scale)
    \item Properly mask padding tokens
    \item Test on variable-length sequences
    \item Monitor both training and validation metrics
\end{enumerate}
\end{solution}



\begin{solution}
\textbf{Exercise 3: Deploy BERT for Production}

\textbf{Deployment Pipeline:}

\textbf{Step 1: Fine-tune on Classification}
\begin{itemize}
    \item Task: Sentiment analysis (binary classification)
    \item Training: 10k examples, 3 epochs
    \item Validation accuracy: 92.3\%
\end{itemize}

\textbf{Step 2: Quantize to INT8}
\begin{itemize}
    \item Method: Dynamic quantization
    \item Model size: 438 MB $\to$ 110 MB (75\% reduction)
    \item Accuracy: 92.3\% $\to$ 91.8\% (0.5\% drop)
    \item Inference speed: 2.4x faster
\end{itemize}

\textbf{Step 3: Export to ONNX}
\begin{lstlisting}[language=Python]
import torch.onnx

# Export model
dummy_input = torch.randint(0, 1000, (1, 128))
torch.onnx.export(
    model,
    dummy_input,
    "bert_sentiment.onnx",
    input_names=['input_ids'],
    output_names=['logits'],
    dynamic_axes={'input_ids': {0: 'batch', 1: 'sequence'}}
)

# Verify with ONNX Runtime
import onnxruntime as ort
session = ort.InferenceSession("bert_sentiment.onnx")
# 1.3x additional speedup
\end{lstlisting}

\textbf{Step 4: Create REST API}
\begin{lstlisting}[language=Python]
from fastapi import FastAPI
from pydantic import BaseModel
import onnxruntime as ort

app = FastAPI()
session = ort.InferenceSession("bert_sentiment.onnx")

class TextInput(BaseModel):
    text: str

@app.post("/predict")
async def predict(input: TextInput):
    # Tokenize
    tokens = tokenizer.encode(input.text, max_length=128, truncation=True)
    
    # Inference
    outputs = session.run(None, {"input_ids": [tokens]})
    logits = outputs[0][0]
    
    # Predict
    prediction = "positive" if logits[1] > logits[0] else "negative"
    confidence = float(max(logits))
    
    return {"prediction": prediction, "confidence": confidence}
\end{lstlisting}

\textbf{Step 5: Load Test and Optimize}

\textbf{Initial Performance:}
\begin{itemize}
    \item Latency: 45ms (p50), 78ms (p99)
    \item Throughput: 22 requests/second
\end{itemize}

\textbf{Optimizations Applied:}
\begin{enumerate}
    \item Dynamic batching (batch size 8): 3.2x throughput
    \item Connection pooling: 1.2x throughput
    \item Async processing: 1.5x throughput
\end{enumerate}

\textbf{Final Performance:}
\begin{itemize}
    \item Latency: 38ms (p50), 62ms (p99)
    \item Throughput: 127 requests/second
    \item 5.8x improvement over baseline
\end{itemize}

\textbf{Production Checklist:}
\begin{itemize}
    \item[$\checkmark$] Model quantized and optimized
    \item[$\checkmark$] ONNX export for cross-platform compatibility
    \item[$\checkmark$] REST API with proper error handling
    \item[$\checkmark$] Load tested and optimized
    \item[$\checkmark$] Monitoring and logging configured
    \item[$\checkmark$] Auto-scaling based on load
    \item[$\checkmark$] Health checks and graceful shutdown
\end{itemize}

\textbf{Deployment Architecture:}
\begin{verbatim}
Load Balancer
    |
    +-- API Server 1 (ONNX Runtime)
    +-- API Server 2 (ONNX Runtime)
    +-- API Server 3 (ONNX Runtime)
    |
Monitoring (Prometheus + Grafana)
\end{verbatim}

\textbf{Key Metrics to Monitor:}
\begin{itemize}
    \item Request latency (p50, p95, p99)
    \item Throughput (requests/second)
    \item Error rate
    \item CPU/GPU utilization
    \item Memory usage
    \item Model accuracy (via A/B testing)
\end{itemize}

\textbf{Cost Analysis:}
\begin{itemize}
    \item Hardware: 3x T4 GPUs (\$0.35/hour each)
    \item Total: \$1.05/hour = \$756/month
    \item Capacity: 127 req/s × 3 = 381 req/s
    \item Cost per 1M requests: \$0.55
\end{itemize}

\textbf{Production Best Practices:}
\begin{enumerate}
    \item Always quantize for inference (2-4x speedup)
    \item Use ONNX for deployment (cross-platform, optimized)
    \item Implement dynamic batching (3-5x throughput)
    \item Monitor latency percentiles (not just average)
    \item Set up auto-scaling for variable load
    \item Use health checks and graceful shutdown
    \item Implement request timeouts and retries
    \item Log predictions for model monitoring
\end{enumerate}

\textbf{Success Criteria Met:}
\begin{itemize}
    \item[$\checkmark$] <50ms p99 latency
    \item[$\checkmark$] >100 requests/second throughput
    \item[$\checkmark$] <1\% accuracy degradation
    \item[$\checkmark$] 75\% model size reduction
    \item[$\checkmark$] Production-ready API
    \item[$\checkmark$] Comprehensive monitoring
\end{itemize}
\end{solution}

