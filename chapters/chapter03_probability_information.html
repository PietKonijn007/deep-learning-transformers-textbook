<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Probability and Information Theory - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../../deeptech.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Probability and Information Theory</h1>

<h2>Chapter Overview</h2>

<p>Deep learning is fundamentally a probabilistic framework. Neural networks learn probability distributions over data, make predictions with uncertainty, and are trained using probabilistic objectives. This chapter develops the probability theory and information theory necessary to understand these probabilistic aspects of deep learning.</p>

<p>We cover probability distributions, conditional probability, expectation, and variance‚Äîthe building blocks for understanding neural network outputs as probabilistic models. We then introduce information theory concepts like entropy, cross-entropy, and KL divergence, which form the basis for loss functions used in training.</p>

<h3>Learning Objectives</h3>

<p>After completing this chapter, you will be able to:</p>

<ol>
    <li>Work with probability distributions and compute expectations
    <li>Apply Bayes' theorem to understand conditional probabilities
    <li>Understand entropy as a measure of uncertainty
    <li>Derive and apply cross-entropy loss for classification
    <li>Use KL divergence to measure distribution differences
    <li>Interpret neural network outputs as probability distributions
</ol>

<h2>Probability Fundamentals</h2>

<h3>Random Variables and Distributions</h3>

<div class="definition"><strong>Definition:</strong> 
A <strong>random variable</strong> $X$ is a function that maps outcomes from a sample space to real numbers. We distinguish between:
<ul>
    <li><strong>Discrete random variables</strong>: Take countable values (e.g., class labels)
    <li><strong>Continuous random variables</strong>: Take values in continuous ranges
</ul>
</div>

<div class="definition"><strong>Definition:</strong> <div class="architecture-diagram">
<h3>Cross-Entropy Loss in Classification</h3>
<pre class="mermaid">
graph LR
    X["Input x\n in R^d"] -->|"Model f(x;theta)"| LOGITS["Logits z\n in R^K\n STORED for backprop"]
    LOGITS -->|"softmax"| P["Predicted P(y|x)\n p_k = exp(z_k)/sum\n in R^K\n STORED: p for gradient"]
    Y["True label y\n one-hot in R^K"] --> CE["Cross-Entropy\n H(y,p) = -sum(y_k log p_k)\n = -log p_y\n scalar"]
    P --> CE
    CE -.->|"dL/dz_k = p_k - y_k\n simple gradient"| LOGITS

    style X fill:#e8f5e9,stroke:#4caf50,color:#000
    style LOGITS fill:#fff3e0,stroke:#ff9800,color:#000
    style P fill:#e3f2fd,stroke:#2196f3,color:#000
    style CE fill:#fce4ec,stroke:#e91e63,color:#000
</pre>
<p class="diagram-caption">Cross-Entropy Loss in Classification</p>
</div>

<p>For discrete random variable $X$, the <strong>probability mass function</strong> is:
<div class="equation">
$$
P(X = x) = p(x)
$$
</div>
satisfying: (1) $0 \leq p(x) \leq 1$ for all $x$, and (2) $\sum_x p(x) = 1$
</div>

<div class="example"><strong>Example:</strong> 
In image classification with 10 classes (digits 0-9), a neural network outputs a probability distribution using softmax:
<div class="equation">
$$
P(Y = k | \vx) = \frac{\exp(z_k)}{\sum_{j=1}^{10} \exp(z_j)}
$$
</div>

<p>For logits $\vz = [2.1, 0.5, -1.2, 3.4, 0.8, -0.5, 1.1, -2.0, 0.3, 1.8]$, the model predicts class 3 with highest probability $\approx 68.9\%$.
</div>

<h3>Conditional Probability and Bayes' Theorem</h3>

<div class="definition"><strong>Definition:</strong> 
The probability of event $A$ given event $B$:
<div class="equation">
$$
P(A|B) = \frac{P(A \cap B)}{P(B)} \quad \text{if } P(B) > 0
$$
</div>
</div>

<div class="theorem"><strong>Theorem:</strong> 
For events $A$ and $B$ with $P(B) > 0$:
<div class="equation">
$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
$$
</div>
where $P(A|B)$ is the posterior, $P(B|A)$ is the likelihood, $P(A)$ is the prior, and $P(B)$ is the evidence.
</div>

<h2>Information Theory</h2>

<h3>Entropy</h3>

<div class="definition"><strong>Definition:</strong> 
For discrete random variable $X$ with PMF $p(x)$:
<div class="equation">
$$
H(X) = -\sum_x p(x) \ln p(x) = \mathbb{E}[-\ln P(X)]
$$
</div>
</div>

<p>Entropy measures average uncertainty. Higher entropy means more uncertainty.</p>

<div class="example"><strong>Example:</strong> 
<strong>Fair coin:</strong> $P(\text{heads}) = P(\text{tails}) = 0.5$
<div class="equation">
$$
H = -[0.5 \log_2(0.5) + 0.5 \log_2(0.5)] = 1 \text{ bit (maximum)}
$$
</div>

<p><strong>Biased coin:</strong> $P(\text{heads}) = 0.9$, $P(\text{tails}) = 0.1$
<div class="equation">
$$
H \approx 0.469 \text{ bits (lower, more predictable)}
$$
</div>
</div>

<h3>Cross-Entropy</h3>

<div class="definition"><strong>Definition:</strong> 
For true distribution $p$ and predicted distribution $q$:
<div class="equation">
$$
H(p, q) = -\sum_x p(x) \log q(x) = \mathbb{E}_{x \sim p}[-\log q(x)]
$$
</div>
</div>

<div class="theorem"><strong>Theorem:</strong> 
For true label $y$ and predicted probabilities $\hat{\mathbf{p}}$:
<div class="equation">
$$
L = -\log \hat{p}_y
$$
</div>
</div>

<div class="example"><strong>Example:</strong> 
For 3-class classification with true label $y=2$:
<ul>
    <li>Predicted: $\hat{\mathbf{p}} = [0.2, 0.6, 0.2]$ $\Rightarrow$ $L = -\log(0.6) \approx 0.511$
    <li>More confident: $\hat{\mathbf{p}} = [0.1, 0.8, 0.1]$ $\Rightarrow$ $L = -\log(0.8) \approx 0.223$ (better)
    <li>Wrong prediction: $\hat{\mathbf{p}} = [0.7, 0.2, 0.1]$ $\Rightarrow$ $L = -\log(0.2) \approx 1.609$ (bad)
</ul>
</div>

<div class="implementation">
PyTorch cross-entropy loss:
<pre><code>import torch
import torch.nn as nn

# Logits: shape (batch_size, num_classes)
logits = torch.tensor([[2.0, 1.0, 0.1],
                       [0.5, 2.5, 1.0]])
labels = torch.tensor([0, 1])

# CrossEntropyLoss applies softmax internally
criterion = nn.CrossEntropyLoss()
loss = criterion(logits, labels)
print(f"Loss: {loss.item():.4f}")
</code></pre>
</div>

<h3>Kullback-Leibler Divergence</h3>

<div class="definition"><strong>Definition:</strong> 
The KL divergence from distribution $q$ to $p$:
<div class="equation">
$$
D_{\text{KL}}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = H(p, q) - H(p)
$$
</div>
</div>

<p>Properties: (1) $D_{\text{KL}}(p \| q) \geq 0$ with equality iff $p = q$, (2) Not symmetric: $D_{\text{KL}}(p \| q) \neq D_{\text{KL}}(q \| p)$</p>

<div class="keypoint">
Minimizing KL divergence is equivalent to minimizing cross-entropy when $p$ is fixed. Training neural networks with cross-entropy loss is maximum likelihood estimation.
</div>

<h2>Practical Considerations for Cross-Entropy and Softmax</h2>

<div class="keypoint">
The cross-entropy loss computation becomes a significant bottleneck for large vocabularies. The logits tensor has shape $B \times n \times V$ (batch size $\times$ sequence length $\times$ vocabulary size), requiring $4BnV$ bytes in FP32. For BERT-base with $B=32$, $n=512$, $V=30{,}000$, logits alone consume $\sim$1.8~GB. The softmax operation over large vocabularies is memory-bandwidth-bound rather than compute-bound on modern GPUs, making vocabulary size one of the most direct levers for controlling training speed.
</div>

<p>Key optimizations for large vocabularies include <strong>sampled softmax</strong> (approximating the full softmax using a subset of $K$ negative samples), <strong>adaptive softmax</strong> (exploiting the Zipfian distribution of natural language with hierarchical prediction), and <strong>subword tokenization</strong> (reducing vocabulary size through BPE or WordPiece). Modern models use vocabularies of 30,000--50,000 subword tokens, balancing per-token cost against sequence length. For a detailed computational analysis, see Chapter~12.</p>

<h2>KL Divergence in Practice</h2>

<p>KL divergence appears throughout modern deep learning as a measure of distribution similarity. Understanding its computational properties and applications is essential for implementing techniques like variational autoencoders, knowledge distillation, and reinforcement learning from human feedback.</p>

<h3>Applications in Modern Deep Learning</h3>

<p><strong>Variational Autoencoders (VAEs)</strong> use KL divergence as a regularization term to ensure that the learned latent distribution $q(z|x)$ remains close to a prior distribution $p(z)$, typically a standard Gaussian. The VAE loss function combines reconstruction loss with a KL divergence term:
<div class="equation">
$$
\mathcal{L}_{\text{VAE}} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{\text{KL}}(q(z|x) \| p(z))
$$
</div>

<p>For a Gaussian encoder with mean $\mu$ and variance $\sigma^2$, the KL divergence to a standard Gaussian has a closed form:
<div class="equation">
$$
D_{\text{KL}}(q \| p) = \frac{1}{2} \sum_{i=1}^{d} (\mu_i^2 + \sigma_i^2 - \log \sigma_i^2 - 1)
$$
</div>

<p>This closed form makes VAEs computationally efficient, as the KL term requires only $O(d)$ operations for a $d$-dimensional latent space, typically much smaller than the reconstruction loss computation.</p>

<p><strong>Knowledge Distillation</strong> transfers knowledge from a large teacher model to a smaller student model by minimizing the KL divergence between their output distributions. The student is trained to match not just the hard labels but the full probability distribution produced by the teacher:
<div class="equation">
$$
\mathcal{L}_{\text{distill}} = \alpha \mathcal{L}_{\text{CE}}(y, \hat{y}_{\text{student}}) + (1-\alpha) T^2 D_{\text{KL}}(\hat{y}_{\text{teacher}} \| \hat{y}_{\text{student}})
$$
</div>

<p>where $T$ is a temperature parameter that softens the distributions. The KL divergence term encourages the student to learn the relative confidences between classes that the teacher has learned, not just the most likely class. This is particularly valuable when the teacher assigns non-negligible probability to multiple classes, indicating genuine ambiguity or similarity between categories.</p>

<p>The computational cost of knowledge distillation is dominated by running both teacher and student models, with the KL divergence computation itself being relatively cheap at $O(BnV)$ for batch size $B$, sequence length $n$, and vocabulary size $V$.</p>

<p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong> uses KL divergence to constrain the policy learned through reinforcement learning to remain close to the original supervised fine-tuned model. This prevents the model from exploiting the reward model by generating adversarial outputs that score highly but are nonsensical. The RLHF objective includes a KL penalty term:
<div class="equation">
$$
\mathcal{L}_{\text{RLHF}} = \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta}[r(x, y)] - \beta D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})
$$
</div>

<p>where $\pi_\theta$ is the policy being optimized, $\pi_{\text{ref}}$ is the reference model, and $\beta$ controls the strength of the KL constraint. Computing this KL divergence requires running both the policy and reference models on the same inputs and computing the divergence over the vocabulary at each token position.</p>

<h3>Numerical Stability Considerations</h3>

<p>Computing KL divergence naively can lead to numerical instability due to the logarithm of very small probabilities. When $q(x)$ is close to zero, $\log q(x)$ approaches negative infinity, and the product $p(x) \log q(x)$ can produce NaN values or catastrophic cancellation. Similarly, when computing $\log(p(x)/q(x))$, direct division can lose precision for very small probabilities.</p>

<p>The numerically stable approach computes KL divergence in log-space using the log-sum-exp trick. Instead of computing probabilities via softmax and then taking logarithms, we work directly with log-probabilities:
<div class="equation">
$$
D_{\text{KL}}(p \| q) = \sum_x p(x) (\log p(x) - \log q(x)) = \sum_x \exp(\log p(x)) \cdot (\log p(x) - \log q(x))
$$
</div>

<p>This formulation avoids computing very small probabilities explicitly. Modern deep learning frameworks like PyTorch provide <code>F.kl\_div</code> that operates on log-probabilities directly, ensuring numerical stability even when probabilities span many orders of magnitude.</p>

<p>Another source of instability arises when $p(x) > 0$ but $q(x) = 0$, which makes the KL divergence infinite. In practice, this occurs when the model assigns zero probability to an event that actually occurs in the data. To prevent this, implementations typically add a small epsilon ($\epsilon \approx 10^{-8}$) to probabilities before computing logarithms, or use label smoothing to ensure that the target distribution $p$ never assigns exactly zero probability to any class. Label smoothing replaces hard targets with a mixture of the true label and a uniform distribution:
<div class="equation">
$$
p_{\text{smooth}}(x) = (1 - \epsilon) p_{\text{true}}(x) + \epsilon / V
$$
</div>

<p>where $\epsilon \approx 0.1$ is typical. This not only improves numerical stability but also acts as a regularizer that prevents overconfident predictions and often improves generalization.</p>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> A neural network outputs $\hat{\mathbf{p}} = [0.15, 0.60, 0.20, 0.05]$ for 4 classes. Compute: (1) entropy $H(\hat{\mathbf{p}})$, (2) cross-entropy loss if true label is class 2, (3) optimal output distribution.
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Show that $H(p, q) = H(p) + D_{\text{KL}}(p \| q)$, proving cross-entropy minimization equals KL divergence minimization when $p$ is fixed.
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> For binary classifier with $\hat{p} = 0.8$ and true label class 1: (1) Compute binary cross-entropy loss, (2) Find $\frac{\partial L}{\partial \hat{p}}$, (3) Compare loss for $\hat{p} \in \{0.99, 0.2\}$.
</div>

<div class="exercise" id="exercise-4"><strong>Exercise 4:</strong> Calculate the memory requirements for storing logits in a GPT-2 model with vocabulary size $V = 50{,}257$, batch size $B = 16$, and sequence length $n = 1024$. How much memory is saved by using FP16 instead of FP32? If you have an NVIDIA A100 with 40 GB of memory, what is the maximum batch size you can use if logits consume at most 25\% of available memory?
</div>

<div class="exercise" id="exercise-5"><strong>Exercise 5:</strong> For sampled softmax with $K = 5{,}000$ negative samples and vocabulary size $V = 100{,}000$: (1) Calculate the speedup factor for the forward pass compared to full softmax, (2) Compute the memory reduction for a batch of 32 sequences with 512 tokens each, (3) Discuss why sampled softmax introduces bias in gradient estimates.
</div>

<div class="exercise" id="exercise-6"><strong>Exercise 6:</strong> An NVIDIA A100 GPU has memory bandwidth of 1.5 TB/s and FP16 compute throughput of 312 TFLOPS. For softmax over a vocabulary of $V = 30{,}000$ tokens: (1) Calculate the time to read and write the logits and probabilities (400 KB total), (2) Calculate the time to compute 30,000 exponentials and divisions at peak throughput, (3) Determine whether the operation is compute-bound or bandwidth-bound and by what factor.
</div>

<div class="exercise" id="exercise-7"><strong>Exercise 7:</strong> In knowledge distillation, the KL divergence loss is scaled by $T^2$ where $T$ is the temperature parameter. Explain why this scaling is necessary by: (1) Showing how temperature affects the magnitude of gradients, (2) Deriving the gradient of $D_{\text{KL}}(\text{softmax}(\mathbf{z}/T) \| \text{softmax}(\mathbf{z}'/T))$ with respect to $\mathbf{z}'$, (3) Demonstrating that without $T^2$ scaling, the distillation loss would vanish as $T \to \infty$.
</div>

<h2>Solutions</h2>

<p>Full solutions for all exercises are available at \url{https://deeplearning.hofkensvermeulen.be}.</p>

<div class="solution"><strong>Solution:</strong> For neural network output $\hat{\mathbf{p}} = [0.15, 0.60, 0.20, 0.05]$:

<p><strong>(1) Entropy:</strong>
<div class="equation">
$$\begin{align}
H(\hat{\mathbf{p}}) &= -\sum_{i=1}^4 \hat{p}_i \log_2 \hat{p}_i \\
&= -(0.15 \log_2 0.15 + 0.60 \log_2 0.60 + 0.20 \log_2 0.20 + 0.05 \log_2 0.05) \\
&= -(0.15(-2.737) + 0.60(-0.737) + 0.20(-2.322) + 0.05(-4.322)) \\
&= -(-0.411 - 0.442 - 0.464 - 0.216) \\
&= 1.533 \text{ bits}
\end{align}$$
</div>

<p><strong>(2) Cross-entropy loss for true label class 2:</strong>
<div class="equation">
$$
L = -\log \hat{p}_2 = -\log 0.60 \approx 0.511 \text{ nats} \quad \text{or} \quad -\log_2 0.60 \approx 0.737 \text{ bits}
$$
</div>

<p><strong>(3) Optimal output distribution:</strong>
The optimal distribution assigns probability 1 to the correct class:
<div class="equation">
$$
\mathbf{p}^* = [0, 1, 0, 0]
$$
</div>

<p>This gives entropy $H(\mathbf{p}^*) = 0$ (no uncertainty) and cross-entropy loss $L = -\log 1 = 0$ (perfect prediction).
</div>

<div class="solution"><strong>Solution:</strong> <strong>Proof that $H(p, q) = H(p) + D_{\text{KL}}(p \| q)$:</strong>

<p>Starting with the definition of cross-entropy:
<div class="equation">
$$\begin{align}
H(p, q) &= -\sum_x p(x) \log q(x) \\
&= -\sum_x p(x) \log q(x) + \sum_x p(x) \log p(x) - \sum_x p(x) \log p(x) \\
&= -\sum_x p(x) \log p(x) + \sum_x p(x) \log \frac{p(x)}{q(x)} \\
&= H(p) + D_{\text{KL}}(p \| q)
\end{align}$$
</div>

<p>Since $H(p)$ is constant with respect to $q$, minimizing $H(p, q)$ is equivalent to minimizing $D_{\text{KL}}(p \| q)$. This shows that training with cross-entropy loss is equivalent to minimizing the KL divergence between the true distribution and the predicted distribution.
</div>

<div class="solution"><strong>Solution:</strong> For binary classifier with $\hat{p} = 0.8$ and true label class 1:

<p><strong>(1) Binary cross-entropy loss:</strong>
<div class="equation">
$$
L = -[y \log \hat{p} + (1-y) \log(1-\hat{p})] = -[1 \cdot \log 0.8 + 0 \cdot \log 0.2] = -\log 0.8 \approx 0.223
$$
</div>

<p><strong>(2) Gradient:</strong>
<div class="equation">
$$\begin{align}
\frac{\partial L}{\partial \hat{p}} &= \frac{\partial}{\partial \hat{p}}[-y \log \hat{p} - (1-y) \log(1-\hat{p})] \\
&= -\frac{y}{\hat{p}} + \frac{1-y}{1-\hat{p}} \\
&= -\frac{1}{0.8} + \frac{0}{0.2} = -1.25
\end{align}$$
</div>

<p><strong>(3) Loss comparison:</strong>
<ul>
    <li>$\hat{p} = 0.99$: $L = -\log 0.99 \approx 0.010$ (very confident, correct)
    <li>$\hat{p} = 0.2$: $L = -\log 0.2 \approx 1.609$ (low confidence, incorrect)
</ul>

<p>The loss heavily penalizes confident wrong predictions, encouraging the model to be calibrated.
</div>

<div class="solution"><strong>Solution:</strong> For GPT-2 with $V = 50{,}257$, $B = 16$, $n = 1024$:

<p><strong>Memory for logits:</strong>
<div class="equation">
$$
B \times n \times V \times 4 \text{ bytes} = 16 \times 1024 \times 50{,}257 \times 4 = 3{,}280{,}838{,}144 \text{ bytes} \approx 3.06 \text{ GB}
$$
</div>

<p><strong>Memory with FP16:</strong>
<div class="equation">
$$
16 \times 1024 \times 50{,}257 \times 2 = 1{,}640{,}419{,}072 \text{ bytes} \approx 1.53 \text{ GB}
$$
</div>

<p>Savings: $3.06 - 1.53 = 1.53$ GB (50\% reduction)</p>

<p><strong>Maximum batch size with 25\% memory budget:</strong>
Available memory: $0.25 \times 40{,}000 \text{ MB} = 10{,}000$ MB</p>

<p>For FP16 logits:
<div class="equation">
$$
B = \frac{10{,}000 \text{ MB}}{n \times V \times 2 \text{ bytes}} = \frac{10{,}000 \times 10^6}{1024 \times 50{,}257 \times 2} \approx 97 \text{ sequences}
$$
</div>

<p>With FP32, maximum batch size would be only 48 sequences.
</div>

<div class="solution"><strong>Solution:</strong> For sampled softmax with $K = 5{,}000$ and $V = 100{,}000$:

<p><strong>(1) Speedup factor:</strong>
<ul>
    <li>Full softmax: $O(V) = 100{,}000$ operations per token
    <li>Sampled softmax: $O(K+1) = 5{,}001$ operations per token
    <li>Speedup: $\frac{100{,}000}{5{,}001} \approx 20\times$
</ul>

<p><strong>(2) Memory reduction:</strong>
For batch of 32 sequences with 512 tokens:
<ul>
    <li>Full softmax logits: $32 \times 512 \times 100{,}000 \times 4 = 6{,}553{,}600{,}000$ bytes $\approx 6.1$ GB
    <li>Sampled softmax logits: $32 \times 512 \times 5{,}001 \times 4 = 327{,}745{,}536$ bytes $\approx 312$ MB
    <li>Reduction: $\frac{6.1 \text{ GB}}{312 \text{ MB}} \approx 20\times$
</ul>

<p><strong>(3) Why sampled softmax introduces bias:</strong>
<ul>
    <li>The gradient estimate is unbiased only if we sample from the true distribution
    <li>In practice, we sample from a proposal distribution (e.g., unigram frequency)
    <li>This creates importance sampling bias in the gradient
    <li>The normalization constant is approximated, not exact
    <li>Bias decreases as $K$ increases, but never reaches zero
    <li>For large $K$ (e.g., 10,000), bias is negligible for most applications
</ul>
</div>

<div class="solution"><strong>Solution:</strong> For A100 GPU with 1.5 TB/s bandwidth and 312 TFLOPS FP16 throughput, softmax over $V = 30{,}000$:

<p><strong>(1) Memory transfer time:</strong>
<div class="equation">
$$
\text{Time} = \frac{400 \text{ KB}}{1{,}500{,}000{,}000 \text{ KB/s}} = \frac{400}{1{,}500{,}000{,}000} \approx 0.267 \text{ microseconds}
$$
</div>

<p><strong>(2) Compute time:</strong>
For 30,000 exponentials and 30,000 divisions:
<div class="equation">
$$
\text{Time} = \frac{60{,}000 \text{ ops}}{312 \times 10^{12} \text{ ops/s}} \approx 0.000192 \text{ microseconds}
$$
</div>

<p><strong>(3) Bottleneck analysis:</strong>
<ul>
    <li>Memory time: 0.267 microseconds
    <li>Compute time: 0.000192 microseconds
    <li>The operation is <strong>memory-bound</strong> by a factor of $\frac{0.267}{0.000192} \approx 1{,}390\times$
</ul>

<p>This extreme memory-bandwidth bottleneck explains why vocabulary size has such a direct impact on training speed, and why reducing precision from FP32 to FP16 provides nearly 2√ó speedup for softmax operations.
</div>

<div class="solution"><strong>Solution:</strong> For knowledge distillation with temperature $T$:

<p><strong>(1) Temperature effect on gradient magnitude:</strong>
The softmax with temperature is:
<div class="equation">
$$
p_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}
$$
</div>

<p>As $T$ increases, the distribution becomes more uniform (softer). The gradient magnitude scales as $O(1/T)$ because:
<div class="equation">
$$
\frac{\partial p_i}{\partial z_j} = \frac{1}{T} p_i(\delta_{ij} - p_j)
$$
</div>

<p><strong>(2) Gradient derivation:</strong>
For KL divergence $D_{\text{KL}}(p_{\text{teacher}} \| p_{\text{student}})$ where both use temperature $T$:
<div class="equation">
$$\begin{align}
\frac{\partial D_{\text{KL}}}{\partial z'_i} &= \frac{\partial}{\partial z'_i} \sum_j p_j^T \log \frac{p_j^T}{q_j^T} \\
&= -\sum_j p_j^T \frac{\partial \log q_j^T}{\partial z'_i} \\
&= -\sum_j p_j^T \frac{1}{q_j^T} \frac{\partial q_j^T}{\partial z'_i} \\
&= -\sum_j p_j^T \frac{1}{q_j^T} \cdot \frac{1}{T} q_j^T(\delta_{ij} - q_i^T) \\
&= -\frac{1}{T} \sum_j p_j^T(\delta_{ij} - q_i^T) \\
&= \frac{1}{T}(q_i^T - p_i^T)
\end{align}$$
</div>

<p><strong>(3) Why $T^2$ scaling is necessary:</strong>
Without $T^2$ scaling, the gradient is $O(1/T)$, which vanishes as $T \to \infty$:
<div class="equation">
$$
\lim_{T \to \infty} \frac{1}{T}(q_i^T - p_i^T) = 0
$$
</div>

<p>With $T^2$ scaling, the effective gradient becomes:
<div class="equation">
$$
T^2 \cdot \frac{1}{T}(q_i^T - p_i^T) = T(q_i^T - p_i^T)
$$
</div>

<p>This compensates for the $1/T$ factor from the softmax derivative, maintaining meaningful gradients even for large $T$. The $T^2$ factor ensures that the distillation loss has the same scale as the hard label loss, allowing proper balancing between the two objectives.
</div>
        
        <div class="chapter-nav">
  <a href="chapter02_calculus_optimization.html">‚Üê Chapter 2: Calculus and Optimization</a>
  <a href="../../deeptech.html">üìö Table of Contents</a>
  <a href="chapter04_feedforward_networks.html">Chapter 4: Feed-Forward Neural Networks ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
