\chapter{Recommendation Systems and Personalization}
\label{chap:recommendations}

\section*{Chapter Overview}

Recommendation systems are the economic engines of the modern internet. They drive 80\% of content consumed on Netflix, 70\% of watch time on YouTube, and 35\% of purchases on Amazon. For these platforms, recommendation quality directly translates to revenue: a 1\% improvement in recommendation accuracy can generate tens to hundreds of millions of dollars in additional revenue annually. Poor recommendations, conversely, lead to user churn, reduced engagement, and lost revenue opportunities.

The business challenge is substantial. Platforms must serve personalized recommendations to billions of users in real-time (under 200ms latency), processing trillions of user-item interactions to learn preferences, while balancing competing objectives: immediate engagement (clicks, watch time) versus long-term retention, personalization versus diversity, and business goals (revenue, growth) versus societal concerns (fairness, filter bubbles).

This chapter examines how transformers and sequence models have revolutionized recommendation systems by capturing temporal dynamics and complex user-item interactions that traditional methods miss. A user who watches action movies followed by documentaries has different preferences than one who watches in reverse order—sequence matters. Transformer-based recommenders capture these patterns, improving recommendation quality by 10-30\% over traditional collaborative filtering.

However, these improvements come with challenges. Training requires processing billions of user interactions. Serving demands sub-second latency at massive scale. Model drift is severe—user preferences change daily, new items arrive constantly, and seasonal patterns shift. Fairness concerns are paramount—biased recommendations can amplify inequality and create filter bubbles that harm users and society. This chapter provides the technical foundation and business context to build recommendation systems that balance these competing demands effectively.

\section*{Learning Objectives}

\begin{enumerate}
\item Understand sequence-based recommendation architectures using transformers
\item Design and optimize ranking systems for accuracy and diversity
\item Implement multi-task learning for recommendations (CTR, conversion, long-term engagement)
\item Build real-time serving systems with latency constraints
\item Address fairness and filter-bubble concerns in personalization
\item Conduct online experiments (A/B tests) to validate recommendation improvements
\item Optimize for business metrics beyond accuracy
\end{enumerate}

\section{Sequence-Aware Recommenders}
\label{sec:sequencerecommendations}

Traditional recommendation systems treat user preferences as static. Matrix factorization, the workhorse of early recommender systems, decomposes a user-item interaction matrix $M \in \mathbb{R}^{U \times I}$ into low-rank factors: user $u$ is represented by a latent vector $\mathbf{p}_u \in \mathbb{R}^k$, and item $i$ by $\mathbf{q}_i \in \mathbb{R}^k$. The predicted rating is simply $\hat{M}_{u,i} = \mathbf{p}_u^T \mathbf{q}_i$. This approach powered early Netflix and Amazon recommendations and remains computationally efficient.

However, this static view ignores a fundamental aspect of human behavior: preferences evolve over time and depend on context. A user watching action movies in January followed by documentaries in February has different current preferences than one who watched in reverse order. A user browsing products on Monday morning (work-related) has different intent than the same user browsing Saturday evening (leisure). Matrix factorization treats these scenarios identically, missing critical temporal and contextual signals.

The business impact of this limitation is substantial. Static recommendations become stale quickly, especially for platforms with frequent user activity. A video platform user who watches 10 videos daily has preferences that shift hour-by-hour based on mood, time of day, and recent viewing. Static recommendations might suggest content from last week's interests, reducing engagement. One major streaming platform found that incorporating sequence information improved watch time by 12\% and reduced churn by 8\%—translating to hundreds of millions in annual revenue.

Sequence-aware models address this by treating recommendations as a language modeling problem: given a user's historical sequence of interactions, predict the next item. This framing is powerful because it leverages decades of NLP research on sequence modeling. Just as language models predict the next word given previous words, recommendation models predict the next item given previous items. The key insight is that user behavior follows patterns—watching a superhero movie increases the probability of watching another superhero movie, just as the word "New" increases the probability of "York."

\begin{definition}[Sequence-Based Recommendations]
\label{def:seqrec}
Given a user's interaction sequence $(i_1, i_2, \ldots, i_t)$, a sequence model predicts the probability distribution over next items:
\begin{align}
P(i_{t+1} \mid i_1, \ldots, i_t) &= \text{softmax}(W \text{encoder}(i_1, \ldots, i_t) + b)
\end{align}
where \texttt{encoder} is a transformer, RNN, or other sequential model that processes the interaction history. The vocabulary is the set of all items $|I|$ (potentially millions), and the logits correspond to scores for each item. Items with higher scores are more likely to be the next interaction.
\end{definition}

This formulation enables the model to capture rich temporal patterns. If a user watches three action movies in a row, the model learns that action movies have high probability for the next interaction. If a user alternates between genres, the model learns that pattern too. The model can even capture long-range dependencies—a user who watched a TV show's first episode two weeks ago is likely interested in episode two, even if they watched other content in between.

Historically, RNNs (LSTM, GRU) were the standard for sequence modeling in recommendations. However, transformers with self-attention provide several critical advantages that translate directly to business value:

\textbf{Parallelization during training.} RNNs process sequences sequentially, making training slow on long user histories. Transformers process entire sequences in parallel, reducing training time by 5-10x. For platforms retraining models daily on billions of interactions, this means the difference between 8-hour and 2-hour training jobs—enabling faster iteration and more frequent model updates.

\textbf{Long-range dependency modeling.} RNNs struggle with dependencies spanning hundreds of steps due to vanishing gradients. Transformers' attention mechanism directly connects any two positions in the sequence, capturing long-range patterns. For users with thousands of historical interactions, this means better recommendations based on preferences from weeks or months ago, not just recent activity.

\textbf{Interpretability through attention weights.} Attention weights show which past items influence the current recommendation. This interpretability helps debug model behavior, explain recommendations to users ("because you watched X"), and identify biases. For regulated industries or platforms facing scrutiny over algorithmic recommendations, this transparency is valuable.

\textbf{Multi-head attention for diverse patterns.} Different attention heads can learn different temporal patterns—one head might focus on recent items (short-term preferences), another on items from the same genre (topical consistency), and another on seasonal patterns (holiday content). This diversity improves recommendation quality by capturing multiple aspects of user behavior simultaneously.

\subsection{SASRec: Self-Attentive Sequential Recommendation}

SASRec is a transformer-based recommender that achieves state-of-the-art performance on benchmark datasets. The architecture:

\begin{enumerate}
\item \textbf{Embedding:} Each item is embedded as $\mathbf{e}_i \in \mathbb{R}^d$. Optionally, add positional encodings to capture temporal positions.
\item \textbf{Transformer layers:} Stack L transformer encoder layers, each with multi-head self-attention and feed-forward networks.
\item \textbf{Causal masking:} Use causal attention mask to prevent the model from attending to future items (maintaining prediction task structure).
\item \textbf{Output:} The representation at position $t$ predicts the next item: 
\begin{align}
\text{logits}_i = \mathbf{h}_t^T \mathbf{e}_i + b_i
\end{align}
where $\mathbf{h}_t$ is the output of the transformer at step $t$.
\item \textbf{Loss:} Cross-entropy loss on the correct next item, computed at each position.
\end{enumerate}

SASRec significantly outperforms RNN-based recommenders and matrix factorization on benchmark datasets (MovieLens, Amazon reviews, e-commerce), especially for longer user histories.

\subsection{Cold-Start Recommendations with Transformer-Capsule Networks (2024-2025)}

Cold-start recommendation—providing quality recommendations for new users or new items with limited interaction history—has been a persistent challenge in recommendation systems. Traditional approaches struggle because collaborative filtering requires sufficient interaction data to learn meaningful embeddings. Recent advances in 2024-2025 using transformer-capsule networks have achieved breakthrough performance on cold-start scenarios.

\textbf{Transformer-Capsule Graph (TCG-CS):} The TCG-CS architecture combines transformers' sequence modeling capabilities with capsule networks' ability to capture hierarchical relationships and graph neural networks' structural reasoning. This hybrid approach achieves 94.2\% accuracy on cold-start recommendation tasks, significantly outperforming previous methods.

\textbf{Key innovations:} Several breakthrough techniques enable TCG-CS to achieve superior cold-start performance.

Capsule-based user representation replaces single embedding vectors with capsules, which are groups of neurons that encode different aspects of preferences such as genre preferences, temporal patterns, and quality sensitivity. This structured representation enables learning from limited data by capturing the multifaceted nature of user preferences. Unlike flat embeddings that compress all preference information into a single vector, capsules maintain separate representations for different preference dimensions, making it easier to generalize from sparse interactions.

Graph-based item relationships model items as a graph where edges represent similarities such as same genre, same director, or co-purchased patterns. Graph neural networks propagate information from items with rich interaction history to cold-start items, enabling better initial embeddings for new items. This structural approach leverages the principle that similar items should have similar embeddings, allowing new items to benefit from the interaction history of related items even before receiving their own interactions.

Meta-learning for rapid adaptation uses techniques like Model-Agnostic Meta-Learning (MAML) to train models that can quickly adapt to new users with just 3-5 interactions. The model learns to learn, discovering which features and patterns are most informative for rapid personalization. This meta-learning approach is fundamentally different from standard training: instead of learning fixed parameters, the model learns initialization parameters that can be quickly fine-tuned to individual users with minimal data.

Content-collaborative hybrid approaches combine content features such as item metadata, descriptions, and images with collaborative signals from user-item interactions. For cold-start items, the system relies more heavily on content features to make initial recommendations. As interactions accumulate, the system gradually shifts weight to collaborative signals, which typically provide stronger personalization. This adaptive weighting ensures that recommendations remain relevant throughout an item's lifecycle, from launch through maturity.

\textbf{Business Impact:} Cold-start is particularly critical for platforms with high user or item churn. E-commerce platforms add thousands of new products daily. Streaming platforms onboard millions of new users monthly. Poor cold-start recommendations cause early churn—users who don't find relevant content in their first session are 3-5x more likely to abandon the platform.

Platforms implementing TCG-CS report substantial improvements across multiple metrics that directly impact business outcomes.

New user retention improves by 25-35\% when measured at day 7, meaning significantly more users continue using the platform after their first week. This improvement is critical because early retention is highly predictive of long-term user value. Users who remain engaged through the first week are 5-10x more likely to become long-term active users.

New item discovery improves by 40-50\%, measured as the fraction of new items that receive engagement within their first week. This metric is crucial for platforms with high content velocity, as it determines how quickly new content can find its audience. Better cold-start recommendations accelerate the discovery process, benefiting both users (who find relevant new content faster) and creators (whose content gains traction more quickly).

Time-to-personalization reduces by 15-20\%, meaning the system delivers personalized recommendations sooner in a user's lifecycle. Traditional systems might require weeks of interaction data before recommendations become truly personalized. TCG-CS achieves meaningful personalization within hours or days, dramatically improving the new user experience and reducing early churn.

Example: E-commerce platform with 1M new users monthly and 30\% day-7 retention. Improving retention by 5 percentage points (to 35\%) retains 50,000 additional users. At \$50 customer lifetime value, this generates \$2.5M monthly = \$30M annually. Development cost: \$500K. ROI: 60x.

\textbf{Implementation Considerations:} TCG-CS is computationally more expensive than standard transformers—training requires 2-3x compute due to capsule routing and graph convolutions. However, the improved cold-start performance often justifies the cost for platforms where cold-start is a critical bottleneck. Open-source implementations are emerging in PyTorch and TensorFlow as of 2025.

\section{Feature Engineering and Behavior Language}
\label{sec:recommenderfeatures}

Recommendations depend on more than just item history. User demographics, item metadata, temporal context, and behavioral signals all influence next-item preferences.

\subsection{DSL for Behavior Data}

Recommendation systems process event streams: each user interaction is an event with timestamp, user ID, item ID, and contextual features. The ``language'' of recommendation is this event schema:

\begin{definition}[Recommendation Event Schema]
Each event in a user session is:
\begin{align}
\text{Event} = \{\text{user\_id}, \text{item\_id}, \text{timestamp}, \text{event\_type}, \text{context}\}
\end{align}
where several components define the structure of each interaction event. The \texttt{event\_type} field categorizes the interaction, taking values from the set \{view, click, purchase, add-to-cart, share, rate\}, with each type carrying different signals about user intent and engagement level. The \texttt{context} field captures situational information including device type, geographic location, search query if applicable, and other environmental factors that influence user behavior and preferences.
\end{definition}

The sequence of events becomes the user's ``behavior language.'' A user's history might be:
\begin{verbatim}
[{item: 42, type: view, time: 10:00},
 {item: 42, type: click, time: 10:05},
 {item: 87, type: view, time: 10:15},
 {item: 87, type: purchase, time: 10:25}]
\end{verbatim}

The model learns that viewing+clicking an item increases the likelihood of purchase; views alone do not. Different event types carry different signals.

\subsection{Dense and Sparse Features}

Feature engineering bridges item history with demographic and contextual signals. Dense features provide continuous numerical representations that capture various aspects of users, items, and context.

User embeddings are learned during pre-training on user similarity tasks, capturing latent user characteristics and preferences in a continuous vector space. These embeddings encode patterns like which types of users tend to have similar preferences, enabling the model to generalize across similar users.

Item embeddings derive from product taxonomy or content embeddings, representing items in a semantic space where similar items are positioned nearby. These embeddings can be learned from item metadata, collaborative filtering patterns, or content features like text descriptions and images.

Temporal signals capture time-dependent patterns including time-of-day effects (users browse differently in morning vs. evening), day-of-week patterns (weekend vs. weekday behavior), and seasonality (holiday shopping, summer content preferences). These temporal features help the model adapt recommendations to the current context.

Historical aggregates summarize past behavior through statistics like the average rating a user gives (indicating rating strictness), the popularity of items the user likes (indicating mainstream vs. niche preferences), and engagement patterns (binge-watching vs. casual browsing). These aggregates provide stable signals about long-term user characteristics.

Sparse categorical features represent discrete attributes that take values from large vocabularies, requiring special handling through embedding layers.

User demographics include categorical attributes like age range, geographic location, and language preference. These features help the model understand how different user segments have different preferences and behaviors.

Item metadata captures categorical properties including category, subcategory, brand, and author. These attributes provide content-based signals that complement collaborative filtering, especially for cold-start items with limited interaction history.

Context features describe the interaction environment including device type (mobile, tablet, desktop), platform (app vs. web), and time zone. These contextual factors significantly influence user behavior and should be incorporated into recommendations.

Embedding these sparse features increases model capacity substantially. A categorical feature with 10,000 categories, such as user's home country, is embedded into a 16--32 dimensional vector. While this increases model parameters significantly, it enables the model to learn complex feature interactions and capture nuanced patterns that would be impossible with one-hot encoding.

\subsection{Multi-Task Learning for Recommendations}

Real recommendation systems optimize multiple objectives simultaneously, each capturing different aspects of user engagement and business value.

CTR (click-through rate) predicts whether the user will click on a recommended item, serving as a primary signal of immediate interest. High CTR indicates that recommendations are relevant and appealing enough to capture user attention. This metric is easy to measure and provides abundant training signal since every impression generates a click or no-click label.

Conversion predicts whether the user will complete a desired action such as making a purchase, subscribing, or completing a form. Conversion is a stronger signal than clicks because it represents actual business value, but it's also sparser—only a small fraction of clicks convert. Optimizing for conversion ensures recommendations drive revenue, not just engagement.

Engagement time predicts how long the user will engage with the recommended item, measured as video watch time, article reading time, or time spent on a product page. This metric captures content quality and user satisfaction better than binary click signals. A user who watches an entire video is more satisfied than one who clicks but immediately leaves.

Long-term value predicts whether the recommendation will lead to sustained engagement and user retention over weeks or months. This forward-looking metric is crucial because optimizing only for immediate engagement can harm long-term retention through clickbait, filter bubbles, or content that satisfies curiosity but doesn't build lasting interest. Long-term value is difficult to measure (requires long observation windows) but essential for platform health.

Multi-task learning trains a shared backbone with task-specific heads. The losses are combined with weights:
\begin{align}
\text{Loss}_{\text{total}} = \lambda_{\text{ctr}} \text{Loss}_{\text{ctr}} + \lambda_{\text{conv}} \text{Loss}_{\text{conversion}} + \lambda_{\text{engagement}} \text{Loss}_{\text{engagement}}
\end{align}

Task weights $\lambda$ are often tuned based on business priorities. A subscription platform may weight long-term engagement more heavily than short-term CTR.

\section{Real-Time Serving and Ranking}
\label{sec:recommendserving}

Training a model is just the first step. Serving recommendations to billions of users in real-time is a massive engineering challenge.

\subsection{Two-Stage Architecture}

Most recommendation systems use a two-stage pipeline:

\begin{enumerate}
\item \textbf{Candidate generation (retrieval):} From millions of items, retrieve a small set of candidates (100--1,000) that are relevant to the user. This stage is fast and approximate; exact ranking over all items is infeasible.
\item \textbf{Ranking:} Score the candidate set with a more complex model. Return the top-k items to the user.
\end{enumerate}

\subsection{Candidate Generation Strategies}

\subsection{Candidate Generation Strategies}

Candidate generation uses simple, fast methods to narrow the search space from millions of items to a manageable set for detailed ranking.

Embedding-based retrieval embeds both the user and all items into a shared vector space, then retrieves items nearest to the user embedding using efficient similarity search. This approach is fast, typically taking only milliseconds for MIPS (maximum inner product search) on GPU-accelerated indices like Faiss. The quality depends on the embedding space—well-trained embeddings that capture semantic similarity produce relevant candidates.

Collaborative filtering identifies similar users based on their interaction patterns, then recommends items that those similar users liked. User similarity can be computed offline using techniques like cosine similarity on interaction vectors or learned embeddings. At serving time, the system retrieves candidates by finding similar users and aggregating their preferences. This approach is effective because users with similar past behavior often have similar future preferences.

Content-based filtering retrieves items similar to those the user has previously interacted with, using item features like category, tags, or content embeddings. This approach works well for users with consistent preferences and provides good coverage for new items with rich metadata. However, it can create filter bubbles by only recommending items similar to past interactions.

Hybrid approaches combine multiple candidate sources to improve both relevance and diversity. A typical hybrid strategy might retrieve top-k trending items (ensuring popular content is considered), personalized candidates from collaborative filtering (ensuring personalization), and content-based candidates (ensuring coverage of user interests). The diversity of sources improves coverage and serendipity, exposing users to a broader range of relevant content than any single method would provide.

\subsection{Ranking Model and Latency Budget}

The ranking model scores candidates. With a 200 ms latency budget for the entire recommendation request and 50 ms allocated to candidate generation, the ranker has 150 ms. This is enough for a small neural network (2--3 layers) but not a 24-layer transformer.

Practical rankers are often gradient-boosted trees (e.g., XGBoost, LightGBM) or shallow neural networks. They consume hundreds of features (user features, item features, candidate-specific features like co-occurrence with items the user has rated) and output a score. The top-k candidates by score are returned.

\subsection{Real-Time Updates and Freshness}

\subsection{Real-Time Updates and Freshness}

Recommendation scores must be updated as new items arrive and user preferences change. Naive approaches that recompute scores for all users at each step are prohibitively expensive. Practical approaches balance freshness with computational feasibility.

Batch serving precomputes recommendations for all users nightly, storing results in a cache for fast retrieval. The system serves recommendations from the cache with occasional refreshes for highly active users who might have significantly changed preferences since the last batch update. This approach minimizes serving latency and computational cost but sacrifices freshness—recommendations can be up to 24 hours stale.

Online serving with feature caching computes expensive features like item embeddings and candidate sets offline, then fetches these precomputed features at request time to score with a fast model. This hybrid approach maintains reasonable freshness for features that change slowly (item embeddings) while enabling real-time scoring based on current user state. The fast scoring model can incorporate recent user actions without recomputing all features.

Streaming updates track user behavior in real-time using stream processing frameworks, updating embeddings and candidate sets incrementally as new interactions arrive. This approach provides the freshest recommendations but requires sophisticated infrastructure to handle high-throughput event streams and maintain consistency. Streaming updates are most valuable for highly active users whose preferences change rapidly within a session.

Freshness vs. latency is a trade-off. Highly personalized real-time recommendations are better but slower. Pre-computed recommendations are faster but stale.

\section{Fairness, Diversity, and Filter Bubbles}
\label{sec:fairnessrecommendations}

Recommendation systems can amplify biases and trap users in filter bubbles---showing only content that aligns with past preferences, limiting exposure to diverse views.

\subsection{Filter Bubble Problem}

If a user watches many political videos from one perspective, a system optimizing for watch time may recommend only that perspective, reinforcing views and reducing exposure to alternative viewpoints. While maximizing engagement, this harms user growth and societal polarization.

Solutions include several complementary approaches to increase recommendation diversity and reduce filter bubble effects.

Diversity metrics measure the variety of recommendations using measures like entropy of recommended categories. By including diversity as an explicit term in the ranking objective, the system can balance relevance with variety. For example, the ranking score might be a weighted combination of predicted engagement and category diversity, ensuring that highly relevant but homogeneous recommendations don't dominate.

Exploration allocates a portion of recommendations (typically 10\%) to items outside the user's typical preferences, enabling discovery of new interests. This exploration can be random or guided by uncertainty estimates—recommending items where the model is uncertain about user preferences. Exploration serves dual purposes: it helps users discover new content they might enjoy, and it generates training data for underexplored items and user segments.

Fairness constraints ensure that underrepresented creators and items receive exposure proportional to their quality, not just their historical popularity. Dynamic allocation algorithms balance personalization with fairness by setting minimum exposure thresholds for different creator demographics or content categories. This prevents winner-take-all dynamics where popular content dominates recommendations at the expense of quality niche content.

User control empowers users to adjust recommendation diversity or opt for curated feeds rather than purely algorithmic recommendations. Providing transparency about why items are recommended ("because you watched X") and controls to adjust recommendation behavior ("show me more diverse content") increases user trust and satisfaction. Users who feel in control of their experience are more likely to remain engaged long-term.

\subsection{Handling Demographic Bias}

\subsection{Handling Demographic Bias}

Models trained on interaction data inherit biases present in historical patterns. If women historically receive fewer views for technical content due to societal biases, the model may learn to downrank women creators in technical categories, perpetuating and amplifying the bias. Mitigation strategies address these biases at multiple stages of the recommendation pipeline.

Balanced datasets oversample interactions from underrepresented groups during training, ensuring the model sees sufficient examples of diverse creator-user interactions. For example, if women creators represent 20\% of technical content but receive only 10\% of views, oversampling their interactions to 20\% of training data helps the model learn unbiased quality signals. This rebalancing corrects for historical underexposure without requiring the model to explicitly reason about fairness.

Fairness-aware loss functions add a penalty term that discourages disparate performance across demographic groups. The loss function might include a term measuring the difference in average recommendation scores between demographic groups, encouraging the model to provide similar quality recommendations regardless of creator demographics. This approach directly optimizes for fairness as a training objective.

Allocation fairness ensures that a minimum fraction of recommendations go to creators from all demographics, regardless of predicted engagement. Post-processing algorithms can enforce these constraints by adjusting recommendation rankings to meet fairness targets while minimizing impact on overall engagement. For example, if the top-10 recommendations contain no creators from underrepresented groups, the system might replace the lowest-ranked item with the highest-ranked item from an underrepresented group.

\subsection{A/B Testing for Recommendation Changes}

Before deploying recommendation model improvements, validate with A/B tests that randomly split users into control and treatment groups.

The control group continues using the existing recommendation algorithm, providing a baseline for comparison. This group represents the status quo performance and helps account for external factors like seasonality or platform-wide changes that affect all users.

The treatment group receives recommendations from the new model, enabling direct measurement of the model's impact. The random assignment ensures that differences between groups are attributable to the model change rather than pre-existing user differences.

Measure metrics over 2--4 weeks to account for short-term novelty effects and capture longer-term impacts on user behavior.

Engagement metrics including watch time, clicks, and session length measure immediate user response to recommendations. These metrics are sensitive and provide quick feedback on whether the new model improves user experience. However, they can be gamed by clickbait or sensational content that attracts clicks but doesn't satisfy users.

Retention metrics measure the return rate, calculating the fraction of users who return the next day or week. Retention is a stronger signal of recommendation quality than engagement because it reflects sustained satisfaction. Users who receive consistently good recommendations are more likely to return, while poor recommendations drive churn.

Diversity metrics measure the entropy of categories recommended, ensuring that improvements in engagement don't come at the cost of reduced variety. A model that only recommends popular mainstream content might achieve high engagement but create filter bubbles and reduce long-term user satisfaction.

Fairness metrics track recommendation volume for different creator demographics, ensuring that model improvements don't disproportionately benefit or harm specific groups. Measuring fairness explicitly prevents unintended bias amplification and ensures the platform serves all creators equitably.

If the treatment significantly improves key metrics without degrading others, roll out to all users. This empirical validation is critical; models that perform well on offline metrics may hurt business KPIs in practice.

\section{Case Study: Video Recommendation for a Streaming Platform}
\label{sec:casestudyrecommendation}

A video streaming platform with 100 million users and 10 million videos seeks to improve watch time and retention through better recommendations.

\subsection{System Architecture}

\textbf{Candidate Generation:} The first stage rapidly narrows millions of videos to a manageable candidate set.

User embedding is pretrained using a siamese network on user-user similarity, where users who watch similar videos receive similar embeddings. This embedding captures latent user preferences and enables fast similarity-based retrieval.

Item embedding combines video embeddings from a collaborative filtering model (capturing which videos are watched together) with content embeddings from video metadata (title, description, tags, thumbnails). This hybrid embedding represents both collaborative patterns and content semantics.

Retrieval uses MIPS (maximum inner product search) with the Faiss library to find the top-500 candidates most similar to the user embedding in just 20 milliseconds. This fast approximate search makes real-time candidate generation feasible at scale.

\textbf{Ranking:} The second stage scores candidates with a more sophisticated model to produce the final top-10 recommendations.

The model uses XGBoost, a gradient-boosted decision tree algorithm, with 500 features capturing user characteristics, item properties, and user-item interactions. XGBoost provides excellent performance with reasonable latency, making it suitable for real-time serving.

Features include user watch time patterns, historical CTR for similar content, video popularity metrics, recency (how recently uploaded), user-item co-occurrence (how often users like this user watch this video), and genre match between user preferences and video category. These diverse features enable the model to capture multiple signals of relevance.

Latency is constrained to 80 milliseconds to score all 500 candidates and rank the top-10, fitting within the overall 200ms latency budget for the recommendation request. This tight latency constraint requires careful model optimization and efficient feature computation.

\textbf{Post-processing:} The final stage applies business rules and fairness constraints to the ranked list.

A diversity filter ensures variety by checking if the top-10 contains too many videos from the same genre (e.g., 5 action movies). If so, the system reranks to ensure a maximum of 3 videos per genre, promoting exploration of different content types.

A freshness boost increases the score of videos uploaded less than 1 week ago, ensuring that new content receives exposure even if it hasn't accumulated many interactions yet. This helps new creators gain traction and keeps the platform feeling current.

Creator fairness constraints ensure the top-10 includes creators from different regions and demographics, preventing the recommendations from being dominated by a small set of popular creators. This promotes platform diversity and gives all creators fair opportunity for exposure.

\subsection{Training and Offline Evaluation}

\textbf{Data:} 100 billion historical events (views, clicks, shares) over 3 months

\textbf{Metrics:} The system is evaluated using standard recommendation metrics that measure both coverage and ranking quality.

Recall@10 measures what fraction of videos the user actually watched next appear in the top-10 recommendations. This metric captures whether the system successfully identifies relevant content, with a target of at least 8\%. Higher recall means users are more likely to find content they want in the recommendations.

NDCG@10 (Normalized Discounted Cumulative Gain) measures ranking quality by assigning higher value to relevant items that appear earlier in the list. Videos that users clicked or watched receive higher scores, and the metric penalizes relevant items that appear lower in the ranking. The target is at least 0.45, indicating that relevant content consistently appears near the top of recommendations.

\textbf{Results:} Progressive model improvements demonstrate clear gains in recommendation quality.

The collaborative filtering baseline achieves NDCG@10 of 0.38, providing a strong foundation but missing temporal patterns in user behavior. This traditional approach serves as the performance floor that newer methods must exceed.

The sequence model (SASRec) improves to NDCG@10 of 0.43, a 13\% improvement over the baseline. By modeling user interaction sequences with transformers, the system captures temporal dynamics and session-level patterns that collaborative filtering misses.

SASRec with multi-task learning reaches NDCG@10 of 0.46, adding another 7\% improvement. By jointly optimizing for multiple objectives (CTR, watch time, retention), the model learns richer representations that better predict user satisfaction across different dimensions.

\subsection{Online A/B Test}

Deploy the improved recommender to 10\% of users.

\textbf{Results over 4 weeks:} The online test validates that offline improvements translate to real business impact.

Watch time increases by 4.2\%, a statistically significant improvement indicating that users engage more deeply with the improved recommendations. This translates to millions of additional hours of content consumption, directly impacting platform value and advertising revenue.

CTR on recommendations improves by 3.8\%, showing that users find the recommendations more appealing and relevant. Higher CTR indicates better matching between recommended content and user interests.

Session length increases by 2.1\%, meaning users explore more content per visit. This suggests that good recommendations create positive momentum, encouraging users to continue browsing and watching rather than leaving the platform.

Retention (30-day) improves by 1.5\%, with more users returning to the platform over the following month. This long-term metric is crucial because it indicates sustained satisfaction rather than just short-term engagement spikes. Improved retention has compounding effects on platform growth and user lifetime value.

Diversity increases by 8\%, measured as entropy of recommended categories. Users explore new genres and content types rather than staying in narrow filter bubbles. This diversity benefits both users (who discover new interests) and creators (who reach broader audiences).

Creator fairness improves with a 12\% increase in watch share for underrepresented creators, rising from 5\% to 5.6\% of total watch time. While seemingly small in absolute terms, this represents thousands of additional creators receiving meaningful exposure and represents progress toward equitable content distribution.

No negative impact on fairness or diversity metrics. The model improved both engagement and societal goals. Deployment proceeds to all users, expected to recover millions of hours of additional user engagement annually.

\section{Model Maintenance and Drift in Recommendation Systems}
\label{sec:recommendationdrift}

Recommendation systems face some of the most severe drift challenges of any machine learning application. User preferences evolve constantly—daily, weekly, and seasonally. New items arrive continuously, creating cold-start problems. Content trends shift rapidly, especially on social platforms. External events (holidays, news, cultural moments) dramatically change consumption patterns. A recommendation model trained on last month's data can become obsolete within weeks, causing measurable degradation in engagement and revenue.

The business stakes are enormous. A 1\% drop in recommendation quality can cost large platforms millions of dollars monthly in lost engagement and advertising revenue. One major video platform observed a 5\% decline in watch time over three months due to undetected model drift, costing an estimated \$30 million in revenue before the issue was identified and corrected. Effective drift management is not optional—it's essential for maintaining competitive advantage and business performance.

\subsection{Domain-Specific Drift Patterns in Recommendations}

Recommendation drift manifests in several distinct ways, each requiring different detection and mitigation strategies:

\textbf{User preference drift.} Individual users' tastes evolve over time. A user interested in action movies in January may shift to documentaries in March. A user who primarily shopped for electronics may start shopping for baby products (life event). A user's music preferences may broaden as they discover new genres. This drift is gradual but pervasive—studies show 20-40\% of users exhibit significant preference changes over 3-6 months.

The business impact is direct: recommendations based on outdated preferences reduce engagement. A user who has moved on from action movies but continues receiving action recommendations will have lower click-through rates and watch time. At scale, even small per-user impacts compound to significant revenue losses.

\textbf{Item cold-start and freshness drift.} New items arrive constantly—new videos uploaded, new products listed, new songs released. These items have no interaction history, making them difficult to recommend (cold-start problem). However, users often prefer fresh content over older content, even if older content has better historical engagement. A recommendation system that doesn't adapt to new items will feel stale, reducing user satisfaction.

The challenge is particularly acute for platforms with high content velocity. A short-form video platform might receive millions of new videos daily. A news platform's content becomes stale within hours. Recommendation systems must balance exploiting known good content (high engagement) with exploring new content (freshness, discovery).

\textbf{Seasonal and event-driven drift.} Consumption patterns exhibit strong seasonal patterns. Holiday shopping peaks in November-December. Sports content surges during major events. Back-to-school shopping spikes in August. Music preferences shift with seasons (upbeat in summer, mellow in winter). These patterns are predictable but must be incorporated into models.

External events create unpredictable drift. A viral trend, breaking news, or cultural moment can shift consumption patterns overnight. During the COVID-19 pandemic, content consumption patterns changed dramatically—more home workout videos, cooking content, and educational material. Models trained on pre-pandemic data performed poorly without rapid adaptation.

\textbf{Popularity and trend drift.} Item popularity is highly dynamic. A video that goes viral sees 1000x increase in views within days. A product featured in a celebrity post sees massive demand spikes. A song that becomes a meme dominates listening. Recommendation systems must adapt to these popularity shifts to remain relevant.

However, over-emphasizing popularity creates problems. Recommending only trending content reduces personalization and creates winner-take-all dynamics that harm content diversity and creator fairness. Balancing popularity signals with personalization is a key challenge.

\textbf{Behavioral pattern drift.} How users interact with platforms evolves. Mobile usage patterns differ from desktop. Short-form video platforms train users to expect rapid content switching. Binge-watching behavior on streaming platforms creates different engagement patterns than episodic viewing. As platform features evolve (new UI, new content formats), user behavior adapts, and recommendation models must follow.

\textbf{Cross-platform and cross-device drift.} Users increasingly interact with platforms across multiple devices (phone, tablet, desktop, TV) and contexts (commute, home, work). Preferences and engagement patterns differ by device and context. A user might watch short clips on mobile during commute but long-form content on TV at home. Models must adapt to these context-dependent patterns.

\subsection{Business Impact of Recommendation Drift}

The business consequences of unmanaged drift in recommendation systems are severe and measurable:

\textbf{Engagement degradation.} As models drift, recommendation quality declines, reducing click-through rates, watch time, and session length. A 2\% drop in CTR might seem small but translates to millions of lost interactions daily for large platforms. One e-commerce platform observed a 3\% decline in conversion rate over four months due to drift, costing \$15 million in lost revenue.

\textbf{User churn and retention impact.} Poor recommendations frustrate users, increasing churn. Users who consistently receive irrelevant recommendations are 2-3x more likely to stop using the platform. For subscription services, this directly impacts recurring revenue. For ad-supported platforms, it reduces the user base and advertising inventory. One streaming service found that users experiencing poor recommendations (measured by low engagement with recommended content) had 25\% higher churn rates.

\textbf{Creator and supplier dissatisfaction.} When recommendation systems drift, content distribution becomes skewed. New creators struggle to get exposure. Niche content gets buried. This frustrates creators and suppliers, potentially causing them to leave the platform or reduce content production. For platforms dependent on user-generated content or third-party suppliers, this threatens content supply and platform viability.

\textbf{Revenue and monetization impact.} Recommendation quality directly affects revenue. E-commerce platforms lose sales when product recommendations are poor. Advertising platforms lose revenue when ad targeting becomes less effective. Subscription platforms face churn. One major platform estimated that a 1\% improvement in recommendation quality generates \$50 million in annual revenue—conversely, 1\% degradation costs \$50 million.

\textbf{Competitive disadvantage.} In competitive markets, recommendation quality is a key differentiator. Users choose platforms with better personalization. If competitors adapt to drift faster, they gain market share. The platform with the best recommendations wins user attention and engagement, creating a competitive moat. Falling behind on recommendation quality can be existential for platforms in crowded markets.

\subsection{Detecting Drift in Recommendation Systems}

Effective drift detection requires monitoring multiple signals across different time scales:

\textbf{Performance-based detection.} Monitor key business metrics continuously: click-through rate, conversion rate, watch time, session length, return rate. Establish baseline performance and alert when metrics degrade beyond thresholds. Use statistical process control to distinguish normal variation from systematic drift.

Example: If CTR drops 1\% week-over-week for three consecutive weeks, trigger drift investigation. If watch time per session declines 2\% month-over-month, investigate potential model staleness. Set up automated dashboards that track these metrics by user segment, content category, and device type to identify where drift is occurring.

\textbf{Prediction confidence monitoring.} Track model confidence scores over time. If average prediction confidence decreases or the proportion of low-confidence predictions increases, the model may be encountering out-of-distribution patterns. For example, if a CTR prediction model's average confidence drops from 0.75 to 0.65 over a month, investigate whether user behavior or item characteristics have shifted.

\textbf{Temporal pattern analysis.} Recommendation systems have strong temporal patterns (hourly, daily, weekly, seasonal). Establish baseline patterns and detect anomalies. If weekend engagement patterns suddenly differ from historical weekends, investigate. If holiday season performance differs from previous years, drift may be occurring. Use time-series anomaly detection to identify deviations from expected patterns.

\textbf{Item and user distribution monitoring.} Track distributions of recommended items and user interactions. If the distribution of recommended categories shifts (e.g., more action movies, fewer documentaries) without corresponding changes in user preferences, the model may be drifting. If new items receive disproportionately low recommendation rates, cold-start handling may be failing.

\textbf{Cohort analysis.} Track performance for different user cohorts: new users vs. long-time users, different demographics, different engagement levels. Drift often affects cohorts differently. New users might be more sensitive to poor recommendations (higher churn), while engaged users might tolerate lower quality. Identifying which cohorts are affected helps prioritize fixes.

\textbf{A/B test monitoring.} Continuously run A/B tests comparing current production model to recent alternatives. If a model trained on more recent data significantly outperforms the production model, drift has occurred and retraining is needed. This provides early warning before business metrics degrade significantly.

\textbf{Human evaluation and feedback.} Continuously sample recommendations for human review. Ask raters: "Are these recommendations relevant and high-quality?" Track agreement between model predictions and human judgments. If agreement drops from 85\% to 75\%, investigate. Collect user feedback ("Why did you recommend this?") and track negative feedback rates.

\subsection{Strategies for Continuous Learning in Recommendation Systems}

Managing drift in recommendation systems requires aggressive continuous learning strategies due to the rapid pace of preference and content evolution:

\textbf{Frequent periodic retraining.} Retrain models on a regular, frequent schedule using recent data. For high-velocity platforms (social media, news), this might mean daily or weekly retraining. For slower-moving platforms (e-commerce, streaming), monthly retraining may suffice. Use rolling windows of recent data (e.g., last 3-6 months) to ensure models reflect current patterns while maintaining sufficient training data.

Implementation: Maintain automated retraining pipelines that run on schedule. Use the most recent N days of interaction data. Validate on held-out recent data (last week). Deploy if validation metrics exceed current production model. For a large platform, daily retraining might cost \$5,000-10,000 in compute but generates \$100,000s in additional revenue through improved recommendations.

\textbf{Online learning and real-time updates.} For systems with continuous user feedback, implement online learning to update models in real-time or near-real-time. This is technically challenging for large models but possible with careful engineering. Use techniques like online gradient descent, incremental matrix factorization, or parameter-efficient fine-tuning (LoRA) to update model weights based on recent interactions.

Example: Update user embeddings hourly based on recent interactions. Update item embeddings daily as new items receive interactions. This keeps the model fresh without full retraining. One video platform implemented hourly user embedding updates and saw 2\% improvement in CTR for active users.

\textbf{Ensemble approaches with temporal diversity.} Maintain an ensemble of models trained on different time periods. Combine predictions from models trained on recent data (captures current trends) and older data (maintains stability and captures long-term preferences). Weight ensemble members based on recency and performance.

Example: Maintain three models: trained on last month, last quarter, and last year. For each user, weight predictions based on user activity level—active users get more weight on recent model, inactive users get more weight on long-term model. This provides robustness to drift while maintaining performance on stable patterns.

\textbf{Separate models for different time scales.} Build separate models for short-term and long-term preferences. Short-term model captures recent session behavior (last hour, last day). Long-term model captures stable preferences (last months, last year). Combine predictions to balance immediate context with enduring interests.

Example: Short-term model uses transformer on recent 20 interactions. Long-term model uses matrix factorization on all historical interactions. Final score is weighted combination: 70\% short-term, 30\% long-term. This captures both "what user wants right now" and "what user generally likes."

\textbf{Contextual bandits and exploration.} Use multi-armed bandit algorithms to balance exploitation (recommend known good items) with exploration (try new items to learn preferences). This provides continuous learning through exploration while maintaining engagement through exploitation. Contextual bandits incorporate user and item features to make exploration more efficient.

Example: Allocate 10\% of recommendations to exploration (items with high uncertainty or new items). Use Thompson sampling or upper confidence bound (UCB) algorithms to select exploration candidates. Track performance and update item value estimates in real-time. This enables rapid adaptation to new items and changing preferences.

\textbf{Transfer learning and meta-learning.} Use transfer learning to adapt models to new users and items quickly. Pretrain on large datasets, then fine-tune on specific user segments or item categories. Use meta-learning (learning to learn) to enable rapid adaptation with few examples. This is particularly valuable for cold-start scenarios.

Example: Pretrain a sequence model on all users. For new users, fine-tune on their first 10 interactions using meta-learning techniques (MAML, Reptile). This enables personalized recommendations after just a few interactions, reducing cold-start period from weeks to hours.

\textbf{Seasonal and event-aware modeling.} Explicitly model seasonal patterns and events. Include temporal features (month, day of week, holiday indicators) in models. Train separate models or model components for different seasons or events. This enables proactive adaptation to predictable drift.

Example: Train separate models for holiday season (November-December) and regular season. Switch models based on calendar. Include features for major events (sports championships, award shows). This improves recommendations during high-value periods when engagement and revenue are highest.

\subsection{Practical Implementation Considerations}

Successfully implementing continuous learning for recommendation systems requires careful attention to operational details:

\textbf{Data pipeline and freshness.} Maintain real-time or near-real-time data pipelines that collect user interactions, process them, and make them available for training. Latency in data pipelines directly impacts model freshness. A 24-hour delay in data availability means models are always one day behind reality. Invest in streaming data infrastructure (Kafka, Flink) to minimize latency.

\textbf{Feature engineering and computation.} Many recommendation features require expensive computation (user similarity, item popularity, co-occurrence statistics). Precompute features offline and update regularly. Use approximate algorithms (MinHash for similarity, HyperLogLog for counting) to make computation tractable at scale. Cache frequently accessed features to reduce latency.

\textbf{Model versioning and experimentation.} Maintain strict versioning of models, training data, and features. Enable rapid experimentation by making it easy to train, validate, and deploy new model variants. Use feature flags to control which users see which models, enabling gradual rollouts and A/B tests. Implement automated rollback if new models degrade metrics.

\textbf{Monitoring and alerting infrastructure.} Build comprehensive monitoring that tracks all relevant metrics: business KPIs, model performance, data quality, system latency, and cost. Set up alerts for anomalies. Ensure dashboards are accessible to both ML engineers and product managers. Invest in observability tools that enable rapid diagnosis when issues occur.

\textbf{Cost management and optimization.} Continuous learning is expensive. Daily retraining for a large platform can cost \$50,000-100,000 monthly. Optimize costs through: (1) efficient model architectures, (2) smart data sampling (don't train on all data, sample strategically), (3) incremental learning where possible, (4) spot instances for training, (5) careful monitoring to avoid unnecessary retraining.

Budget example: Platform with 100M users, 10M items, 1B daily interactions. Daily retraining costs \$3,000 (compute). Inference costs \$20,000/day (serving). Monitoring and storage \$5,000/day. Total: \$28,000/day = \$840,000/month. Compare to business value: if continuous learning maintains 2\% higher engagement worth \$5M/month in revenue, ROI is 6x.

\textbf{Organizational structure and ownership.} Recommendation systems require cross-functional collaboration: ML engineers (models), data engineers (pipelines), backend engineers (serving), product managers (metrics), and data scientists (analysis). Establish clear ownership and communication channels. Use shared dashboards and regular reviews to ensure alignment. Invest in tools that enable non-ML team members to understand and influence recommendation behavior.

\subsection{Cross-Domain Patterns and Connections}

The continuous learning challenges in recommendation systems share patterns with other domains while having unique characteristics:

\textbf{Chapter 24 (Domain-Specific Models):} The general continuous learning framework from Chapter~\ref{chap:domainspecificmodels} applies here, but recommendations face more severe drift due to the dynamic nature of user preferences and content. While enterprise NLP might retrain quarterly, recommendation systems often need daily or weekly updates to maintain performance.

\textbf{Chapter 27 (Video \& Visual):} Video recommendation systems combine visual content understanding (Chapter~\ref{chap:videoandvisual}) with recommendation algorithms. Drift occurs in both visual content characteristics (new video styles, effects) and user preferences. The visual understanding models and recommendation models must be updated in coordination to maintain quality.

\textbf{Chapter 28 (Knowledge Graphs):} Knowledge graph-based recommendations (Chapter~\ref{chap:knowledgegraphs}) face drift as the knowledge graph evolves—new entities, new relationships, changing entity attributes. The graph structure itself drifts, requiring continuous graph updates and embedding recomputation. Techniques like temporal knowledge graphs and incremental embedding updates are essential.

\textbf{Chapter 30 (Healthcare):} Healthcare recommendation systems (treatment recommendations, care pathways) face drift as medical knowledge evolves and patient populations change. However, the stakes are higher (patient safety) and the pace is slower (medical knowledge evolves over years, not days). The continuous learning strategies must be adapted to prioritize safety and regulatory compliance over rapid adaptation.

\textbf{Chapter 33 (Observability):} Monitoring recommendation systems requires specialized observability infrastructure discussed in Chapter~\ref{chap:observability}. Recommendation quality is multidimensional (accuracy, diversity, fairness) and difficult to measure in real-time. Effective observability is essential for detecting drift early and diagnosing root causes. Invest in recommendation-specific monitoring tools and dashboards.

\section{Exercises}

\begin{exercise}
Implement a simple sequence-based recommender using a 2-layer transformer encoder. Train on MovieLens-1M. Evaluate using Recall@20 and NDCG@20 metrics. How does performance compare to a baseline RNN-based recommender?
\end{exercise}

\begin{exercise}
Design a multi-task recommendation system that predicts both click-through rate (CTR) and conversion rate (CVR). What is the relationship between the two tasks? Should they share weights or have separate heads? How would you weight the two losses?
\end{exercise}

\begin{exercise}
Analyze the filter bubble effect in a recommendation system. Given historical user interactions, recommend items and measure recommendation diversity. Propose modifications to increase diversity while maintaining engagement.
\end{exercise}

\section{Solutions}

\begin{solution}
\textbf{Exercise 1: Sequence-Based Recommender}

\textit{Architecture:}
\begin{itemize}
\item Item embedding dimension: 64
\item Transformer: 2 layers, 4 attention heads, FFN hidden dim = 256
\item Causal masking: prevent attending to future items
\item Loss: Cross-entropy on next item (max sequence length = 20)
\end{itemize}

\textit{Training (MovieLens-1M):}
\begin{itemize}
\item Prepare sequences of movie ratings. Threshold rating > 3 as positive interactions.
\item Filter users with $\geq$ 5 interactions; obtain 650,000 user sequences.
\item Train/val split: 80/20
\item Batch size: 128, learning rate: $1 \times 10^{-3}$, epochs: 20
\end{itemize}

\textit{Results:}
\begin{itemize}
\item Transformer Recall@20: 0.52, NDCG@20: 0.38
\item RNN baseline (GRU-128): Recall@20: 0.48, NDCG@20: 0.35
\item Transformer is 8\% better on recall, 9\% better on NDCG
\item Inference latency: 15 ms per user (transformer) vs. 8 ms (RNN); trade-off acceptable
\end{itemize}
\end{solution}

\begin{solution}
\textbf{Exercise 2: Multi-Task Learning for CTR and CVR}

\textit{Task Relationship:}
CTR (click) and CVR (conversion/purchase) are sequential: a user must click before converting. Correlation is high (users who click are more likely to convert), but causality is clear: click enables conversion.

\textit{Architecture:}
\begin{itemize}
\item Shared backbone: embedding layer + 2 fully-connected hidden layers (256 -> 128)
\item Task-specific heads:
  \begin{itemize}
  \item CTR head: 1 dense layer -> sigmoid -> CTR probability
  \item CVR head: 1 dense layer -> sigmoid -> CVR probability
  \end{itemize}
\item Note: CVR is computed on clicked items only (conditional probability)
\end{itemize}

\textit{Loss weighting:}
\begin{align}
\text{Loss} = \lambda_{\text{ctr}} \text{BCE}(\hat{y}_{\text{ctr}}, y_{\text{ctr}}) + \lambda_{\text{cvr}} \text{BCE}(\hat{y}_{\text{cvr}}, y_{\text{cvr}})
\end{align}

Suggest $\lambda_{\text{ctr}} = 0.7, \lambda_{\text{cvr}} = 0.3$ since CTR is more frequent and diverse.

\textit{Alternative: CTR as auxiliary task:}
Some systems use CTR as an auxiliary task to regularize the CVR model, since more training signal is available for CTR. This improves CVR generalization.

\itshape Results (on dataset with 10\% CTR, 2\% CVR):
\begin{itemize}
\item Standalone CVR model: AUC = 0.72
\item Multi-task (CTR + CVR): AUC = 0.78 (8\% improvement)
\item Improvement from auxiliary task signal and shared representation learning
\end{itemize}
\end{solution}

\begin{solution}
\textbf{Exercise 3: Filter Bubble Analysis}

\textit{Diversity Measurement:}
For a user with recommendation sequence, compute entropy of recommended categories:
\begin{align}
\text{Diversity} = -\sum_c p_c \log p_c
\end{align}
where $p_c$ is the fraction of recommendations in category $c$.

\textit{Analysis:}
\begin{itemize}
\item Baseline recommender: Average diversity = 1.2 bits (low; most recommendations in user's primary interests)
\item Improvement proposal: Allocate 10\% of top-10 recommendations for exploration (diverse categories)
\item Result: Diversity increases to 1.8 bits (+50\%)
\item Engagement impact: Watch time on exploration recommendations is 30\% lower but leads to user growth (10\% increase in diversity preference)
\end{itemize}

\itshape Implementation:
Re-rank top-20 candidates to ensure diversity:
\begin{enumerate}
\item Sort by engagement score
\item Greedily select top-10 while maintaining max 3 items per category
\item For each selected item, remove other items from the same category to increase diversity
\end{enumerate}

This increases diversity with minimal engagement loss ($< 0.5\%$) and aligns system with user long-term interests.
\end{solution}
