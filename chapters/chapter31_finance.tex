\chapter{Finance, Risk, and Time Series Modeling}
\label{chap:finance}

\section*{Chapter Overview}

Financial services represent one of the most data-intensive and competitive industries in the world, where milliseconds matter and billions of dollars are at stake. Global financial markets trade over \$6 trillion daily in foreign exchange alone. Credit card fraud costs \$28 billion annually. Credit decisions affect millions of consumers and trillions in lending. Algorithmic trading accounts for 60-75\% of US equity trading volume. In this environment, even marginal improvements in prediction accuracy, fraud detection, or risk management translate to enormous competitive advantages and financial returns.

This chapter examines how transformers and deep learning are transforming financial services across three critical domains: market prediction and algorithmic trading, financial text analysis for investment decisions, and credit risk assessment. Each domain presents unique challenges and opportunities. Market prediction faces non-stationary time series where patterns change constantly and historical data provides limited guidance. Financial NLP must process earnings calls, SEC filings, and news in real-time to extract actionable insights before markets react. Credit modeling must balance predictive accuracy with fairness, explainability, and regulatory compliance.

The business stakes are extraordinary. A hedge fund with a 1\% edge in market prediction can generate hundreds of millions in annual returns. A fraud detection system that improves accuracy by 2\% saves tens of millions in prevented losses. A credit model that reduces default rates by 0.5\% while maintaining approval rates generates millions in reduced losses. These improvements compound over time, creating sustainable competitive advantages in an industry where competitors are constantly innovating.

However, financial AI faces unique challenges that make it fundamentally different from other domains. Markets are adversarial—as soon as a profitable pattern is discovered and exploited, it disappears as others exploit it too. Non-stationarity is severe—market regimes shift suddenly during crises, rendering historical models useless. Regulatory scrutiny is intense—models must be explainable, auditable, and fair. Data is limited—unlike computer vision with millions of images, financial history provides only decades of data with few independent samples. And critically, financial AI operates in a zero-sum environment—your gain is someone else's loss, creating intense competitive pressure and secrecy.

This chapter provides the technical foundation and business context to build financial AI systems that navigate these challenges. We examine successful strategies, regulatory requirements, risk management frameworks, and the economic models that make financial AI viable despite its unique difficulties. The focus is on practical deployment: what works in production, what fails, and why.

\section*{Learning Objectives}

\begin{enumerate}
\item Understand financial time series, stationarity, and non-stationarity
\item Build transformer-based models for price prediction and risk forecasting
\item Extract information from financial text: earnings calls, news, SEC filings
\item Design credit scoring systems aligned with fairness and regulatory requirements
\item Implement risk management workflows: backtesting, stress testing, VaR estimation
\item Address regulatory requirements: explainability, model validation, governance
\item Understand evaluation metrics specific to finance (Sharpe ratio, maximum drawdown, win rate)
\end{enumerate}

\section{Market Data and Time Series Forecasting}
\label{sec:timeseries}

Financial time series (stock prices, exchange rates, commodities) are notoriously difficult to predict. They are non-stationary (mean and variance change over time) and driven by news, sentiment, and complex market dynamics.

\subsection{Time Series Characteristics}

\begin{definition}[Non-Stationary Time Series]
\label{def:nonstationarity}
A time series is non-stationary if its statistical properties (mean, variance, autocorrelation) change over time. Financial returns are approximately stationary (mean ≈ 0, variance varies); prices themselves are non-stationary.
\end{definition}

Deep learning faces several fundamental challenges when applied to financial time series. Non-stationarity is perhaps the most severe: models trained on 2020 data often fail completely in 2024 as market regimes evolve and statistical properties change. This is compounded by regime shifts, where market behavior changes suddenly and dramatically—the 2008 financial crisis, COVID-19 crash, and 2022 inflation shock all represented regime shifts that broke many quantitative models overnight. Models must continuously adapt to these changing conditions or risk catastrophic failure.

The limited data problem is unique to finance. Unlike computer vision with millions of labeled images or NLP with billions of text documents, financial markets provide only decades of historical data with relatively few independent samples. A model trained on 10 years of daily data has only 2,500 trading days—far fewer than the millions of examples typical in other domains. This scarcity makes overfitting a constant danger: with few samples and many parameters, models easily memorize noise rather than learning genuine patterns. Finally, look-ahead bias—accidentally using future information during training—is surprisingly easy to introduce and can make worthless models appear profitable in backtesting.

\subsection{Transformer-Based Time Series Models}

Transformers, originally designed for sequences, adapt well to time series:

\begin{definition}[Temporal Transformer for Time Series]
\label{def:temporaltransformer}
\begin{enumerate}
\item \textbf{Input:} Past L trading days of OHLCV (open, high, low, close, volume) data
\item \textbf{Embedding:} Project each day's features to d-dimensional space
\item \textbf{Position encoding:} Time-based position encodings; relative time differences matter
\item \textbf{Transformer encoder:} Self-attention allows model to weight recent vs. older prices
\item \textbf{Output:} Predict next day's close price or return
\end{enumerate}
\end{definition}

Transformers offer several key advantages for financial time series modeling. Parallelization enables processing the entire price history simultaneously, unlike RNNs which must process sequences step-by-step. This dramatically reduces training time and enables efficient use of modern GPU hardware. The attention mechanism naturally captures long-range dependencies, allowing the model to identify patterns like mean reversion over months or seasonal effects over years—relationships that RNNs struggle to learn due to vanishing gradients. Finally, attention weights provide interpretability by revealing which historical days most influenced each prediction, helping traders understand and trust model decisions.

\subsection{Addressing Non-Stationarity}

Several techniques help address non-stationarity in financial models. Differencing transforms the problem by modeling log-returns instead of raw prices—returns are approximately stationary with mean near zero, while prices exhibit clear trends and non-stationarity. Normalization standardizes features to zero mean and unit variance before feeding them to the model, preventing scale differences from dominating learning. Regime detection explicitly models market state changes, using different models or parameters for bull markets, bear markets, and crisis periods. Online learning continuously retrains models on recent data, weighting recent observations more heavily to adapt to current conditions. Finally, ensemble methods combine predictions from models trained on different time periods, providing robustness when any single model fails due to regime changes.

\subsection{Evaluation and Backtesting}

Standard accuracy metrics (RMSE, MAE) are misleading for trading. A model could predict prices with low RMSE but lose money trading.

Trading-specific metrics evaluate what actually matters for profitability. The Sharpe ratio measures risk-adjusted returns by dividing average return by volatility—higher values indicate better performance per unit of risk, with values above 1.0 considered good and above 2.0 exceptional. Maximum drawdown captures the largest peak-to-trough decline, critical for risk management since large drawdowns can trigger margin calls or investor redemptions regardless of long-term returns. Win rate measures the fraction of profitable trades, though a strategy can be profitable with low win rate if winning trades are much larger than losing trades. Profit factor divides gross profits by gross losses, with values above 1.5 indicating robust strategies. The Calmar ratio divides annualized return by maximum drawdown, rewarding strategies that generate returns without large drawdowns.

Rigorous backtesting requires several critical practices. Walk-forward validation prevents look-ahead bias by training on historical data and testing on subsequent periods, then rolling forward: train on year 1, test on year 2, retrain on years 1-2, test on year 3, and so on. This simulates realistic deployment where models only have access to past data. Transaction costs must include all real-world expenses: commissions, bid-ask spreads, and slippage (the difference between expected and actual execution prices). Market impact modeling accounts for how large orders move prices against the trader—a strategy that works with small positions may fail at scale. Regulatory requirements impose capital requirements, position limits, and reporting obligations that constrain real-world trading. Finally, out-of-sample testing on completely held-out recent data provides the ultimate validation before deployment, ensuring the model works on data it has never seen during development.

\section{Financial NLP}
\label{sec:financialnlp}

Markets are driven by information. News, earnings reports, and regulatory filings move prices. NLP extracts this information.

\subsection{Financial Domain Text}

Financial text comes in several distinct forms, each with unique characteristics and information content. Earnings calls provide transcripts of quarterly conference calls where company executives discuss results and answer analyst questions—forward guidance and management tone often signal future performance before it appears in financial statements. SEC filings include 10-K annual reports, 10-Q quarterly reports, and 8-K event disclosures, containing detailed financial and operational information required by regulators. News from Reuters, Bloomberg, and CNBC is highly time-sensitive, with price reactions occurring within seconds of publication. Analyst reports from investment banks provide buy, hold, or sell recommendations with significant influence on institutional investors. Finally, Twitter and social media capture retail sentiment, rumors, and market commentary that increasingly affects stock prices, particularly for retail-popular stocks.

\subsection{Information Extraction}

Information extraction from financial text involves several key tasks. Named entity recognition identifies companies, executives, and financial instruments mentioned in text, enabling tracking of which entities are discussed and in what context. Event extraction detects significant occurrences like merger and acquisition announcements, earnings surprises, and executive changes—events that typically trigger immediate market reactions. Sentiment analysis classifies text tone as positive, negative, or neutral, capturing market psychology and expectations. Relationship extraction identifies connections between entities, such as which company acquired whom or which companies compete in the same market, building knowledge graphs of corporate relationships.

Consider a news headline: ``Apple Announces Record Q4 Revenue; Beats Analyst Expectations.'' Information extraction would identify the company as Apple, classify the event as an earnings announcement, extract the metric as Q4 revenue, and determine the sentiment as positive based on the words ``record'' and ``beats expectations.'' The predicted market impact would be a likely stock price increase, as positive earnings surprises typically drive immediate buying. This structured information can then feed into trading algorithms that act on news before human traders can react.

\subsection{Sentiment Analysis for Trading}

Aggregate sentiment from multiple sources to predict price movements:

\begin{enumerate}
\item Collect news, social media, analyst sentiment from past hour
\item Compute aggregate sentiment score (weighted average)
\item If sentiment strongly positive and persistent, go long; strongly negative, go short
\item Backtest: Does this strategy beat a buy-and-hold baseline?
\end{enumerate}

Practical results: Simple sentiment strategies achieve 50--55\% win rate (barely above random), but with low transaction costs and diversification, can be profitable.

\section{Credit Modeling and Risk Management}
\label{sec:creditmodeling}

Credit scoring determines who gets loans and at what interest rate. Deep learning has improved credit modeling, but fairness and explainability are critical.

\subsection{Credit Risk Assessment}

Deep learning enables credit models to incorporate richer information than traditional approaches. Time series analysis processes payment history over years rather than just summary statistics, capturing patterns like seasonal payment behavior or gradual deterioration. Text analysis examines loan applications and borrower explanations, where unusual language patterns or inconsistencies may indicate fraud or misrepresentation. Network analysis identifies relationships between borrowers, detecting fraud rings where multiple applications share suspicious connections. Behavioral analysis tracks how borrowers interact with the application system—hesitation, multiple retries, or unusual navigation patterns can signal uncertainty or deception.

\subsection{Deep Learning for Credit}

\begin{definition}[Credit Default Prediction]
\label{def:creditdefault}
Given borrower features (income, credit history, collateral, payment patterns), predict probability of default within specified time horizon (12 months, 3 years).

Model: \(P(\text{default} = 1 \mid \text{features})\) = logistic(neural network)

Loss: Cross-entropy on default labels (highly imbalanced: 2--5\% default rate)
\end{definition}

Deep learning credit models employ several architectural components. Feature embedding encodes categorical variables like employment status and zip code as learned dense vectors, capturing semantic similarities (e.g., neighboring zip codes have similar embeddings). Temporal modules use LSTMs or transformers to process payment history sequences, learning patterns like improving or deteriorating payment behavior over time. Attention mechanisms identify which factors most influenced each decision, providing interpretability required for regulatory compliance. Regularization through L1/L2 penalties and dropout prevents overfitting, critical when default examples are scarce (typically 2-5% of applications).

\subsection{Fairness in Credit Decisions}

Regulatory requirements (Fair Credit Reporting Act, Equal Credit Opportunity Act) prohibit discrimination on protected attributes (race, gender, religion). Yet models trained on historical data inherit biases:

If minorities historically received higher interest rates, the model learns to predict higher risk for minorities.

Several approaches address fairness in credit decisions. Constraint-based fairness requires equal acceptance rates across demographic groups, enforced during training through constrained optimization. Adversarial debiasing adds a classifier that attempts to predict protected attributes from model predictions, then trains the main model to fool this classifier—if the adversary cannot predict race from the model's outputs, the model is not using race as a proxy. Continuous monitoring measures disparate impact in production, alerting when approval rates diverge across groups beyond acceptable thresholds. Threshold tuning adjusts decision boundaries separately for each demographic group to achieve equal approval rates, though this approach raises legal questions about differential treatment.

\subsection{Explainability for Credit Decisions}

When a loan is denied, applicants have the right to explanation. ``The AI said no'' is insufficient.

Several interpretability methods provide explanations for credit decisions. SHAP values decompose each prediction into contributions from individual features, showing exactly how much each factor (income, credit score, payment history) influenced the decision. Attention analysis reveals which factors the model weighted most heavily, providing insight into the decision process. Counterfactual explanations answer ``what if'' questions: ``If your income were \$10,000 higher, you would be approved''—giving applicants actionable guidance. Prototype examples show similar applicants who were approved, helping applicants understand what successful profiles look like and how their application compares.

\section{Risk Management and Regulatory Requirements}
\label{sec:riskmanagement}

Financial institutions are heavily regulated. Models used for trading or lending must be validated and auditable.

\subsection{Model Risk Management Framework}

Financial regulations require comprehensive model risk management. Model documentation must provide detailed specifications of the model architecture, training data, assumptions, and known limitations—sufficient for independent reviewers to understand and reproduce the model. Governance requires model review and approval by a risk committee before deployment, with ongoing oversight and periodic re-approval. Validation involves independent testing on data not used in development, performed by teams separate from model developers to ensure objectivity. Monitoring tracks ongoing performance in production, alerting when metrics degrade beyond acceptable thresholds. Backtesting compares historical predictions against actual outcomes to verify model accuracy and calibration. Stress testing evaluates performance under extreme market conditions like the 2008 financial crisis or COVID-19 crash, ensuring models remain safe during tail events.

\subsection{Value at Risk (VaR) Estimation}

VaR is the maximum loss expected at a given confidence level (e.g., 95\% VaR for a portfolio).

Deep learning provides a more sophisticated approach to VaR estimation. Rather than assuming returns follow a normal distribution (an assumption frequently violated in financial markets), neural networks or flow-based models learn the actual return distribution from data, capturing fat tails, skewness, and other non-normal characteristics. The model can then sample from this learned distribution or use quantile regression to directly estimate VaR at the desired confidence level (e.g., 95th or 99th percentile). This approach produces more accurate VaR estimates that better capture tail risk—the extreme losses that matter most for risk management but are poorly modeled by traditional parametric methods.

\section{Case Study: Fraud Detection System}
\label{sec:casefrauddetection}

A payment processor wants to detect fraudulent transactions in real-time.

\subsection{Problem Setup}

The fraud detection problem presents several challenges. The dataset contains 10 billion transactions annually with only 0.1% fraud rate, creating severe class imbalance with 10 million fraudulent transactions among 10 billion total. Latency requirements are strict—predictions must complete in under 100 milliseconds at checkout to avoid degrading user experience. The cost structure is asymmetric: false positives block legitimate transactions, creating user friction and potentially losing customers, while false negatives allow fraud losses. The system must handle massive scale, processing 100,000 transactions per second during peak shopping periods like Black Friday.

\subsection{Model Architecture}

The fraud detection architecture combines multiple components. Features include card ID, merchant information, transaction amount, location, time, and velocity metrics like transactions per card per hour. Embeddings encode high-cardinality categorical variables like card ID, merchant category, country, and device fingerprint as dense vectors. Temporal modeling uses RNNs or transformers to process the sequence of past 10 transactions for each card, learning patterns like normal spending behavior versus suspicious sequences. The output is a fraud probability score; transactions exceeding a threshold trigger additional verification like two-factor authentication or manual review.

\subsection{Results}

Offline evaluation measured performance on historical data. The key metric was recall at 1% false positive rate—the system achieved 60% fraud detection while blocking only 1% of legitimate transactions. ROC-AUC reached 0.94, indicating strong discrimination between fraudulent and legitimate transactions. These offline metrics suggested the model was ready for production deployment.

Online deployment results demonstrated real-world effectiveness. The system achieved 88% fraud detection rate, catching 88% of fraudulent transactions in production. The false positive rate was 0.8%, meaning only 0.8% of legitimate users were asked for additional verification. User friction remained minimal, with most two-factor authentication challenges completed in under 30 seconds. The business impact was substantial: \$200 million per year in prevented fraudulent charges, against \$50 million annual cost for model development, infrastructure, and monitoring—a 4x return on investment.

\section{Model Drift in Financial Systems}
\label{sec:financedrift}

Financial AI systems face unique drift challenges that distinguish them from other domains. The general framework for understanding and managing model drift is presented in Chapter~24; here we focus on finance-specific patterns.

\subsection{Finance-Specific Drift Patterns}

\textbf{Market regime shifts} cause sudden, fundamental changes in the relationships that models have learned. A model trained during a bull market may fail catastrophically during a bear market or financial crisis because the statistical relationships between features change qualitatively, not just quantitatively. The 2008 financial crisis, COVID-19 crash, and 2022 inflation shock all broke quantitative models that had not been exposed to comparable regimes. For example, a volatility model trained on 2015--2019 data predicted VIX below 20; in March 2020, VIX spiked to 80.

\textbf{Alpha decay} is unique to trading models: as more market participants discover and exploit a predictive signal, the signal weakens through market efficiency. A trading strategy that generates 5\% annual returns may see returns decay to 1--2\% within 12--18 months as competitors adopt similar approaches. The industry average alpha half-life is estimated at 2--3 years, with high-frequency strategies decaying within months.

\textbf{Adversarial fraud evolution} occurs because fraudsters actively adapt to detection models. Unlike natural drift where the data distribution shifts gradually, fraud patterns change deliberately and rapidly in response to detection. New fraud schemes---geographic shifts, velocity changes, amount manipulation, synthetic identities---can emerge within weeks of a model deployment, requiring continuous model updates.

\textbf{Regulatory drift} arises from changing compliance requirements (e.g., Basel III to Basel IV, new stress testing mandates, interest rate regime changes) that alter the features, constraints, or objectives of financial models. When the Federal Reserve raised rates from 0\% to 5\% in 2022--2023, models trained in the zero-rate environment required complete recalibration.

\textbf{Economic cycle drift} affects credit models as borrower behavior differs between expansion and recession. A mortgage model trained on 2010--2019 expansion data predicted 2\% defaults; during the 2020 recession, actual defaults reached 5\%, causing hundreds of millions in unexpected losses.

\subsection{Monitoring and Adaptation}

Financial drift detection requires domain-specific metrics: Sharpe ratio and drawdown for trading, precision/recall for fraud, calibration accuracy for credit. Use walk-forward validation continuously and implement regime detection (hidden Markov models or clustering on volatility patterns) to trigger model switching. For fraud detection, conduct regular adversarial red-teaming. Retrain aggressively: trading models weekly, fraud models daily, credit models quarterly. See Chapter~24 for the general continuous learning framework.

\begin{keypoint}
Financial drift patterns are distinguished by their adversarial nature (fraud), reflexivity (alpha decay), and regime-dependent behavior (market shifts). Standard drift detection approaches from Chapter~24 must be augmented with regime-aware monitoring and adversarial robustness testing specific to financial applications.
\end{keypoint}

\section{Exercises}

\begin{exercise}
Build a time series model for stock price prediction. Train on 5 years of historical S\&P 500 data. Evaluate using Sharpe ratio and maximum drawdown in addition to RMSE. Can you beat a buy-and-hold baseline?
\end{exercise}

\begin{exercise}
Extract events from earnings call transcripts. Identify mentions of: new products, executive changes, competitive threats, guidance changes. Build a classifier to predict stock price movement after earnings announcement.
\end{exercise}

\begin{exercise}
Design a fair credit scoring model. Start with a baseline model that uses standard features. Measure disparate impact (difference in approval rates across demographic groups). Apply debiasing techniques. Can you reduce disparate impact while maintaining predictive power?
\end{exercise}

\section{Solutions}

Full solutions for all exercises are available at \url{https://deeplearning.hofkensvermeulen.be}.

\begin{solution}
\textbf{Exercise 1: Stock Price Prediction}

\itshape Data:
\begin{itemize}
\item S\&P 500 daily OHLCV data, 2018--2023
\item 1260 trading days per year; 6300 days total
\item Train on 2018--2021 (4 years), test on 2022--2023 (2 years)
\end{itemize}

\itshape Model:
\begin{itemize}
\item Input: 60-day rolling window of returns
\item Transformer: 2 layers, 4 heads
\item Output: Predict next-day return
\item Loss: MSE on returns (not prices)
\end{itemize}

\itshape Evaluation:
\begin{itemize}
\item RMSE: 0.015 (1.5\% prediction error)
\item But is this useful for trading?
\item Sharpe ratio of predictions: 0.3 (very low; barely profitable after costs)
\item Maximum drawdown: -18\% (high risk)
\item Comparison: Buy-and-hold S\&P 500 2022-2023 achieved Sharpe ratio 0.2, max drawdown -20\%
\item Conclusion: Model slightly outperforms but not statistically significant; overfitting likely
\end{itemize}

\itshape Improvements:
\begin{itemize}
\item More features: Volume, sector rotation, VIX (volatility index)
\item Regime detection: Different models for bull vs. bear markets
\item Ensemble: Combine with technical indicators, sentiment models
\item Transaction costs: Even profitable strategies lose money after commissions/spreads
\end{itemize}

Bottom line: Stock prediction is hard; even slight edge requires careful implementation and is fragile to market changes.
\end{solution}

\begin{solution}
\textbf{Exercise 2: Event Extraction from Earnings Calls}

\itshape Data collection:
\begin{itemize}
\item Source: Seeking Alpha, company investor relations websites
\item Dataset: 200 earnings call transcripts with stock price movements next day
\item Annotation: For each call, label event type and price movement direction
\end{itemize}

\itshape Model:
\begin{itemize}
\item Pre-processing: Split transcript into speaker turns (management vs. analyst)
\item NER + relation extraction: Identify company names, executives, products
\item Event detection: Multi-class classification for each sentence: new product, executive change, etc.
\item Sentiment: Overall call sentiment (positive/negative/neutral)
\end{itemize}

\itshape Evaluation:
\begin{itemize}
\item Event extraction F1: 0.75 (reasonable; humans also disagree on event interpretation)
\item Sentiment classification: 0.82 accuracy
\item Price movement prediction: Train a model on extracted events + sentiment → next day return
\item Results: 55\% accuracy (barely above 50\% random for binary up/down)
\item Reason: Stock movements driven by many factors; earnings data alone insufficient
\end{itemize}

\itshape Practical use:
Despite low accuracy, events provide valuable context for traders. Supplemented with other signals, earnings event extraction improves trading decisions.
\end{solution}

\begin{solution}
\textbf{Exercise 3: Fair Credit Scoring}

\itshape Baseline model:
\begin{itemize}
\item Data: 50K applicants, 5\% default rate
\item Features: Income, credit score, debt-to-income ratio, employment status, zip code
\item Model: Logistic regression
\item Approval rate: 80\% overall; 85\% white applicants, 70\% black applicants (disparate impact)
\end{itemize}

\itshape Fairness metrics:
\begin{itemize}
\item Disparate impact ratio: 70\% / 85\% = 0.82 (rule of 4/5 threshold is 0.80)
\item Just barely legal, but problematic
\end{itemize}

\itshape Debiasing approach 1: Adversarial debiasing
\begin{itemize}
\item Main model: Predict default
\item Adversary: Predict race from model's prediction
\item Train: Minimize default loss + maximize adversary's confusion about race
\item Result: Disparate impact improved to 0.92; default prediction accuracy maintained at 0.72 AUC
\end{itemize}

\itshape Debiasing approach 2: Threshold tuning
\begin{itemize}
\item Use different acceptance thresholds for different demographics
\item Adjust to ensure equal approval rates: 80\% for all groups
\item Trade-off: Default rates become slightly different (78\% default prediction accuracy for white, 75\% for black)
\item Acceptable if default prediction accuracy reasonably maintained
\end{itemize}

\itshape Conclusion:
Fairness improvements are possible but often involve accuracy trade-offs or legal complexity. Regulatory guidance evolving; best practice is to measure disparate impact and document decisions transparently.
\end{solution}
