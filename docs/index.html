<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning and Transformers - A Graduate-Level Course</title>
    <meta name="description" content="A comprehensive graduate-level textbook on deep learning and transformer architectures with mathematical rigor and practical implementation guidance.">
    <meta name="theme-color" content="#2c3e50">
    <link rel="stylesheet" href="css/style.css">
    <link rel="manifest" href="manifest.json">
    <link rel="icon" type="image/svg+xml" href="icons/icon.svg">
    <link rel="apple-touch-icon" href="icons/icon.svg">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>Deep Learning and Transformers</h1>
        <p style="font-size: 1.2em; text-align: center; color: #666;">
            Theory, Mathematics, and Implementation<br>
            <em>A Graduate-Level Course</em>
        </p>
    </header>

    <main>
        <div class="toc">
            <h2>Table of Contents</h2>

            <h3>Front Matter</h3>
            <ul>
                <li><a href="chapters/preface.html">Preface</a></li>
                <li><a href="chapters/notation.html">Notation and Conventions</a></li>
            </ul>

            <h3>Part I: Mathematical Foundations</h3>
            <ul>
                <li><a href="chapters/chapter01_linear_algebra.html">Chapter 1: Linear Algebra for Deep Learning</a></li>
                <li><a href="chapters/chapter02_calculus_optimization.html">Chapter 2: Calculus and Optimization</a></li>
                <li><a href="chapters/chapter03_probability_information.html">Chapter 3: Probability and Information Theory</a></li>
            </ul>

            <h3>Part II: Neural Network Fundamentals</h3>
            <ul>
                <li><a href="chapters/chapter04_feedforward_networks.html">Chapter 4: Feed-Forward Neural Networks</a></li>
                <li><a href="chapters/chapter05_convolutional_networks.html">Chapter 5: Convolutional Neural Networks</a></li>
                <li><a href="chapters/chapter06_recurrent_networks.html">Chapter 6: Recurrent Neural Networks</a></li>
            </ul>

            <h3>Part III: Attention Mechanisms</h3>
            <ul>
                <li><a href="chapters/chapter07_attention_fundamentals.html">Chapter 7: Attention Mechanisms: Fundamentals</a></li>
                <li><a href="chapters/chapter08_self_attention.html">Chapter 8: Self-Attention and Multi-Head Attention</a></li>
                <li><a href="chapters/chapter09_attention_variants.html">Chapter 9: Attention Variants and Mechanisms</a></li>
            </ul>

            <h3>Part IV: Transformer Architecture</h3>
            <ul>
                <li><a href="chapters/chapter10_transformer_model.html">Chapter 10: The Transformer Model</a></li>
                <li><a href="chapters/chapter11_training_transformers.html">Chapter 11: Training Transformers</a></li>
                <li><a href="chapters/chapter12_computational_analysis.html">Chapter 12: Computational Analysis</a></li>
            </ul>

            <h3>Part V: Modern Transformer Variants</h3>
            <ul>
                <li><a href="chapters/chapter13_bert.html">Chapter 13: BERT and Bidirectional Models</a></li>
                <li><a href="chapters/chapter14_gpt.html">Chapter 14: GPT and Autoregressive Models</a></li>
                <li><a href="chapters/chapter15_t5_bart.html">Chapter 15: T5, BART, and Encoder-Decoder Models</a></li>
                <li><a href="chapters/chapter16_efficient_transformers.html">Chapter 16: Efficient Transformers</a></li>
            </ul>

            <h3>Part VI: Advanced Topics</h3>
            <ul>
                <li><a href="chapters/chapter17_vision_transformers.html">Chapter 17: Vision Transformers</a></li>
                <li><a href="chapters/chapter18_multimodal_transformers.html">Chapter 18: Multimodal Transformers</a></li>
                <li><a href="chapters/chapter19_long_context.html">Chapter 19: Long Context and Memory</a></li>
                <li><a href="chapters/chapter20_pretraining_strategies.html">Chapter 20: Pretraining Strategies</a></li>
            </ul>

            <h3>Part VII: Practical Implementation</h3>
            <ul>
                <li><a href="chapters/chapter21_pytorch_implementation.html">Chapter 21: PyTorch Implementation</a></li>
                <li><a href="chapters/chapter22_hardware_optimization.html">Chapter 22: Hardware Optimization</a></li>
                <li><a href="chapters/chapter23_best_practices.html">Chapter 23: Best Practices</a></li>
            </ul>
        </div>

        <div style="margin-top: 3em; padding: 2em; background-color: #f8f9fa; border-radius: 5px;">
            <h2>About This Book</h2>
            <p>
                This textbook provides a comprehensive, graduate-level treatment of deep learning and transformer architectures.
                It emphasizes mathematical rigor while maintaining practical relevance, with complete derivations,
                concrete numerical examples, and implementation guidance.
            </p>
            <p>
                <strong>Key Features:</strong>
            </p>
            <ul>
                <li>Complete mathematical derivations with geometric intuition</li>
                <li>Explicit dimension tracking for all operations</li>
                <li>Concrete numerical examples from real models (BERT, GPT, etc.)</li>
                <li>Implementation notes and code examples</li>
                <li>Progressive complexity building from foundations</li>
            </ul>
        </div>
    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
        <p>
            <a href="https://github.com/jhammant/deep-learning-textbook">View on GitHub</a>
        </p>
    </footer>

    <script src="js/main.js"></script>
    <script>
        // Register service worker for offline support
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', () => {
                navigator.serviceWorker.register('service-worker.js')
                    .then(registration => {
                        console.log('ServiceWorker registered:', registration.scope);
                    })
                    .catch(error => {
                        console.log('ServiceWorker registration failed:', error);
                    });
            });
        }
    </script>
</body>
</html>
