<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning and Transformers</title>
    <style>
        body {
            font-family: Georgia, serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #2c3e50;
            margin-top: 30px;
        }
        h3 {
            color: #34495e;
            margin-top: 20px;
        }
        ul {
            list-style: none;
            padding-left: 0;
        }
        li {
            margin: 10px 0;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .toc {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 5px;
            border-left: 4px solid #3498db;
        }
        .about {
            margin-top: 30px;
            padding: 20px;
            background-color: #f8f9fa;
            border-radius: 5px;
        }
        footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 2px solid #ddd;
            text-align: center;
            color: #666;
        }
    </style>
</head>
<body>
    <h1>Deep Learning and Transformers</h1>
    <p style="text-align: center; font-size: 1.2em; color: #666;">
        Theory, Mathematics, and Implementation<br>
        <em>A Graduate-Level Course</em>
    </p>

    <div class="toc">
        <h2>Table of Contents</h2>
        
        <h3>Front Matter</h3>
        <ul>
            <li>ðŸ“„ <a href="chapters/preface.html">Preface</a></li>
            <li>ðŸ“„ <a href="chapters/notation.html">Notation and Conventions</a></li>
        </ul>

        <h3>Part I: Mathematical Foundations</h3>
        <ul>
            <li>ðŸ“– <a href="chapters/chapter01_linear_algebra.html">Chapter 1: Linear Algebra for Deep Learning</a></li>
            <li>ðŸ“– <a href="chapters/chapter02_calculus_optimization.html">Chapter 2: Calculus and Optimization</a></li>
            <li>ðŸ“– <a href="chapters/chapter03_probability_information.html">Chapter 3: Probability and Information Theory</a></li>
        </ul>

        <h3>Part II: Neural Network Fundamentals</h3>
        <ul>
            <li>ðŸ“– <a href="chapters/chapter04_feedforward_networks.html">Chapter 4: Feed-Forward Neural Networks</a></li>
            <li>ðŸ“– <a href="chapters/chapter05_convolutional_networks.html">Chapter 5: Convolutional Neural Networks</a></li>
            <li>ðŸ“– <a href="chapters/chapter06_recurrent_networks.html">Chapter 6: Recurrent Neural Networks</a></li>
        </ul>

        <h3>Part III: Attention Mechanisms</h3>
        <ul>
            <li>ðŸ“– <a href="chapters/chapter07_attention_fundamentals.html">Chapter 7: Attention Mechanisms: Fundamentals</a></li>
            <li>ðŸ“– <a href="chapters/chapter08_self_attention.html">Chapter 8: Self-Attention and Multi-Head Attention</a></li>
            <li>ðŸ“– <a href="chapters/chapter09_attention_variants.html">Chapter 9: Attention Variants and Mechanisms</a></li>
        </ul>

        <h3>Part IV: Transformer Architecture</h3>
        <ul>
            <li>ðŸ“– <a href="chapters/chapter10_transformer_model.html">Chapter 10: The Transformer Model</a></li>
            <li>ðŸ“– <a href="chapters/chapter11_training_transformers.html">Chapter 11: Training Transformers</a></li>
            <li>ðŸ“– <a href="chapters/chapter12_computational_analysis.html">Chapter 12: Computational Analysis</a></li>
        </ul>

        <h3>Part V: Modern Transformer Variants</h3>
        <ul>
            <li>ðŸ“– <a href="chapters/chapter13_bert.html">Chapter 13: BERT</a></li>
            <li>ðŸ“– <a href="chapters/chapter14_gpt.html">Chapter 14: GPT</a></li>
            <li>ðŸ“– <a href="chapters/chapter15_t5_bart.html">Chapter 15: T5 and BART</a></li>
            <li>ðŸ“– <a href="chapters/chapter16_efficient_transformers.html">Chapter 16: Efficient Transformers</a></li>
        </ul>

        <h3>Part VI: Advanced Topics</h3>
        <ul>
            <li>ðŸ“– <a href="chapters/chapter17_vision_transformers.html">Chapter 17: Vision Transformers</a></li>
            <li>ðŸ“– <a href="chapters/chapter18_multimodal_transformers.html">Chapter 18: Multimodal Transformers</a></li>
            <li>ðŸ“– <a href="chapters/chapter19_long_context.html">Chapter 19: Long Context Handling</a></li>
            <li>ðŸ“– <a href="chapters/chapter20_pretraining_strategies.html">Chapter 20: Pretraining Strategies</a></li>
        </ul>

        <h3>Part VII: Practical Implementation</h3>
        <ul>
            <li>ðŸ“– <a href="chapters/chapter21_pytorch_implementation.html">Chapter 21: PyTorch Implementation</a></li>
            <li>ðŸ“– <a href="chapters/chapter22_hardware_optimization.html">Chapter 22: Hardware Optimization</a></li>
            <li>ðŸ“– <a href="chapters/chapter23_best_practices.html">Chapter 23: Best Practices</a></li>
        </ul>
    </div>

    <div class="about">
        <h2>About This Book</h2>
        <p>
            This textbook provides a comprehensive, graduate-level treatment of deep learning and transformer architectures.
            It emphasizes mathematical rigor while maintaining practical relevance, with complete derivations,
            concrete numerical examples, and implementation guidance.
        </p>
        <p><strong>Key Features:</strong></p>
        <ul>
            <li>âœ“ Complete mathematical derivations with geometric intuition</li>
            <li>âœ“ Explicit dimension tracking for all operations</li>
            <li>âœ“ Concrete numerical examples from real models (BERT, GPT, etc.)</li>
            <li>âœ“ Implementation notes and code examples</li>
            <li>âœ“ Progressive complexity building from foundations</li>
        </ul>
    </div>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
        <p>
            <a href="https://github.com/PietKonijn007/deep-learning-transformers-textbook">View on GitHub</a>
        </p>
    </footer>

    <script>
        console.log('Page loaded successfully!');
    </script>
</body>
</html>
