<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 5: Convolutional Neural Networks - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../css/style.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                vh: '{\\mathbf{h}}',
                vw: '{\\mathbf{w}}',
                vb: '{\\mathbf{b}}',
                vq: '{\\mathbf{q}}',
                vk: '{\\mathbf{k}}',
                vv: '{\\mathbf{v}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mQ: '{\\mathbf{Q}}',
                mK: '{\\mathbf{K}}',
                mV: '{\\mathbf{V}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mU: '{\\mathbf{U}}',
                mM: '{\\mathbf{M}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
    </nav>

    <main>
        <h1>Convolutional Neural Networks</h1>

<h2>Chapter Overview</h2>

<p>Convolutional Neural Networks (CNNs) revolutionized computer vision by exploiting spatial structure. This chapter develops convolution operations, pooling, and modern CNN architectures including ResNet.</p>

<h3>Learning Objectives</h3>

<ol>
    <li>Understand convolution operations and compute output dimensions
    <li>Design CNN architectures with appropriate pooling and stride
    <li>Understand translation equivariance
    <li>Implement modern CNN architectures (ResNet, VGG)
</ol>

<h2>Convolution Operation</h2>

<div class="definition"><strong>Definition:</strong> 
For input $\mX \in \R^{H \times W}$ and kernel $\mK \in \R^{k_h \times k_w}$:
<div class="equation">
$$
(\mX \star \mK)_{i,j} = \sum_{m=0}^{k_h-1} \sum_{n=0}^{k_w-1} \mX_{i+m, j+n} \cdot \mK_{m,n}
$$
</div>
</div>

<div class="example"><strong>Example:</strong> 
Input $4\times4$, kernel $3\times3$ (edge detector), output $2\times2$. Computing first position: sum of element-wise products gives edge response.
</div>

<h3>Output Dimensions</h3>

<div class="theorem"><strong>Theorem:</strong> 
For input size $H \times W$, kernel $k_h \times k_w$, padding $p$, stride $s$:
<div class="equation">
$$
H_{\text{out}} = \left\lfloor \frac{H + 2p - k_h}{s} \right\rfloor + 1
$$
</div>
</div>

<h2>Multi-Channel Convolutions</h2>

<div class="definition"><strong>Definition:</strong> 
For input $\mathbf{X} \in \R^{C_{\text{in}} \times H \times W}$ with $C_{\text{out}}$ output channels:
<div class="equation">
$$
\mathbf{Y}^{(i)} = \sum_{c=1}^{C_{\text{in}}} \mathbf{X}^{(c)} \star \mathbf{K}^{(i,c)} + b^{(i)}
$$
</div>
</div>

<div class="example"><strong>Example:</strong> 
Input: $\mathbf{X} \in \R^{3 \times 224 \times 224}$. Conv layer: 64 filters $3\times3$, stride 1, padding 1.

<p>Parameters: $64 \times 3 \times 3 \times 3 + 64 = 1{,}792$</p>

<p>Output: $\mathbf{Y} \in \R^{64 \times 224 \times 224}$</p>

<p>Compare to fully-connected: $\approx 483$ billion parameters!
</div>

<div class="keypoint">
Convolution provides: (1) Parameter sharing, (2) Local connectivity, (3) Translation equivariance. Massive parameter reduction compared to fully-connected layers.
</div>

<h2>Pooling Layers</h2>

<div class="definition"><strong>Definition:</strong> 
For window $k \times k$ and stride $s$:
<div class="equation">
$$
\text{MaxPool}(\mathbf{X})_{i,j} = \max_{m,n \in \text{window}} \mathbf{X}_{si+m, sj+n}
$$
</div>
</div>

<p>Pooling reduces spatial dimensions, increases receptive field, and provides translation invariance.</p>

<h2>Classic Architectures</h2>

<h3>VGG-16 (2014)</h3>

<p>Deep network with small $3\times3$ filters. Pattern: $[\text{Conv}3\times3]^n \to \text{MaxPool} \to \text{Double channels}$</p>

<p>Total: 138 million parameters</p>

<h3>ResNet (2015)</h3>

<div class="definition"><strong>Definition:</strong> 
Learn residual:
<div class="equation">
$$
\mathbf{y} = \mathcal{F}(\mathbf{x}) + \mathbf{x}
$$
</div>
</div>

<p>ResNet-50: 25.6M parameters, enables training 100+ layer networks.</p>

<div class="keypoint">
Residual connections enable extremely deep networks by allowing gradients to flow through skip connections. Analogous to skip connections in transformers.
</div>

<h2>Batch Normalization</h2>

<div class="definition"><strong>Definition:</strong> 
For mini-batch, normalize each feature:
<div class="equation">
$$\begin{align}
\hat{\mathbf{x}}_i &= \frac{\mathbf{x}_i - \mu_{\mathcal{B}}}{\sqrt{\sigma^2_{\mathcal{B}} + \epsilon}} \\
\mathbf{y}_i &= \gamma \hat{\mathbf{x}}_i + \beta
\end{align}$$
</div>
where $\gamma, \beta$ are learnable.
</div>

<p>Benefits: Reduces covariate shift, allows higher learning rates, acts as regularization.</p>

<h2>Exercises</h2>

<p>\begin{exercise}
For $32\times32\times3$ input, compute dimensions after: Conv(64, $5\times5$, s=1, p=2), MaxPool($2\times2$, s=2), Conv(128, $3\times3$, s=1, p=1), MaxPool($2\times2$, s=2). Count parameters.
</div>

<p>\begin{exercise}
Show two $3\times3$ convolutions equal one $5\times5$ receptive field. Compare parameter counts.
</div>

<p>\begin{exercise}
Design CNN for CIFAR-10 with 3 blocks, channels [64, 128, 256]. Calculate total parameters.
</div>
        
        <div class="chapter-nav">
  <a href="chapter04_feedforward_networks.html">‚Üê Chapter 4: Feed-Forward Neural Networks</a>
  <a href="../index.html">üìö Table of Contents</a>
  <a href="chapter06_recurrent_networks.html">Chapter 6: Recurrent Neural Networks ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
