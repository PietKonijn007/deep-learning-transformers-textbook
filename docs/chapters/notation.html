<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notation and Conventions - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../css/style.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
    </nav>

    <main>
        <h1>Notation and Conventions</h1>

<p>This book adopts consistent notation throughout to enhance readability and comprehension.</p>

<h2>General Mathematical Notation</h2>

<table>
<tr><th><strong>Symbol</strong></th><th><strong>Meaning</strong></th></tr>
<tr><td>$a, b, c$</td><td>Scalars (lowercase italic)</td></tr>
<tr><td>$n, m, d$</td><td>Integer scalars (dimensions, indices)</td></tr>
<tr><td>$\vx, \vy, \vz$</td><td>Vectors (lowercase bold)</td></tr>
<tr><td>$\mA, \mB, \mC$</td><td>Matrices (uppercase bold)</td></tr>
<tr><td>$\mathcal{X}, \mathcal{D}$</td><td>Sets (uppercase calligraphic)</td></tr>
<tr><td>$f, g, h$</td><td>Functions (lowercase italic)</td></tr>
<tr><td>$\R, \N, \Z, \C$</td><td>Number sets (blackboard bold)</td></tr>
</table>

<h2>Linear Algebra</h2>

<table>
<tr><th><strong>Symbol</strong></th><th><strong>Meaning</strong></th></tr>
<tr><td>$\vx \in \R^n$</td><td>Vector $\vx$ with $n$ components</td></tr>
<tr><td>$\mA \in \R^{m \times n}$</td><td>Matrix $\mA$ with $m$ rows and $n$ columns</td></tr>
<tr><td>$a_{i,j}$ or $[\mA]_{i,j}$</td><td>Element in row $i$, column $j$ of matrix $\mA$</td></tr>
<tr><td>$\mA\transpose$</td><td>Transpose of matrix $\mA$</td></tr>
<tr><td>$\mA^{-1}$</td><td>Inverse of matrix $\mA$</td></tr>
<tr><td>$\mA \mB$</td><td>Matrix multiplication</td></tr>
<tr><td>$\mA \odot \mB$</td><td>Element-wise (Hadamard) product</td></tr>
<tr><td>$\vx \transpose \vy$</td><td>Dot product of vectors $\vx$ and $\vy$</td></tr>
<tr><td>$\norm{\vx}_2$</td><td>Euclidean (L2) norm</td></tr>
<tr><td>$\norm{\vx}_1$</td><td>L1 norm</td></tr>
<tr><td>$\norm{\mA}_F$</td><td>Frobenius norm of matrix $\mA$</td></tr>
<tr><td>$\text{tr}(\mA)$</td><td>Trace of matrix $\mA$</td></tr>
<tr><td>$\det(\mA)$</td><td>Determinant of matrix $\mA$</td></tr>
<tr><td>$\mI$ or $\mI_n$</td><td>Identity matrix</td></tr>
</table>

<h2>Deep Learning Specific</h2>

<table>
<tr><th><strong>Symbol</strong></th><th><strong>Meaning</strong></th></tr>
<tr><td>$\vx^{(i)}$</td><td>$i$-th training example</td></tr>
<tr><td>$\vx_t$</td><td>Input at time step $t$</td></tr>
<tr><td>$\vh^{(\ell)}$</td><td>Hidden state at layer $\ell$</td></tr>
<tr><td>$\mW^{(\ell)}$</td><td>Weight matrix at layer $\ell$</td></tr>
<tr><td>$\vb^{(\ell)}$</td><td>Bias vector at layer $\ell$</td></tr>
<tr><td>$\sigma(\cdot)$</td><td>Activation function (generic)</td></tr>
<tr><td>$\text{ReLU}(x)$</td><td>Rectified Linear Unit: $\max(0, x)$</td></tr>
<tr><td>$\text{softmax}(\vx)$</td><td>Softmax function</td></tr>
<tr><td>$N$ or $B$</td><td>Batch size</td></tr>
<tr><td>$d_{\text{model}}$</td><td>Model dimension</td></tr>
<tr><td>$d_k, d_v$</td><td>Dimension of keys and values</td></tr>
<tr><td>$h$</td><td>Number of attention heads</td></tr>
<tr><td>$L$</td><td>Number of layers</td></tr>
<tr><td>$V$</td><td>Vocabulary size</td></tr>
<tr><td>$n$ or $T$</td><td>Sequence length</td></tr>
<tr><td>$\eta$</td><td>Learning rate</td></tr>
</table>

<h2>Dimension Conventions</h2>

<p>Throughout this book, we explicitly annotate dimensions:
<ul>
    <li>For $\mW \in \R^{m \times n}$: $m$ rows, $n$ columns
    <li>Batch dimensions listed first: $\mX \in \R^{B \times n \times d}$
    <li>Superscripts for layer indices: $\vh^{(\ell)}$
    <li>Subscripts for time/position indices: $\vx_t$
</ul>
        
        <div class="chapter-nav">
  <a href="preface.html">‚Üê Preface</a>
  <a href="../index.html">üìö Table of Contents</a>
  <a href="chapter01_linear_algebra.html">Chapter 1: Linear Algebra for Deep Learning ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
