\chapter{Code as a Domain: Code LLMs and Developer Tooling}
\label{chap:codelanguage}

\section*{Chapter Overview}

Programming languages represent a rich and highly structured domain for deep learning. Unlike natural language, code has formal syntax, executable semantics, and built-in evaluation mechanisms. This chapter explores how transformers trained on code repositories have revolutionized developer productivity through code completion, generation, and analysis. We examine code-specific pre-training, from tokenization to context window design, and cover practical applications: IDE copilots, automated testing, refactoring, and repository-wide code understanding. Unlike natural language applications where ``hallucination'' produces grammatically correct but false text, code hallucinations produce syntactically correct but incorrect implementations---a property that enables testing and automated quality control.

\section*{Learning Objectives}

\begin{enumerate}
\item Understand how programming languages differ from natural language for LLM training
\item Design and implement code-specific tokenization and vocabulary
\item Train or fine-tune models on code repositories at scale
\item Develop IDE copilots and code completion interfaces
\item Build static analysis and refactoring tools using code LLMs
\item Implement automated testing and correctness verification for model outputs
\item Address dataset licensing, attribution, and ethical concerns
\end{enumerate}

\section{Code-Specific Pre-Training}
\label{sec:codepretraining}

Code as a training signal differs fundamentally from natural language, and understanding these differences is crucial for building effective code models. While natural language exhibits ambiguity---the same sentence can have multiple valid interpretations---code must be unambiguous to execute. A Python function either runs correctly or throws an error; there's no middle ground. This property---the ability to test code for correctness---provides a powerful training signal unavailable for general language models.

The business implications are significant. When a language model hallucinates a fact about history, it's embarrassing but not immediately verifiable. When a code model generates incorrect code, you can run it and see that it fails. This executability means code models can be evaluated objectively, their outputs can be automatically tested, and developers can quickly determine if a suggestion is useful. This makes code generation one of the most practical applications of large language models, with measurable productivity gains.

\subsection{Tokenization and Vocabulary}

Standard natural language tokenizers (BPE, WordPiece) are suboptimal for code, and understanding why reveals important insights about the structure of programming languages. Natural language tokenizers are designed to handle the statistical properties of human language: common words appear frequently, rare words can be broken into subwords, and word boundaries are relatively clear.

Programming languages have different statistical properties. Keywords like \texttt{def}, \texttt{class}, \texttt{if}, and \texttt{for} appear extremely frequently and should never be split. Multi-character operators like \texttt{==}, \texttt{->}, and \texttt{**} are atomic units with specific meanings; splitting them into individual characters destroys their semantics. Variable names follow conventions (\texttt{camelCase}, \texttt{snake\_case}) that should be preserved. And identifiers can be arbitrarily long and unique, creating a long-tail distribution that challenges vocabulary-based tokenization.

A tokenizer trained on natural language might split \texttt{initialize\_database} into \texttt{[init, ial, ize, \_data, base]}, losing the semantic structure. A code-specific tokenizer learns to preserve meaningful units: \texttt{[initialize, \_, database]} or even \texttt{[initialize\_database]} if the pattern is common enough.

\begin{definition}[Code Tokenization]
\label{def:codetokenization}

A BPE tokenizer trained on a diverse corpus of programming languages learns to preserve the semantic structure of code by treating meaningful units as single tokens. This approach differs fundamentally from natural language tokenization in several important ways.

Keywords such as \texttt{def}, \texttt{class}, \texttt{if}, and \texttt{for} remain unmerged as single tokens. These are the fundamental building blocks of the language; splitting them would be analogous to splitting the word ``the'' in English, destroying their semantic meaning. The tokenizer learns through frequency analysis that these keywords appear so often and in such consistent contexts that they should never be decomposed.

Multi-character operators like \texttt{==}, \texttt{->}, and \texttt{**} are treated as atomic operations. This is crucial because the operator \texttt{==} (equality test) has a completely different meaning than two separate \texttt{=} tokens (assignment). Preserving these as single tokens helps the model understand operator semantics directly, rather than having to learn that certain character sequences form operators. The tokenizer identifies these patterns during training by observing that these character combinations always appear together and function as single syntactic units.

Frequently-used variable names like \texttt{self}, \texttt{args}, and \texttt{kwargs} merge into single tokens. These are conventions in Python that appear across millions of functions; treating them as units helps the model learn idiomatic patterns more efficiently. When the model sees \texttt{self} as a single token, it can more easily learn that this token typically appears as the first parameter of instance methods and is used to access instance attributes.

Common patterns and idioms like \texttt{self.}, \texttt{if \_\_name\_\_}, and \texttt{import *} often merge into single tokens. These are multi-token patterns that appear together so frequently that treating them as units improves both compression and understanding. The tokenizer discovers these patterns through statistical analysis of co-occurrence: when certain tokens always or nearly always appear together, merging them reduces the sequence length and makes the pattern more salient to the model.

The key insight is that code tokenization should respect the syntactic and semantic structure of programming languages, not just optimize for compression. A good code tokenizer makes the model's job easier by presenting code in meaningful chunks that align with how programmers think about code structure. This alignment between tokenization and semantic units allows the model to learn more efficiently and generate more coherent code.
\end{definition}

The vocabulary size for code tokenizers is typically 32,000--50,000 tokens, comparable to natural language models. However, the token distribution differs significantly: code has higher entropy (more unique tokens appear frequently), reducing compression compared to natural language. A natural language model might compress English text to 0.7 tokens per word on average; a code model might only achieve 1.2 tokens per word due to the diversity of identifiers and the precision required for operators.

\subsection{Context Window and Code Understanding}

Code understanding benefits from longer context windows, and the reasons are both technical and practical. A function definition might span 50 lines; understanding its behavior requires reading the entire function plus imported modules and class definitions. A class might span hundreds of lines; understanding a method requires knowing the class's state and other methods. A module might import dozens of dependencies; understanding a function call requires knowing what those dependencies provide.

Modern code models use context windows of 2,048--8,192 tokens, allowing models to see entire functions or small files. This is a significant advantage over earlier models with 512-token windows, which could barely fit a single function. The business impact is tangible: with longer context, models can generate more accurate completions, understand cross-function dependencies, and provide better suggestions for refactoring.

For example, consider a developer writing a new method in a class. With a 512-token context, the model might only see the method signature and a few lines of surrounding code. With a 4,096-token context, the model can see the entire class, understand the class's purpose and state, and generate a method implementation that correctly uses the class's attributes and other methods. The difference in suggestion quality is dramatic.

Positional encodings must handle long contexts effectively. Relative position biases (e.g., T5-style) often outperform absolute sinusoidal encodings for code, as they capture the importance of nearby tokens (e.g., variable definitions a few lines above a usage) independent of absolute position. In code, the relative distance between a variable definition and its usage matters more than their absolute positions in the file. A variable defined 5 lines above is relevant regardless of whether it's at line 10 or line 1000.

\subsection{Pre-Training Data and Curation}

High-quality code datasets are essential, and data quality matters more for code than for natural language. Bad code in the training set teaches the model bad patterns; the model will suggest buggy, inefficient, or insecure code. Data curation is not just about quantity---it's about ensuring the model learns from good examples.

Models are trained on public GitHub repositories, academic code, and project-specific codebases. The challenge is that GitHub contains code of wildly varying quality: production-grade libraries, student homework, abandoned experiments, and auto-generated boilerplate. Indiscriminately training on all of it produces mediocre models.

Data curation involves several steps, each addressing a specific quality concern. The first step is filtering out low-quality code, which includes auto-generated files such as protobuf definitions and parser outputs, minified code like compressed JavaScript, and test code containing only simple examples. These artifacts don't teach useful patterns and add noise to the training distribution, potentially causing the model to learn superficial patterns rather than deep programming knowledge.

Deduplication is essential to remove duplicate code snippets and avoid data leakage to test sets. GitHub contains massive duplication through forked repositories, copied code snippets, and repeated boilerplate. Training on duplicates causes the model to memorize specific implementations rather than generalize to new problems. Exact deduplication removes identical files, while near-deduplication uses techniques like MinHash to identify and remove highly similar code that differs only in variable names or minor details.

Language balancing ensures that diverse programming languages including Python, Java, C++, and JavaScript are represented proportionally in the training data. Without balancing, the model might become Python-heavy since Python is disproportionately popular on GitHub, and perform poorly on other languages. Balancing can be achieved by sampling from each language according to its real-world usage or by ensuring each language has a minimum representation threshold.

Sensitive data removal filters out code containing credentials, API keys, or proprietary algorithms. Developers sometimes accidentally commit secrets to public repositories, and training on this data risks the model suggesting those secrets in completions---a serious security issue. Automated scanning tools identify patterns like API key formats, database connection strings, and private keys, removing any code containing these patterns.

License filtering respects code licenses and their legal implications. Some licenses like GPL require derivative works to be open-sourced, creating legal ambiguity when training commercial models on GPL code. Conservative approaches exclude GPL code entirely or clearly document its inclusion and the potential licensing implications. Other permissive licenses like MIT and Apache are generally safe for training, but attribution requirements should still be respected.

Major datasets for code model training include several large-scale collections, each with different characteristics and use cases. The Stack is a massive 6.4 TB corpus of source code from GitHub and other sources, carefully curated for quality and license compliance. It represents 358 programming languages, making it one of the most comprehensive multilingual code datasets available. The curation process includes deduplication, license filtering, and quality scoring based on repository metrics like stars and commit activity.

CodeSearchNet provides 6 million functions from open-source GitHub code in 6 languages: Python, Java, JavaScript, PHP, Ruby, and Go. Importantly, it includes docstrings paired with their corresponding functions, enabling code-documentation alignment training. This pairing allows models to learn the relationship between natural language descriptions and code implementations, supporting both code generation from descriptions and documentation generation from code.

GitHub data in its raw, public form consists of terabytes of diverse repositories with wildly varying quality. This uncurated data includes everything from production-grade libraries to student homework assignments and abandoned experiments. While comprehensive, it requires significant filtering and quality control for production use. Organizations often start with raw GitHub data and apply their own curation pipelines tailored to their specific needs and quality standards.

Pre-training objectives for code models employ several complementary approaches, each teaching the model different aspects of code understanding and generation. Causal language modeling, the GPT-style approach, predicts the next token given all previous tokens. This is the standard approach for code generation because it directly mirrors the code completion task: given a prefix of code, predict what comes next. The model learns to continue code given a context, which translates naturally to IDE code completion where developers type a partial line and the model suggests the continuation.

Masked language modeling, following the BERT-style approach, predicts masked tokens using bidirectional context from both before and after the masked position. This is particularly effective for code understanding tasks like bug detection and code search, where the model needs to understand the full context around a code element. The bidirectional context allows the model to learn how code before and after a token constrains its value, which is crucial for understanding variable types, function signatures, and data flow.

Contrastive learning treats docstrings and code as paired data, training the model so that embeddings of related code and documentation are close in the embedding space while unrelated pairs are far apart. This teaches the model to understand the semantic relationship between natural language descriptions and code implementations. The learned embeddings can be used for code search (finding code that matches a natural language query) and for understanding whether a piece of code implements a given specification.

Documentation alignment training focuses on bidirectional generation between code and documentation. The model learns both code-to-doc generation (given a function, generate its docstring) and doc-to-code generation (given a specification, generate the implementation). This dual training enables the model to serve multiple use cases: generating documentation for undocumented code, generating code from natural language specifications, and verifying that code matches its documentation.

\subsection{Model Families and Capabilities}

The landscape of code models has evolved rapidly, with several families offering different trade-offs between size, performance, and accessibility. Understanding these options helps you choose the right model for your use case.

The landscape of code models has evolved rapidly, with several families offering different trade-offs between size, performance, and accessibility. Understanding these options helps you choose the right model for your specific use case and deployment constraints.

Codex and GPT-Codex, with 12 billion parameters, were trained on GitHub code and power GitHub Copilot. These models excel at few-shot code generation and understanding, often producing high-quality code from minimal context. However, they are proprietary and accessed only via API, which means best-in-class performance comes with ongoing costs and requires sending code to OpenAI's servers---a potential concern for organizations with strict data privacy requirements.

Code Llama offers models ranging from 7 billion to 34 billion parameters, fine-tuned from the LLaMA foundation model specifically on code. These models support remarkably long context windows of up to 100,000 tokens, enabling them to understand entire files or even small codebases. Performance is competitive with Codex on code generation tasks, and the models are open-source, allowing on-premises deployment. The 7B model runs efficiently on consumer GPUs, making it accessible for individual developers, while the 34B model requires high-end hardware but offers substantially better performance for complex tasks.

StarCoder, with 15 billion parameters, was trained on The Stack dataset and designed specifically for production deployment. It is open-source, fast, and particularly effective for code completion tasks. The model strikes a good balance between performance and resource requirements, making it a popular choice for organizations building their own code assistance tools without the scale to train models from scratch.

DeepSeek-Coder offers models ranging from 1.3 billion to 33 billion parameters, specialized for mathematical reasoning and multi-language support. These models are particularly strong on algorithmic problems and competitive programming tasks, often outperforming larger general-purpose models on problems requiring complex reasoning or mathematical insight.

Smaller models like PolyCoder (2.7B parameters) and CodeGPT (125M--355M parameters) provide efficient alternatives for edge deployment or resource-constrained environments. While their performance is lower than larger models, they can run on CPU or even mobile devices, making them suitable for scenarios where latency, privacy, or resource constraints preclude using larger models. These smaller models are acceptable for basic completion tasks like suggesting common patterns or completing simple functions.

Model scaling for code follows similar power laws as natural language, though code models often achieve higher performance at smaller scales due to the structured nature of the domain. A 7B code model often outperforms a 13B natural language model on code tasks, suggesting that code's structure makes it a more learnable domain. The formal syntax and executable semantics provide strong constraints that help smaller models learn effectively.

The business implication: you don't necessarily need the largest model. A well-trained 7B model deployed on-premises might outperform a 175B general-purpose model accessed via API, while being faster, cheaper, and more private. The key is choosing a model trained specifically on code, not just a large general-purpose model.

\section{Developer Assistants and IDE Integration}
\label{sec:idecopilots}

Copilots like GitHub Copilot, Tabnine, and Codeium integrate code models directly into development environments, providing real-time suggestions as developers type. This is where code AI meets daily developer workflow, and the user experience details matter enormously. A suggestion that appears instantly feels like magic; one that takes 2 seconds feels broken. A suggestion that's syntactically correct but semantically wrong wastes time; one that's both correct and idiomatic saves time.

\subsection{Completion Architecture}

An IDE copilot follows this workflow:

\begin{enumerate}
\item \textbf{Context gathering:} Extract surrounding code (current file, related imports, class definitions)
\item \textbf{Prompt construction:} Format context for the model: 
\begin{verbatim}
<file_header>
import numpy as np
from utils import process_data
</file_header>
<function_signature>
def analyze_dataset(data: np.ndarray) -> dict:
    """Analyze statistical properties of the dataset."""
    # User cursor is here <CURSOR>
\end{verbatim}
\item \textbf{Generation:} Run model with temperature $\approx 0.2$ (low randomness for deterministic completions). Generate 1--3 candidate completions.
\item \textbf{Ranking:} Score candidates by multiple criteria to select the best suggestion. Language model probability provides the base score, with higher probabilities indicating the model is more confident in the suggestion. Syntactic validity is checked by parsing the suggestion; code that doesn't parse is immediately rejected. Semantic relevance is assessed by checking whether the suggestion matches the function signature, uses appropriate types, and aligns with any docstring or comments. Popularity is estimated by checking whether similar patterns appeared frequently in the training data, as common patterns are more likely to be correct and idiomatic.
\item \textbf{Display:} Show top candidate as a gray suggestion; allow user to accept (Tab), reject (Esc), or view alternatives.
\end{enumerate}

\subsection{Practical Challenges and Solutions}

\textbf{Latency:} Users expect suggestions within 100--200 milliseconds, but full model inference through all layers takes approximately 500 milliseconds on CPU. This latency gap creates a poor user experience where suggestions feel sluggish rather than responsive. Several solutions address this challenge. On-device inference deploys a smaller quantized model (typically 3B parameters) locally on the developer's machine, eliminating network latency and enabling sub-200ms response times. Speculative decoding generates multiple tokens in parallel rather than sequentially, reducing per-token latency by predicting several likely continuations simultaneously. Caching and KV caching store embeddings of common code patterns and reuse them for similar contexts, avoiding redundant computation. Batching combines multiple simultaneous requests to the same model server, amortizing the fixed overhead of model loading and improving throughput.

\textbf{Hallucination and Correctness:} Code completions must be syntactically valid at minimum; completely broken code that won't parse is worse than no suggestion. Ranking by syntax validity filters out many bad suggestions before they reach the user. However, syntactically correct code can still be semantically wrong, implementing an incorrect algorithm or using inefficient approaches. Displaying suggestions with lower confidence scores (perhaps in a lighter gray or with a warning icon) helps users recognize uncertain completions and apply appropriate scrutiny. Some systems also run basic static analysis on suggestions, flagging potential issues like type mismatches or undefined variables.

\textbf{Multi-file Context:} A function may depend on definitions in other files---imported modules, base classes, or utility functions. Retrieving all necessary context is non-trivial because the dependency graph can be large and complex. Hybrid approaches combine static analysis (parsing the AST to find import statements and follow dependencies) with semantic retrieval (embedding code snippets and retrieving those similar to the current context). This allows the system to include both explicit dependencies (imports) and implicit ones (similar code patterns that might be relevant).

\textbf{Privacy:} Copilots deployed on proprietary codebases risk leaking sensitive code to external servers if they send code to cloud-based models. Several solutions address this concern. On-device models run inference locally, ensuring no code leaves the developer's machine. Custom models can be fine-tuned on proprietary code and deployed internally, keeping both the model and the code within the organization's infrastructure. Federated learning trains models on distributed code without centralizing the data, allowing multiple organizations to collaboratively improve a model while keeping their code private.

\section{Code Analysis, Testing, and Refactoring}
\label{sec:codeanalysis}

Beyond completion, language models enable code analysis and transformation at scale.

\subsection{Static Analysis and Bug Detection}

Models fine-tuned on bug-fix datasets can identify likely bugs by learning patterns of common mistakes. Training on GitHub ``closed issue'' to ``fix commit'' pairs teaches the model to recognize problematic code patterns and their corrections. The model learns to identify several categories of common bugs through this training process.

Off-by-one errors in loops represent a classic category where the model learns to spot incorrect loop bounds, such as iterating to \texttt{len(array)} instead of \texttt{len(array)-1} when using indices. The model recognizes these patterns by seeing thousands of examples where such errors were fixed.

Null pointer dereferences occur when code attempts to access members of objects that might be null or undefined. The model learns to identify code paths where null checks are missing before dereferencing, a pattern that appears frequently in bug fix commits across many languages.

Resource leaks happen when files, database connections, or network sockets are opened but not properly closed. The model learns to identify missing \texttt{close()} calls or contexts where resources should be managed with try-finally blocks or context managers. This pattern recognition comes from seeing many examples where developers added proper resource cleanup in bug fixes.

Type mismatches represent another common category where the model learns to identify cases where a function expects one type but receives another, particularly in dynamically-typed languages where such errors aren't caught at compile time. The model learns these patterns by observing type-related bug fixes and the type annotations or checks that were added.

A trained model encodes the code and generates a ranked set of potential bugs or improvements, often with explanations of why each issue might be problematic. Tools like Infer and DeepCode leverage machine learning for static analysis, combining traditional program analysis techniques with learned patterns to achieve higher accuracy than either approach alone.

\subsection{Test Generation}

Transformers can generate test cases from code. Given a function:
\begin{verbatim}
def factorial(n: int) -> int:
    """Return n!. Assumes n >= 0."""
    if n <= 1:
        return 1
    return n * factorial(n - 1)
\end{verbatim}

A model fine-tuned on code-test pairs generates:
\begin{verbatim}
def test_factorial():
    assert factorial(0) == 1
    assert factorial(1) == 1
    assert factorial(5) == 120
    assert factorial(10) == 3628800
    # Edge case: negative input (should raise or handle gracefully)
\end{verbatim}

Test generation accelerates development and uncovers edge cases. However, generated tests must be validated: do they exercise the intended behavior? Are they redundant? A ranking model can prioritize high-value tests.

\subsection{Refactoring and Code Quality Improvement}

Models can suggest refactoring: breaking large functions into smaller ones, replacing loops with vectorized operations, or simplifying logic. For instance, a repetitive loop:
\begin{verbatim}
result = []
for x in data:
    if x > threshold:
        result.append(x * 2)
\end{verbatim}

Can be refactored to:
\begin{verbatim}
result = [x * 2 for x in data if x > threshold]
\end{verbatim}

A model fine-tuned on refactoring commits learns these transformations. Paired with a linter or style checker, automated refactoring improves code quality without developer intervention.

\section{Repository-Scale Code Understanding}
\label{sec:repositoryunderstanding}

Large language models trained on entire repositories can answer questions like: ``What does this configuration file control?'' or ``Find all places where this function is called.''

\subsection{Graph-Based Retrieval and Reasoning}

A single file provides limited context for understanding code behavior. Repository-scale understanding requires reasoning over the entire codebase, considering how different components interact and depend on each other. Several complementary approaches enable this broader understanding.

Static call graphs extract function call dependencies by parsing the code and building a graph where nodes represent functions and edges represent calls. This allows the system to retrieve all callees (functions that a given function calls) and callers (functions that call a given function). When a developer modifies a function, the call graph identifies all locations that might be affected by the change.

Type information leverages type annotations and type inference to understand data flow through the program. By tracking what types a variable can hold at different points in the code, the system can identify potential type errors, understand how data transforms through function calls, and suggest appropriate operations for each variable. This is particularly powerful in statically-typed languages but can also be applied to dynamically-typed languages with type hints.

Semantic retrieval embeds code snippets into a vector space where semantically similar code is close together. When a developer is working on a particular function, the system can retrieve semantically similar code across the entire repository, even if there are no explicit dependencies. This helps developers find relevant examples, identify duplicate or similar functionality, and understand patterns used elsewhere in the codebase.

Hybrid reasoning combines structured analysis (AST parsing, type checking, control flow analysis) with learned embeddings to leverage both the formal structure of code and the semantic patterns learned from data. For example, the system might use the AST to identify all assignments to a variable, then use semantic embeddings to understand what those assignments mean in the context of the program's purpose.

For instance, if a developer changes a function signature, tools can identify all callers that must be updated by traversing the call graph to find direct callers, then using semantic retrieval to find indirect dependencies where the function's behavior is assumed. This combination of techniques provides more complete coverage than either approach alone.

\subsection{Documentation and Code Generation from Specifications}

Models trained on docstring-code pairs can generate code from documentation. A developer writes:
\begin{verbatim}
def solve_quadratic(a, b, c):
    """
    Solve the quadratic equation ax^2 + bx + c = 0.
    
    Args:
        a, b, c: Coefficients of the quadratic.
    
    Returns:
        A tuple of two roots (may be complex).
    """
\end{verbatim}

A model generates the implementation. Conversely, given code, models summarize it into documentation.

\section{Safety, Licensing, and Ethics}
\label{sec:codesafety}

Training on public code raises concerns about licensing, data contamination, and responsible deployment.

\subsection{Licensing and Attribution}

Public code on GitHub carries various licenses including MIT, GPL, Apache, and others, each with different requirements and restrictions. Models trained on GPL-licensed code may, by some legal interpretations, be required to release their outputs under GPL as well, creating a derivative work issue. This is legally ambiguous and actively debated in both legal and technical communities, with no clear consensus on whether model training constitutes creating a derivative work.

Best practices for handling licensing concerns include several complementary approaches. First, document the licenses of all training data, maintaining a clear record of what code was used and under what terms. This transparency allows users of the model to make informed decisions about their own licensing obligations. Second, avoid or downweight GPL code if licensing compliance is critical for your use case, focusing instead on permissively-licensed code under MIT, Apache, or BSD licenses. Third, attribute code to original authors when possible, for example by retrieving the original function from the repository when the model generates something very similar to existing code. Fourth, provide transparency reports on model training data composition, detailing what percentage of training data came from each license category and how the data was curated.

\subsection{Data Contamination and Test Leakage}

If benchmarks like HumanEval or CodeNet are included in training data, reported performance is inflated. Filtering and deduplication are essential. Libraries like ``exact-substring matching'' identify potential duplicates between training data and benchmarks.

\subsection{Responsible Deployment}

Models should not suggest code patterns that create security vulnerabilities, legal issues, or maintenance problems. Responsible deployment requires active filtering and guardrails to prevent harmful suggestions.

Insecure patterns must be actively filtered, including weak cryptography (using MD5 or SHA1 for security purposes), hardcoded credentials (API keys or passwords embedded in code), and SQL injection vulnerabilities (building SQL queries through string concatenation). The model should be trained to recognize these patterns as anti-patterns and either avoid suggesting them or suggest secure alternatives. This can be achieved through negative example training where the model learns to assign low probability to insecure code.

Copyrighted code presents another concern, as models should not produce exact reproductions of proprietary implementations. While learning general patterns from copyrighted code is likely acceptable, memorizing and reproducing substantial portions of specific implementations creates legal risk. Techniques like membership inference can detect when the model has memorized specific code, and deduplication during training reduces the likelihood of memorization.

Outdated or deprecated APIs should be avoided in suggestions. When libraries release new versions with breaking changes, the model should prefer suggesting current APIs over deprecated ones. This requires either retraining on recent code or maintaining a blacklist of deprecated patterns. Some systems use API documentation to identify deprecated functions and filter suggestions accordingly.

Guardrails and fine-tuning on secure code examples reduce these risks substantially. By training on curated examples of secure code and explicitly teaching the model to avoid insecure patterns, the model learns to generate safer suggestions. Explicit filtering for credentials using pattern matching (detecting strings that look like API keys or tokens) prevents leakage of secrets that may have appeared in training data.

\section{Continuous Learning and Drift in Code Models}
\label{sec:codedrift}

Code models face unique drift challenges because programming languages evolve, libraries update, best practices change, and development patterns shift. A model trained on Python 3.8 code in 2020 will struggle with Python 3.12 features in 2024. Understanding and addressing this drift is essential for maintaining long-term value from code AI investments.

The business impact of code model drift is direct and measurable. When a copilot suggests outdated APIs or deprecated patterns, developers waste time correcting the suggestions. When it fails to understand new language features, it provides no value for modern code. Studies show that code model effectiveness degrades approximately 15--25\% per year without updates, translating to millions in lost productivity for large development teams.

\subsection{Language Evolution and API Changes}

Programming languages evolve continuously, introducing new features, deprecating old ones, and changing best practices. This creates several types of drift that code models must handle.

\textbf{Syntax evolution:} New language features introduce syntax the model has never seen. Python 3.10 added structural pattern matching (\texttt{match/case}); Python 3.12 added type parameter syntax. A model trained before these features won't recognize them, let alone suggest them. This manifests as the model suggesting older, more verbose patterns when newer, cleaner syntax exists.

\textbf{API deprecation:} Libraries deprecate old APIs and introduce new ones. TensorFlow 2.0 completely redesigned the API from TensorFlow 1.x. A model trained on TensorFlow 1.x code will suggest deprecated patterns that no longer work. This is particularly problematic because the suggestions are syntactically valid but semantically broken.

\textbf{Best practice evolution:} What's considered good code changes over time. Type hints in Python went from rare to expected. Async/await patterns replaced callback-based async code. A model trained on old code suggests outdated patterns that work but aren't idiomatic.

\textbf{Security pattern updates:} Security best practices evolve as new vulnerabilities are discovered. A model trained before a major security issue might suggest vulnerable patterns. For example, models trained before widespread awareness of SQL injection might suggest string concatenation for SQL queries rather than parameterized queries.

\begin{keypoint}
For the generic drift detection and continuous learning framework (detection strategies, retraining approaches, implementation infrastructure), see Chapter~\ref{chap:domainspecificmodels}, Section~\ref{sec:continuouslearning}. Here we highlight code-specific considerations.
\end{keypoint}

Key code-specific adaptation strategies include:
\begin{itemize}
\item \textbf{Acceptance rate monitoring:} Track what percentage of suggestions developers accept (typically 25--35\% for a healthy copilot). Declining rates signal drift.
\item \textbf{Negative example training:} Collect deprecated or insecure patterns and train the model to avoid them---especially important as security best practices evolve.
\item \textbf{Continuous pre-training on recent commits:} Schedule monthly or quarterly pre-training on internal code to keep the model aligned with evolving team practices.
\item \textbf{Language and library version tracking:} Integrate with dependency management (pip, npm, Maven) to prioritize retraining for the versions your team actually uses.
\end{itemize}

\subsection{Case Study: Adapting to Python 3.11 and 3.12}

A 200-developer team's code copilot dropped from 32\% to 22\% acceptance rate after upgrading to Python 3.12, because the model (trained on 3.8--3.10) didn't recognize new features like exception groups and type parameter syntax. Fine-tuning on a 60/40 mix of new and historical code recovered acceptance to 29\% within two weeks. Quarterly retraining at \$5,000/cycle then stabilized acceptance at 30--33\%, maintaining \$4M/year in productivity gains for a \$20K/year continuous learning investment.

\section{Case Study: IDE Copilot for Python Development}
\label{sec:casecopilot}

A team building a Python IDE wants to add code completion to enhance developer productivity.

\subsection{System Design}

The team selected StarCoder-7B as their base model and fine-tuned it on a carefully curated dataset of 50,000 Python functions. This dataset combined code from company repositories, ensuring the model learned internal coding patterns and conventions, with curated open-source code that provided exposure to diverse, high-quality implementations. The fine-tuning process took approximately 48 hours on 4 A100 GPUs.

For context handling, the system uses a 2,048-token window that includes the current file plus imported modules. This window size balances comprehensiveness (seeing enough context to understand the code's purpose) with efficiency (keeping inference time low). The system intelligently selects which imports to include, prioritizing those most relevant to the current cursor position.

Deployment uses a quantized FP16 model that reduces the model size to 3.5 GB, small enough to run locally on developer machines. The model runs on the developer's GPU when available, falling back to CPU when necessary. This local deployment ensures zero latency from network communication and complete privacy since no code leaves the developer's machine.

The system maintains a latency service level objective (SLO) of 200 milliseconds at the 95th percentile for the first suggestion. This means 95\% of suggestions appear within 200ms, fast enough to feel responsive during active coding. The remaining 5\% of requests may take longer, typically when processing unusually large context or complex code patterns.

\subsection{Metrics}

The system's acceptance rate of 35\% represents a substantial improvement over the 10\% acceptance rate of traditional keyword-based completion. This means developers accept more than one in three suggestions, indicating the model provides genuinely useful completions rather than noise. The 3.5Ã— improvement over keyword completion demonstrates the value of deep learning approaches for code.

Code quality metrics show that suggestions pass linting in 87\% of cases, meaning they follow style guidelines and don't trigger warnings. Syntax errors occur in only 8\% of suggestions, demonstrating the model has learned to generate well-formed code. Semantic errors, where the code is syntactically correct but implements incorrect logic, occur in 5\% of cases. These semantic errors are the hardest to eliminate because they require deep understanding of the code's intended behavior.

Developer productivity measurements, based on self-reported surveys and task timing studies, show a 15\% reduction in time to write unit tests. The model excels at generating test boilerplate and common test patterns. Debugging time decreased by 10\%, likely because the model's suggestions are generally correct and reduce the introduction of bugs during initial coding.

Latency measurements show the system achieves 120 milliseconds at the 50th percentile and 180 milliseconds at the 95th percentile. Both values fall well below the 200ms target, ensuring suggestions feel responsive. The p50 latency of 120ms means half of all suggestions appear in barely more than a tenth of a second, fast enough to feel nearly instantaneous.

Privacy is absolute: 100\% of inference happens on-device, meaning zero code leaves the developer's machine. This complete privacy guarantee makes the system acceptable for use on proprietary codebases and sensitive projects where code confidentiality is paramount.

\section{Exercises}

\begin{exercise}
Tokenize this Python code snippet using a standard BPE tokenizer and a code-specific tokenizer. Compare token counts and observe which tokens are merged differently:
\begin{verbatim}
def fibonacci(n: int) -> int:
    if n <= 1:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)
\end{verbatim}
\end{exercise}

\begin{exercise}
Design a system to generate unit tests for a given function. What should the prompt look like? How would you rank generated tests? How would you handle functions with side effects (file I/O, database access)?
\end{exercise}

\begin{exercise}
Train a code summarization model on code-docstring pairs. Given a complex function, generate a one-sentence summary. How would you evaluate the quality of summaries?
\end{exercise}

\section{Solutions}

Full solutions for all exercises are available at \url{https://deeplearning.hofkensvermeulen.be}.

\begin{solution}
\textbf{Exercise 1: Code Tokenization}

Using a natural language BPE tokenizer (e.g., GPT-2), the code snippet might tokenize as:
\begin{verbatim}
["def", "Fibonacci", "(", "n", ":", "int", ")", "->", "int", ":", 
 "\n", "if", "n", "<=", "1", ":", ...]  (60+ tokens)
\end{verbatim}

Using a code-specific tokenizer:
\begin{verbatim}
["def", "fibonacci", "(", "n", ":", "int", ")", "->", "int", ":", 
 "\n", "if", "n", "<=", "1", ":", ...]  (45 tokens)
\end{verbatim}

The key differences reveal how code-specific tokenization improves efficiency and semantic preservation. Keywords like \texttt{def}, \texttt{if}, and \texttt{int} are always single tokens in code tokenizers because they are fundamental language constructs that should never be split. The operator \texttt{<=} merges into a single token in code tokenizers since it's a common comparison operator that appears frequently in conditional statements.

Indentation handling differs significantly: code tokenizers may explicitly preserve whitespace structure as special tokens, recognizing that indentation carries semantic meaning in Python. The function name \texttt{fibonacci} (lowercase) may be split differently based on training corpus frequency---if the tokenizer has seen many functions with similar names, it might keep it as one token or split it into meaningful subwords like \texttt{[fib, onacci]}.

The overall token count reduction from 60+ to 45 tokens demonstrates the compression advantage of code-specific tokenization. This 25\% reduction means the model can fit more code in its context window and process it more efficiently, directly translating to better performance on code understanding and generation tasks.
\end{solution}

\begin{solution}
\textbf{Exercise 2: Unit Test Generation}

\textit{Prompt Design:}
\begin{verbatim}
# Given a function, generate comprehensive unit tests

def add(a: int, b: int) -> int:
    """Return the sum of a and b."""
    return a + b

# Generate pytest-compatible tests covering normal cases, edge cases, and errors:

import pytest

def test_add():
    # Normal cases
    assert add(1, 2) == 3
    assert add(0, 0) == 0
    assert add(-1, 1) == 0
    
    # Edge cases
    assert add(int_max, 1)  # May overflow
    
    # Type errors (if desired)
    with pytest.raises(TypeError):
        add("1", 2)
\end{verbatim}

\textit{Ranking Generated Tests:} Effective test ranking requires multiple criteria to identify the most valuable tests. Line coverage should be prioritized, favoring tests that exercise different code paths through the function. A test that executes a branch not covered by other tests is more valuable than one that duplicates existing coverage.

Branch coverage ensures both sides of conditional statements are tested. For an \texttt{if-else} statement, you want tests that trigger both the true and false branches. This reveals bugs that only manifest under specific conditions.

Redundancy elimination removes tests that are subsets of others. If one test checks \texttt{add(1, 2) == 3} and another checks \texttt{add(1, 2) == 3} with additional assertions, the first test is redundant and can be removed. This keeps the test suite concise and maintainable.

Diversity in test inputs is crucial: prefer tests exploring different input classes including positive numbers, negative numbers, zero, and boundary values like maximum and minimum integers. This systematic exploration of the input space is more likely to uncover edge cases than random testing.

\textit{Handling Side Effects:} Functions with side effects like file I/O or database access require mocking or test fixtures to run reliably and quickly. The model should generate test setup code that creates controlled environments. For file operations, use temporary directories that are automatically cleaned up. For database operations, use in-memory databases or transaction rollback to ensure tests don't affect persistent state. The generated test should include both the setup (creating the test environment), the execution (calling the function), and the teardown (cleaning up resources), following the Arrange-Act-Assert pattern.
\end{solution}

\begin{solution}
\textbf{Exercise 3: Code Summarization}

\textit{Model Training:}
Collect code-docstring pairs from open-source projects. Format as:
\begin{verbatim}
Input: [function code]
Output: [one-sentence summary or docstring]
\end{verbatim}

Fine-tune an encoder-decoder model (T5 or BART) for 10 epochs on 100,000 pairs.

\textit{Evaluation Metrics:} Assessing code summarization quality requires both automated metrics and human evaluation. ROUGE-L measures the longest common subsequence overlap between generated and reference summaries, producing a score from 0 to 1 where higher values indicate better overlap. This metric captures how well the generated summary matches the reference in terms of word choice and structure.

BLEU score measures the precision of n-grams (sequences of n words) in generated summaries compared to references, producing a score from 0 to 100. BLEU is particularly sensitive to exact word matches and is commonly used in machine translation evaluation. For code summarization, BLEU helps identify whether the model uses appropriate technical terminology.

Human evaluation remains essential because automated metrics can't fully capture semantic accuracy and clarity. Domain experts---in this case, experienced developers---rate summaries on a 1--5 scale for both accuracy (does the summary correctly describe what the code does?) and clarity (is the summary easy to understand?). This subjective evaluation catches issues like technically correct but confusing summaries, or summaries that miss the code's primary purpose.

Length constraints ensure summaries are concise and useful. The average summary length should be 1--2 sentences, long enough to convey the function's purpose but short enough to read quickly. Summaries that are too long defeat the purpose of summarization, while those that are too short may omit important details.

\textit{Results (example):} A well-trained model might achieve a ROUGE-L score of 0.42, indicating moderate overlap with reference summaries. This is reasonable because there are many valid ways to describe the same code, so perfect overlap isn't expected. Human ratings of 4.1 out of 5 suggest developers find the summaries helpful and accurate in practice, which is ultimately more important than automated metrics.

However, the model produces false summaries in approximately 2\% of cases, where the generated summary contradicts the actual code behavior. For example, claiming a function returns an integer when it actually returns a boolean. These errors are serious because they mislead developers, so any deployment should include manual review or automated verification to catch such mistakes. The summaries can be deployed as IDE tooltips or auto-documentation features, but with clear indication that they are machine-generated and should be verified for critical code.
\end{solution}
