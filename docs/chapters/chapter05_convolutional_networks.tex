\chapter{Convolutional Neural Networks}
\label{chap:convolutional_networks}

\section*{Chapter Overview}

Convolutional Neural Networks (CNNs) revolutionized computer vision by exploiting spatial structure. This chapter develops convolution operations, pooling, and modern CNN architectures including ResNet.

\subsection*{Learning Objectives}

\begin{enumerate}
    \item Understand convolution operations and compute output dimensions
    \item Design CNN architectures with appropriate pooling and stride
    \item Understand translation equivariance
    \item Implement modern CNN architectures (ResNet, VGG)
\end{enumerate}

\section{Convolution Operation}
\label{sec:convolution_operation}

\begin{definition}[2D Convolution]
\label{def:2d_convolution}
For input $\mX \in \R^{H \times W}$ and kernel $\mK \in \R^{k_h \times k_w}$:
\begin{equation}
(\mX \star \mK)_{i,j} = \sum_{m=0}^{k_h-1} \sum_{n=0}^{k_w-1} \mX_{i+m, j+n} \cdot \mK_{m,n}
\end{equation}
\end{definition}

\begin{example}[3x3 Convolution]
\label{ex:3x3_conv}
Input $4\times4$, kernel $3\times3$ (edge detector), output $2\times2$. Computing first position: sum of element-wise products gives edge response.
\end{example}

\subsection{Output Dimensions}

\begin{theorem}[Output Size]
\label{thm:conv_output_size}
For input size $H \times W$, kernel $k_h \times k_w$, padding $p$, stride $s$:
\begin{equation}
H_{\text{out}} = \left\lfloor \frac{H + 2p - k_h}{s} \right\rfloor + 1
\end{equation}
\end{theorem}

\section{Multi-Channel Convolutions}
\label{sec:multi_channel}

\begin{definition}[Convolutional Layer]
\label{def:conv_layer}
For input $\mathbf{X} \in \R^{C_{\text{in}} \times H \times W}$ with $C_{\text{out}}$ output channels:
\begin{equation}
\mathbf{Y}^{(i)} = \sum_{c=1}^{C_{\text{in}}} \mathbf{X}^{(c)} \star \mathbf{K}^{(i,c)} + b^{(i)}
\end{equation}
\end{definition}

\begin{example}[RGB Convolution]
\label{ex:rgb_conv}
Input: $\mathbf{X} \in \R^{3 \times 224 \times 224}$. Conv layer: 64 filters $3\times3$, stride 1, padding 1.

Parameters: $64 \times 3 \times 3 \times 3 + 64 = 1{,}792$

Output: $\mathbf{Y} \in \R^{64 \times 224 \times 224}$

Compare to fully-connected: $\approx 483$ billion parameters!
\end{example}

\begin{keypoint}
Convolution provides: (1) Parameter sharing, (2) Local connectivity, (3) Translation equivariance. Massive parameter reduction compared to fully-connected layers.
\end{keypoint}

\section{Computational Analysis of Convolutions}
\label{sec:conv_computation}

Understanding the computational cost and memory requirements of convolutional layers is essential for designing efficient architectures and comparing CNNs with alternative approaches like transformers. The relationship between parameters, FLOPs, and memory usage in convolutions differs fundamentally from fully-connected layers, leading to distinct performance characteristics on modern hardware.

\subsection{FLOPs for Convolution Operations}

The computational cost of a convolutional layer is determined by the number of multiply-accumulate operations required to compute all output feature maps. For a convolutional layer with input shape $C_{\text{in}} \times H \times W$, kernel size $k \times k$, and $C_{\text{out}}$ output channels, each output position requires $C_{\text{in}} \times k \times k$ multiply-accumulate operations. With output spatial dimensions $H_{\text{out}} \times W_{\text{out}}$ and $C_{\text{out}}$ output channels, the total FLOPs is:

\begin{equation}
\text{FLOPs}_{\text{conv}} = 2 \times C_{\text{out}} \times C_{\text{in}} \times k^2 \times H_{\text{out}} \times W_{\text{out}}
\end{equation}

The factor of 2 accounts for the multiply-accumulate operation (one multiplication and one addition per operation). This formula reveals that convolution FLOPs scale linearly with both input and output channels, quadratically with kernel size, and linearly with output spatial dimensions.

For the RGB convolution example in Example~\ref{ex:rgb_conv} with input $3 \times 224 \times 224$, kernel size $3 \times 3$, and 64 output channels with stride 1 and padding 1, the output dimensions are $64 \times 224 \times 224$. The FLOPs calculation is $2 \times 64 \times 3 \times 9 \times 224 \times 224 = 173{,}408{,}192$ FLOPs, or approximately 173 MFLOPs. Despite having only 1,792 parameters, this layer requires 173 million floating-point operations, giving a FLOPs-to-parameter ratio of approximately 96,768. This ratio is dramatically higher than fully-connected layers, which have a FLOPs-to-parameter ratio of approximately 2-3.

The high FLOPs-to-parameter ratio of convolutions has important implications for model design. Convolutional layers are compute-intensive relative to their memory footprint, making them well-suited for modern GPUs that have abundant compute throughput but limited memory bandwidth. A ResNet-50 model with 25.6 million parameters requires approximately 4.1 billion FLOPs for a single forward pass on a $224 \times 224$ image, giving an overall FLOPs-to-parameter ratio of 160. This means that during training, the computational cost dominates over the memory cost of loading parameters, and GPU utilization is primarily limited by compute throughput rather than memory bandwidth.

The scaling behavior of convolution FLOPs explains several architectural design choices in modern CNNs. Early layers operating on high-resolution feature maps ($224 \times 224$ or larger) consume the majority of FLOPs despite having relatively few parameters. For ResNet-50, the first convolutional layer with kernel size $7 \times 7$ and 64 output channels accounts for only 0.4\% of parameters but 5.8\% of total FLOPs. Conversely, later layers operating on low-resolution feature maps ($7 \times 7$ or smaller) have many parameters but relatively few FLOPs. The final fully-connected layer in ResNet-50 accounts for 7.8\% of parameters but only 0.1\% of FLOPs. This distribution motivates the use of larger kernels and more channels in early layers (where spatial dimensions are large) and smaller kernels with many channels in later layers (where spatial dimensions are small).

\subsection{Memory Requirements for Feature Maps}

During training, convolutional networks must store intermediate feature maps for use in the backward pass, and these activations typically consume far more memory than the model parameters. Understanding activation memory is critical for determining maximum batch size and input resolution.

For a convolutional layer with input shape $B \times C_{\text{in}} \times H \times W$ (where $B$ is batch size) and output shape $B \times C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}}$, the network must store both the input feature map and the output feature map. In FP32, this requires $4B(C_{\text{in}} HW + C_{\text{out}} H_{\text{out}} W_{\text{out}})$ bytes of memory. For the RGB convolution example with batch size $B = 32$, input $3 \times 224 \times 224$, and output $64 \times 224 \times 224$, the activation memory is $4 \times 32 \times (3 \times 224 \times 224 + 64 \times 224 \times 224) = 130{,}809{,}792$ bytes, or approximately 125 MB. This is 70,000× larger than the parameter memory (1,792 parameters = 7,168 bytes), demonstrating that activation memory dominates for convolutional layers.

The memory consumption of a full CNN scales with the number of layers and the spatial dimensions of feature maps. For ResNet-50 processing batch size 32 with input $3 \times 224 \times 224$, the total activation memory is approximately 8.2 GB in FP32. This includes the input image (6.4 MB), early high-resolution feature maps (hundreds of MB), and later low-resolution feature maps (tens of MB). The parameter memory for ResNet-50 is only 102 MB (25.6 million parameters × 4 bytes), making activations 80× larger than parameters. This ratio increases with batch size: at batch size 256, activations consume 65.6 GB while parameters remain 102 MB, a ratio of 643×.

The quadratic scaling of activation memory with spatial resolution has profound implications for input image size. Doubling the input resolution from $224 \times 224$ to $448 \times 448$ increases the number of pixels by 4×, and since early feature maps maintain similar spatial dimensions to the input, activation memory increases by approximately 4×. For ResNet-50 with batch size 32, increasing resolution from $224 \times 224$ to $448 \times 448$ increases activation memory from 8.2 GB to approximately 32.8 GB, exceeding the capacity of most GPUs. This explains why high-resolution image processing typically requires smaller batch sizes or gradient accumulation: the activation memory grows faster than available GPU memory.

Modern techniques for reducing activation memory include gradient checkpointing, which recomputes activations during the backward pass rather than storing them, trading computation for memory. For ResNet-50, gradient checkpointing can reduce activation memory by 5-10× at the cost of increasing training time by 20-30\%. This trade-off is often worthwhile for training with larger batch sizes or higher resolutions, as the improved convergence from larger batches can offset the increased computation time.

\subsection{GPU Optimization: im2col and Winograd}

Efficient implementation of convolution on GPUs requires specialized algorithms that transform the convolution operation into a form amenable to highly optimized matrix multiplication routines. The two primary approaches are im2col (image-to-column) and Winograd convolution, each with distinct performance characteristics.

The im2col algorithm transforms convolution into matrix multiplication by unrolling the input feature map into a large matrix where each column contains the input values for one output position. For a convolutional layer with input $C_{\text{in}} \times H \times W$, kernel size $k \times k$, and output $C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}}$, im2col creates a matrix of shape $(C_{\text{in}} k^2) \times (H_{\text{out}} W_{\text{out}})$ by extracting all $k \times k$ patches from the input. The convolution kernels are reshaped into a matrix of shape $C_{\text{out}} \times (C_{\text{in}} k^2)$. The convolution is then computed as a single matrix multiplication: $\text{output} = \text{kernels} \times \text{im2col}(\text{input})$, producing a matrix of shape $C_{\text{out}} \times (H_{\text{out}} W_{\text{out}})$ that is reshaped to the final output dimensions.

For the RGB convolution example with input $3 \times 224 \times 224$, kernel size $3 \times 3$, and 64 output channels, im2col creates a matrix of shape $27 \times 50{,}176$ (since $C_{\text{in}} k^2 = 3 \times 9 = 27$ and $H_{\text{out}} W_{\text{out}} = 224 \times 224 = 50{,}176$). The kernel matrix has shape $64 \times 27$. The matrix multiplication $64 \times 27$ times $27 \times 50{,}176$ requires $2 \times 64 \times 27 \times 50{,}176 = 173{,}408{,}192$ FLOPs, matching the direct convolution calculation. However, the im2col matrix requires $27 \times 50{,}176 \times 4 = 5{,}419{,}008$ bytes (5.2 MB) of temporary storage, which is 757× larger than the original input (7,168 bytes for $3 \times 224 \times 224$ in FP32).

The advantage of im2col is that it leverages highly optimized BLAS (Basic Linear Algebra Subprograms) libraries like cuBLAS on NVIDIA GPUs, which achieve 80-95\% of peak hardware throughput for large matrix multiplications. For the $64 \times 27$ times $27 \times 50{,}176$ multiplication on an NVIDIA A100 GPU with 312 TFLOPS FP16 throughput, the operation completes in approximately 0.6 microseconds at 90\% efficiency, achieving 280 TFLOPS. Direct convolution implementations without im2col typically achieve only 40-60\% efficiency due to irregular memory access patterns and difficulty saturating the GPU's parallel execution units.

The disadvantage of im2col is the memory overhead. For batch size $B = 32$, the im2col matrix grows to $32 \times 27 \times 50{,}176 = 43{,}352{,}064$ elements, requiring 167 MB of temporary storage. This memory must be allocated and deallocated for each convolutional layer, adding memory pressure and potentially causing out-of-memory errors for large batch sizes or high-resolution inputs. Modern implementations mitigate this by processing the batch in chunks or fusing the im2col transformation with the matrix multiplication to avoid materializing the full im2col matrix.

Winograd convolution is an alternative algorithm that reduces the number of multiplications required for small convolutions (typically $3 \times 3$ or $5 \times 5$ kernels) by using a mathematical transformation that trades multiplications for additions. For $3 \times 3$ convolutions, Winograd reduces the number of multiplications by 2.25× compared to direct convolution, from 9 multiplications per output to 4 multiplications per output. This reduction translates directly to FLOPs savings: the RGB convolution example requires only $173{,}408{,}192 / 2.25 = 77{,}070{,}752$ FLOPs with Winograd, a 56\% reduction.

However, Winograd convolution has several limitations. First, it requires additional memory for intermediate transformations, typically 2-3× the input size. Second, it is numerically less stable than direct convolution, particularly in FP16, due to the transformation matrices having large condition numbers. Third, it is only applicable to small kernel sizes ($3 \times 3$ and $5 \times 5$) and becomes inefficient for larger kernels. Fourth, the transformation overhead becomes significant for small spatial dimensions, making Winograd most effective for early layers with large feature maps.

In practice, modern deep learning frameworks like PyTorch and TensorFlow automatically select between im2col, Winograd, and direct convolution based on layer dimensions, batch size, and hardware characteristics. For $3 \times 3$ convolutions on high-resolution feature maps ($\geq 56 \times 56$) with batch size $\geq 16$, Winograd typically provides 1.5-2× speedup over im2col. For larger kernels ($5 \times 5$ or $7 \times 7$) or smaller feature maps, im2col is preferred. For very small batch sizes ($\leq 4$), direct convolution may be fastest due to lower overhead. NVIDIA's cuDNN library implements all three algorithms and includes heuristics to select the optimal approach for each layer configuration.

\subsection{Comparison with Transformer Attention}

Comparing the computational characteristics of convolutional layers with transformer self-attention reveals fundamental trade-offs between local and global receptive fields, parameter efficiency, and computational scaling.

A convolutional layer with kernel size $k \times k$ has a local receptive field: each output position depends only on a $k \times k$ neighborhood of the input. To achieve a global receptive field spanning the entire input, multiple convolutional layers must be stacked. For an input of size $H \times W$, achieving a receptive field covering the full input requires approximately $\log_k(\max(H, W))$ layers. For a $224 \times 224$ image with $3 \times 3$ convolutions, this requires approximately $\log_3(224) \approx 5$ layers. Each layer adds computational cost, but the cost per layer remains $O(C_{\text{out}} C_{\text{in}} k^2 HW)$, scaling linearly with spatial dimensions.

In contrast, self-attention in transformers has a global receptive field: each output position attends to all input positions in a single layer. For an input sequence of length $n = HW$ (treating the 2D image as a 1D sequence) with model dimension $d$, self-attention requires computing query-key products for all pairs of positions, resulting in $O(n^2 d)$ FLOPs. For a $224 \times 224$ image with $n = 50{,}176$ positions and $d = 768$ (typical for Vision Transformers), self-attention requires approximately $2 \times 50{,}176^2 \times 768 = 3.86 \times 10^{12}$ FLOPs, or 3.86 TFLOPs per layer. This is 22,000× more expensive than the RGB convolution example (173 MFLOPs), despite both operating on the same input resolution.

The quadratic scaling of attention with spatial resolution makes it prohibitively expensive for high-resolution images. Doubling the resolution from $224 \times 224$ to $448 \times 448$ increases attention FLOPs by 16× (since $n$ increases by 4× and attention scales as $n^2$), while convolution FLOPs increase by only 4× (linear scaling with spatial dimensions). For a $448 \times 448$ image, self-attention requires 61.8 TFLOPs per layer, making it impractical without modifications like hierarchical attention or local attention windows.

Vision Transformers (ViTs) address this computational challenge by dividing the image into patches and treating each patch as a token. For a $224 \times 224$ image with patch size $16 \times 16$, the sequence length is $n = (224/16)^2 = 196$ patches. Self-attention on 196 patches with $d = 768$ requires $2 \times 196^2 \times 768 = 59{,}015{,}168$ FLOPs, or approximately 59 MFLOPs per layer. This is 65× less expensive than attention on individual pixels and comparable to the RGB convolution example (173 MFLOPs). However, the patch-based approach sacrifices fine-grained spatial resolution: each patch is treated as a single token, and the model cannot attend to individual pixels within a patch.

The parameter efficiency of convolutions versus attention also differs significantly. A convolutional layer with $C_{\text{in}}$ input channels, $C_{\text{out}}$ output channels, and kernel size $k \times k$ has $C_{\text{out}} C_{\text{in}} k^2$ parameters. For the RGB convolution example, this is $64 \times 3 \times 9 = 1{,}728$ parameters. A self-attention layer with model dimension $d$ has query, key, and value projection matrices, each of size $d \times d$, totaling $3d^2$ parameters (ignoring the output projection). For $d = 768$, this is $3 \times 768^2 = 1{,}769{,}472$ parameters, which is 1,024× more than the convolutional layer. However, the attention parameters are independent of spatial resolution, while convolution parameters are independent of spatial resolution as well. The key difference is that attention parameters scale with $d^2$ while convolution parameters scale with $C_{\text{in}} C_{\text{out}} k^2$, and typically $d \gg C_{\text{in}}$ for early layers.

For complete models, ResNet-50 has 25.6 million parameters and requires 4.1 GFLOPs per image, while ViT-Base has 86 million parameters and requires 17.6 GFLOPs per image. The ViT has 3.4× more parameters and 4.3× more FLOPs, but achieves comparable or better accuracy on ImageNet classification. The higher computational cost of ViT is offset by its ability to leverage large-scale pretraining on datasets like ImageNet-21k or JFT-300M, where the global receptive field and flexibility of attention provide advantages over the inductive biases of convolution.

\section{Parameter Efficiency: CNNs vs Transformers}
\label{sec:parameter_efficiency}

The parameter efficiency of convolutional networks compared to transformers is a critical consideration for model design, particularly for vision tasks where input dimensions are large. Understanding why CNNs achieve strong performance with fewer parameters than transformers reveals fundamental differences in their architectural inductive biases.

\subsection{Why CNNs are More Parameter-Efficient for Images}

Convolutional networks achieve parameter efficiency through three key mechanisms: weight sharing, local connectivity, and hierarchical feature learning. These properties are particularly well-suited to natural images, which exhibit strong spatial locality and translation invariance.

Weight sharing in convolutions means that the same kernel is applied to all spatial positions in the input. A $3 \times 3$ convolutional kernel with 64 input channels and 64 output channels has $64 \times 64 \times 9 = 36{,}864$ parameters, regardless of whether the input is $32 \times 32$ or $224 \times 224$. This kernel is applied $H \times W$ times (once per output position), effectively sharing the same 36,864 parameters across all spatial locations. In contrast, a fully-connected layer connecting a $224 \times 224 \times 64$ input to a $224 \times 224 \times 64$ output would require $224^2 \times 64 \times 224^2 \times 64 = 1.3 \times 10^{11}$ parameters, which is 3.5 million times larger. Weight sharing reduces parameters by a factor equal to the spatial dimensions, which is enormous for images.

Local connectivity means that each output position depends only on a small neighborhood of the input, rather than the entire input. For a $3 \times 3$ convolution, each output depends on only 9 input positions (plus all input channels). This locality assumption is well-matched to natural images, where nearby pixels are highly correlated and distant pixels are largely independent. By restricting connectivity to local neighborhoods, convolutions avoid the quadratic parameter growth of fully-connected layers while still capturing the relevant spatial structure.

Hierarchical feature learning in CNNs builds global receptive fields through stacking local operations. Early layers learn low-level features like edges and textures with small receptive fields, middle layers learn mid-level features like object parts with medium receptive fields, and late layers learn high-level features like whole objects with large receptive fields. This hierarchy is achieved by stacking convolutional layers with pooling or strided convolutions to progressively reduce spatial dimensions. For ResNet-50, the receptive field grows from $7 \times 7$ in the first layer to $427 \times 427$ in the final layer, covering the entire $224 \times 224$ input multiple times over. This hierarchical approach requires far fewer parameters than directly modeling global dependencies, as each layer only needs to model local relationships.

The parameter efficiency of CNNs is evident in model comparisons. ResNet-50 achieves 76.1\% top-1 accuracy on ImageNet with 25.6 million parameters, while ViT-Base achieves 77.9\% accuracy with 86 million parameters—3.4× more parameters for a 1.8 percentage point improvement. EfficientNet-B0 achieves 77.1\% accuracy with only 5.3 million parameters, demonstrating that carefully designed CNNs can match or exceed transformer performance with 16× fewer parameters. The parameter efficiency of CNNs makes them particularly attractive for deployment on resource-constrained devices like mobile phones or embedded systems, where model size directly impacts memory usage and inference latency.

However, the parameter efficiency of CNNs comes with trade-offs. The strong inductive biases of weight sharing and local connectivity make CNNs sample-efficient for small datasets but potentially limit their capacity to learn from very large datasets. Vision Transformers, with their weaker inductive biases and higher parameter counts, can leverage massive datasets like ImageNet-21k (14 million images) or JFT-300M (300 million images) to achieve superior performance. When pretrained on JFT-300M, ViT-Large (307 million parameters) achieves 87.8\% accuracy on ImageNet, significantly outperforming any CNN. The optimal architecture depends on the available data: CNNs excel with limited data, while transformers excel with abundant data.

\subsection{Vision Transformer Comparison}

Vision Transformers (ViTs) adapt the transformer architecture from NLP to computer vision by treating images as sequences of patches. Understanding the architectural differences and performance trade-offs between ViTs and CNNs is essential for selecting the appropriate model for a given task and dataset.

A Vision Transformer divides an input image into non-overlapping patches, linearly embeds each patch, and processes the sequence of patch embeddings with standard transformer layers. For a $224 \times 224$ image with patch size $16 \times 16$, the image is divided into $(224/16)^2 = 196$ patches. Each patch is flattened to a vector of size $16 \times 16 \times 3 = 768$ and linearly projected to the model dimension $d$ (typically 768 for ViT-Base). The resulting sequence of 196 tokens is processed by 12 transformer layers with multi-head self-attention and feed-forward networks, identical to BERT.

The parameter breakdown for ViT-Base reveals where parameters are allocated. The patch embedding layer has $768 \times 768 = 589{,}824$ parameters (projecting flattened patches to model dimension). Each of the 12 transformer layers has approximately 7.1 million parameters: 2.4 million for self-attention (query, key, value, and output projections) and 4.7 million for the feed-forward network (two linear layers with 4× expansion). The classification head has $768 \times 1000 = 768{,}000$ parameters for ImageNet's 1000 classes. The total is approximately 86 million parameters, with 85\% in the transformer layers and 15\% in embeddings and classification head.

Comparing ViT-Base to ResNet-50 reveals fundamental differences in parameter allocation. ResNet-50 has 25.6 million parameters distributed across convolutional layers (23.5 million, 92\%), batch normalization (1.1 million, 4\%), and the classification head (1.0 million, 4\%). The convolutional parameters are concentrated in later layers operating on low-resolution feature maps: the final residual block has 7.1 million parameters despite operating on $7 \times 7$ feature maps, while the first convolutional layer has only 9,408 parameters despite operating on $224 \times 224$ inputs. This distribution reflects the CNN's hierarchical design, where early layers extract simple features with few parameters and late layers combine features with many parameters.

In contrast, ViT-Base distributes parameters uniformly across layers: each of the 12 transformer layers has approximately 7.1 million parameters, regardless of the stage of processing. This uniform distribution reflects the transformer's lack of hierarchical structure: all layers operate on the same sequence length (196 patches) and model dimension (768), performing the same operations. The absence of hierarchy means that ViT must learn hierarchical features implicitly through the attention mechanism, rather than having them built into the architecture as in CNNs.

The computational cost comparison is similarly revealing. ResNet-50 requires 4.1 GFLOPs per image, with 3.8 GFLOPs (93\%) in convolutional layers and 0.3 GFLOPs (7\%) in other operations. The FLOPs are concentrated in early layers: the first residual block accounts for 1.2 GFLOPs (29\%) despite having only 0.2 million parameters (0.8\%), while the final residual block accounts for 0.1 GFLOPs (2.4\%) despite having 7.1 million parameters (28\%). This distribution reflects the $O(C_{\text{in}} C_{\text{out}} k^2 HW)$ scaling of convolution FLOPs: early layers have large $HW$ but small $C$, while late layers have small $HW$ but large $C$.

ViT-Base requires 17.6 GFLOPs per image, with 16.8 GFLOPs (95\%) in transformer layers and 0.8 GFLOPs (5\%) in patch embedding and classification. The FLOPs are distributed uniformly across layers: each transformer layer accounts for approximately 1.4 GFLOPs (8\%). The self-attention in each layer requires 0.6 GFLOPs (computed as $2 \times 196^2 \times 768 \times 12 / 10^9$ for query-key products and attention-value products across 12 heads), while the feed-forward network requires 0.8 GFLOPs (computed as $2 \times 196 \times 768 \times 3072 \times 2 / 10^9$ for two linear layers). The uniform distribution reflects the constant sequence length and model dimension throughout the network.

The accuracy comparison on ImageNet reveals the impact of pretraining scale. When trained from scratch on ImageNet-1k (1.3 million images), ResNet-50 achieves 76.1\% top-1 accuracy while ViT-Base achieves only 72.3\% accuracy—3.8 percentage points worse despite having 3.4× more parameters and 4.3× more FLOPs. This performance gap demonstrates that ViT's weak inductive biases require more data to learn effectively. However, when pretrained on ImageNet-21k (14 million images) and fine-tuned on ImageNet-1k, ViT-Base achieves 81.8\% accuracy, surpassing ResNet-50 by 5.7 percentage points. With even larger pretraining on JFT-300M (300 million images), ViT-Large achieves 87.8\% accuracy, establishing a new state-of-the-art.

The lesson is clear: CNNs are more parameter-efficient and sample-efficient for small to medium datasets, making them the preferred choice when data is limited or computational resources are constrained. Vision Transformers excel when large-scale pretraining data is available, leveraging their flexibility and capacity to achieve superior performance. The optimal choice depends on the specific use case: CNNs for resource-constrained deployment or limited data, transformers for maximum accuracy with abundant data and compute.

\subsection{Hybrid Architectures}

Hybrid architectures combine convolutional and transformer components to leverage the strengths of both approaches. These models use convolutional layers for early feature extraction, exploiting the parameter efficiency and translation equivariance of convolutions, then apply transformer layers for global reasoning, exploiting the flexibility and long-range modeling of attention.

The Convolutional Vision Transformer (CvT) replaces the patch embedding in ViT with a convolutional stem consisting of several convolutional layers with stride. For a $224 \times 224$ input, the convolutional stem progressively reduces spatial dimensions to $56 \times 56$, $28 \times 28$, and $14 \times 14$ while increasing channels to 64, 192, and 384. At each stage, a transformer layer processes the feature map (treating spatial positions as tokens), then a strided convolution reduces dimensions for the next stage. This hierarchical design combines the parameter efficiency of convolutions with the global modeling of transformers, achieving 81.6\% ImageNet accuracy with only 20 million parameters—4.3× fewer than ViT-Base while matching its accuracy.

The Swin Transformer introduces hierarchical transformers with shifted windows, creating a pyramid structure similar to CNNs. The input is divided into $4 \times 4$ patches (rather than $16 \times 16$ in ViT), creating a sequence of $56 \times 56 = 3{,}136$ tokens. Transformer layers process this sequence using local attention within $7 \times 7$ windows (rather than global attention), reducing computational cost from $O(n^2)$ to $O(n)$. After several layers, adjacent patches are merged to create a $28 \times 28$ sequence with doubled channel dimension, and the process repeats. This hierarchical design achieves 83.3\% ImageNet accuracy with 29 million parameters, outperforming both ResNet-50 and ViT-Base while using fewer parameters than ViT.

The success of hybrid architectures demonstrates that the dichotomy between CNNs and transformers is not absolute. By combining convolutional inductive biases for early processing with transformer flexibility for late processing, hybrid models achieve better parameter efficiency and accuracy than either pure CNNs or pure transformers. This trend suggests that future vision models will increasingly blend architectural components rather than adhering strictly to one paradigm or the other.

\section{Hardware Optimization for Convolutions}
\label{sec:hardware_optimization}

Efficient execution of convolutional networks on modern hardware requires understanding the interaction between algorithm design, memory hierarchy, and specialized compute units. This section examines how convolutions map to GPU architectures and how to maximize hardware utilization.

\subsection{Tensor Core Utilization for Convolutions}

Modern NVIDIA GPUs include Tensor Cores, specialized hardware units that accelerate matrix multiplication for specific data types and dimensions. Understanding how convolutions map to Tensor Cores is essential for achieving peak performance.

Tensor Cores on NVIDIA A100 GPUs perform matrix multiplication on $16 \times 16$ tiles in FP16, producing FP32 accumulation. Each Tensor Core can execute one $16 \times 16 \times 16$ matrix multiplication per clock cycle, computing $C = A \times B$ where $A$ is $16 \times 16$, $B$ is $16 \times 16$, and $C$ is $16 \times 16$. The A100 has 432 Tensor Cores running at 1.41 GHz, providing peak throughput of $432 \times 2 \times 16^3 \times 1.41 \times 10^9 = 312$ TFLOPS in FP16. To achieve this peak throughput, matrix dimensions must be multiples of 16, and the matrices must be large enough to saturate all Tensor Cores.

Convolutions map to Tensor Cores through the im2col transformation described in Section~\ref{sec:conv_computation}. For a convolutional layer with $C_{\text{out}}$ output channels, $C_{\text{in}}$ input channels, kernel size $k \times k$, and output spatial dimensions $H_{\text{out}} \times W_{\text{out}}$, im2col creates a matrix multiplication of shape $(C_{\text{out}}) \times (C_{\text{in}} k^2) \times (H_{\text{out}} W_{\text{out}})$. To achieve high Tensor Core utilization, all three dimensions should be multiples of 16 and sufficiently large.

For the RGB convolution example with $C_{\text{out}} = 64$, $C_{\text{in}} = 3$, $k = 3$, $H_{\text{out}} = W_{\text{out}} = 224$, the matrix multiplication has dimensions $64 \times 27 \times 50{,}176$. The output dimension (64) is a multiple of 16, which is good. The inner dimension (27) is not a multiple of 16, which reduces efficiency: Tensor Cores will pad to 32, wasting 5/32 = 15.6\% of compute. The batch dimension (50,176) is large and a multiple of 16, which is good. Overall, this configuration achieves approximately 80-85\% of peak Tensor Core throughput, limited primarily by the non-multiple-of-16 inner dimension.

To improve Tensor Core utilization, modern CNN architectures use channel counts that are multiples of 16 or 32. ResNet-50 uses channel counts of 64, 128, 256, 512, and 1024, all of which are multiples of 16. EfficientNet uses channel counts like 32, 40, 80, 112, 192, 320, all chosen to be multiples of 8 or 16. These choices ensure that matrix dimensions align with Tensor Core tile sizes, maximizing hardware efficiency. The performance impact is substantial: a convolutional layer with 63 output channels achieves only 75\% of the throughput of a layer with 64 output channels, despite having 98.4\% as many parameters.

Batch size also affects Tensor Core utilization. For the RGB convolution example, the spatial dimension $H_{\text{out}} W_{\text{out}} = 50{,}176$ is large enough to saturate Tensor Cores even with batch size 1. However, for later layers with smaller spatial dimensions, batch size becomes critical. A layer with output dimensions $7 \times 7 = 49$ requires batch size at least 16 to provide sufficient parallelism ($49 \times 16 = 784$ output positions). With batch size 1, this layer achieves only 10-15\% of peak throughput, as most Tensor Cores remain idle. With batch size 32, utilization increases to 60-70\%, and with batch size 128, it reaches 85-90\%. This scaling explains why larger batch sizes improve training throughput: they provide more parallelism to saturate the hardware.

\subsection{cuDNN Optimizations}

NVIDIA's cuDNN library provides highly optimized implementations of convolutional operations, incorporating years of engineering effort to maximize performance on NVIDIA GPUs. Understanding cuDNN's optimization strategies provides insight into how to design efficient CNN architectures.

cuDNN implements multiple convolution algorithms and automatically selects the fastest for each layer configuration. The primary algorithms are: (1) implicit GEMM (im2col-based matrix multiplication), (2) Winograd convolution for $3 \times 3$ and $5 \times 5$ kernels, (3) direct convolution for small batch sizes, and (4) FFT-based convolution for large kernels. For each forward pass, cuDNN benchmarks all applicable algorithms and caches the fastest choice, amortizing the benchmarking cost over many iterations.

For the RGB convolution example with $3 \times 3$ kernel, batch size 32, and $224 \times 224$ spatial dimensions, cuDNN typically selects Winograd convolution, which provides 1.5-2× speedup over implicit GEMM. The Winograd algorithm reduces FLOPs from 173 MFLOPs to 77 MFLOPs (2.25× reduction) and achieves approximately 200 TFLOPS on an A100 GPU, or 64\% of peak throughput. The lower-than-expected utilization (compared to 80-85\% for implicit GEMM) arises because Winograd has higher memory bandwidth requirements and less regular computation patterns, making it harder to saturate Tensor Cores.

For later layers with smaller spatial dimensions, cuDNN typically selects implicit GEMM. A layer with output dimensions $7 \times 7$, 512 input channels, 512 output channels, and $3 \times 3$ kernel has matrix multiplication dimensions $512 \times 4{,}608 \times 49$ (where $4{,}608 = 512 \times 9$). With batch size 32, the batch dimension becomes $49 \times 32 = 1{,}568$, giving dimensions $512 \times 4{,}608 \times 1{,}568$. This configuration achieves approximately 250 TFLOPS on an A100 GPU, or 80\% of peak throughput. The high utilization arises because all dimensions are large and multiples of 16, providing excellent Tensor Core efficiency.

cuDNN also provides fused operations that combine multiple layers into a single GPU kernel, reducing memory traffic. A fused convolution-bias-ReLU kernel computes $\text{ReLU}(\text{Conv}(\mathbf{X}) + \mathbf{b})$ in a single pass, eliminating the need to write intermediate results to memory. For the RGB convolution example, this fusion reduces memory traffic from 375 MB (write conv output, read for bias add, write bias result, read for ReLU, write ReLU output) to 250 MB (write final output only), providing a 1.3-1.5× speedup. cuDNN automatically applies these fusions when possible, but they require that the operation sequence be known at compile time.

The performance impact of cuDNN optimizations is dramatic. A naive convolution implementation in pure CUDA typically achieves 20-40 TFLOPS on an A100 GPU, or 6-13\% of peak throughput. cuDNN's optimized implementations achieve 200-280 TFLOPS, or 64-90\% of peak throughput—a 5-10× speedup. This performance gap explains why all modern deep learning frameworks (PyTorch, TensorFlow, JAX) use cuDNN as their backend for convolutional operations rather than implementing convolutions from scratch.

\subsection{Memory Bandwidth vs Compute}

Understanding whether a convolutional layer is compute-bound or memory-bandwidth-bound is essential for optimization. Compute-bound layers are limited by arithmetic throughput and benefit from algorithmic improvements like Winograd, while memory-bound layers are limited by data transfer rates and benefit from memory optimizations like fusion.

The arithmetic intensity of a convolutional layer is the ratio of FLOPs to bytes transferred: $\text{AI} = \text{FLOPs} / \text{bytes}$. For a layer to be compute-bound on an A100 GPU with 312 TFLOPS FP16 compute and 1.5 TB/s memory bandwidth, the arithmetic intensity must exceed $312 \times 10^{12} / (1.5 \times 10^{12}) = 208$ FLOPs per byte. Layers with lower arithmetic intensity are memory-bound.

For the RGB convolution example, the FLOPs are 173 MFLOPs. The memory transfers include reading the input ($3 \times 224 \times 224 \times 2 = 301{,}056$ bytes in FP16), reading the kernel ($64 \times 3 \times 9 \times 2 = 3{,}456$ bytes), and writing the output ($64 \times 224 \times 224 \times 2 = 6{,}422{,}528$ bytes), totaling 6.7 MB. The arithmetic intensity is $173 \times 10^6 / (6.7 \times 10^6) = 25.8$ FLOPs per byte, which is far below the 208 threshold. This layer is memory-bound: it spends most of its time waiting for data transfers rather than computing.

The memory-bound nature of this layer explains why Winograd provides less than the theoretical 2.25× speedup: reducing FLOPs from 173 MFLOPs to 77 MFLOPs (2.25× reduction) does not proportionally reduce runtime because the layer is limited by memory bandwidth, not compute. The actual speedup is approximately 1.5×, as Winograd reduces some memory traffic through better cache utilization but cannot eliminate the fundamental memory bottleneck.

In contrast, later layers with larger channel counts and smaller spatial dimensions are typically compute-bound. A layer with 512 input channels, 512 output channels, $7 \times 7$ spatial dimensions, and $3 \times 3$ kernel has $2 \times 512 \times 512 \times 9 \times 7 \times 7 = 230{,}686{,}720$ FLOPs. The memory transfers include reading the input ($512 \times 7 \times 7 \times 2 = 50{,}176$ bytes), reading the kernel ($512 \times 512 \times 9 \times 2 = 4{,}718{,}592$ bytes), and writing the output ($512 \times 7 \times 7 \times 2 = 50{,}176$ bytes), totaling 4.8 MB. The arithmetic intensity is $230{,}686{,}720 / (4.8 \times 10^6) = 48$ FLOPs per byte, which is still below the 208 threshold but much higher than the early layer. With batch size 32, the spatial dimension becomes $7 \times 7 \times 32 = 1{,}568$, and the arithmetic intensity increases to $48 \times 32 = 1{,}536$ FLOPs per byte, making the layer strongly compute-bound.

The transition from memory-bound to compute-bound as networks deepen has important implications for optimization. Early layers benefit from memory optimizations like fused operations and efficient data layouts, while late layers benefit from compute optimizations like Tensor Core utilization and algorithmic improvements. Profiling tools like NVIDIA Nsight Systems can identify whether each layer is memory-bound or compute-bound, guiding optimization efforts.

\subsection{Batch Size Impact on Convolution Performance}

Batch size is the primary lever for controlling GPU utilization in convolutional networks. Larger batches provide more parallelism, improving hardware efficiency and throughput measured in images per second. However, larger batches also require more memory and may affect convergence.

For ResNet-50 on an A100 GPU, the relationship between batch size and throughput is approximately logarithmic. With batch size 1, ResNet-50 achieves approximately 140 images per second. At batch size 8, throughput increases to 680 images per second (4.9× improvement). At batch size 32, throughput reaches 1,920 images per second (2.8× improvement). At batch size 128, throughput reaches 3,840 images per second (2.0× improvement). The diminishing returns arise because larger batches improve GPU utilization but eventually become limited by memory bandwidth and kernel launch overhead.

The memory cost of larger batches scales linearly with batch size for parameters and optimizer states (which are independent of batch size) but linearly for activations. For ResNet-50, batch size 1 requires approximately 1.2 GB of GPU memory (100 MB for parameters, 1.1 GB for activations). Batch size 8 requires 3.8 GB (100 MB parameters, 3.7 GB activations). Batch size 32 requires 12.4 GB (100 MB parameters, 12.3 GB activations). Batch size 128 requires 47.6 GB (100 MB parameters, 47.5 GB activations). An A100 GPU with 80 GB of memory can accommodate batch size 128 for ResNet-50, but larger batches require gradient accumulation or distributed training.

The optimal batch size balances throughput, memory usage, and convergence. From a hardware efficiency perspective, larger batches are always better, as they improve GPU utilization and images-per-second throughput. However, from an optimization perspective, very large batches can slow convergence by reducing the number of parameter updates per epoch. Empirically, batch sizes of 256-1024 work well for ResNet-50 on ImageNet, providing good hardware efficiency (70-85\% GPU utilization) while maintaining reasonable convergence speed. Larger batches require careful tuning of learning rate and warmup schedule to maintain training stability and final model accuracy.

\section{Pooling Layers}
\label{sec:pooling}

\begin{definition}[Max Pooling]
\label{def:max_pooling}
For window $k \times k$ and stride $s$:
\begin{equation}
\text{MaxPool}(\mathbf{X})_{i,j} = \max_{m,n \in \text{window}} \mathbf{X}_{si+m, sj+n}
\end{equation}
\end{definition}

Pooling reduces spatial dimensions, increases receptive field, and provides translation invariance.

\section{Classic Architectures}
\label{sec:classic_architectures}

\subsection{VGG-16 (2014)}

Deep network with small $3\times3$ filters. Pattern: $[\text{Conv}3\times3]^n \to \text{MaxPool} \to \text{Double channels}$

Total: 138 million parameters

\subsection{ResNet (2015)}

\begin{definition}[Residual Block]
\label{def:residual_block}
Learn residual:
\begin{equation}
\mathbf{y} = \mathcal{F}(\mathbf{x}) + \mathbf{x}
\end{equation}
\end{definition}

ResNet-50: 25.6M parameters, enables training 100+ layer networks.

\begin{keypoint}
Residual connections enable extremely deep networks by allowing gradients to flow through skip connections. Analogous to skip connections in transformers.
\end{keypoint}

\section{Batch Normalization}
\label{sec:batch_norm}

\begin{definition}[Batch Normalization]
\label{def:batch_norm}
For mini-batch, normalize each feature:
\begin{align}
\hat{\mathbf{x}}_i &= \frac{\mathbf{x}_i - \mu_{\mathcal{B}}}{\sqrt{\sigma^2_{\mathcal{B}} + \epsilon}} \\
\mathbf{y}_i &= \gamma \hat{\mathbf{x}}_i + \beta
\end{align}
where $\gamma, \beta$ are learnable.
\end{definition}

Benefits: Reduces covariate shift, allows higher learning rates, acts as regularization.

\section{Exercises}

\begin{exercise}
For $32\times32\times3$ input, compute dimensions after: Conv(64, $5\times5$, s=1, p=2), MaxPool($2\times2$, s=2), Conv(128, $3\times3$, s=1, p=1), MaxPool($2\times2$, s=2). Count parameters.
\end{exercise}

\begin{exercise}
Show two $3\times3$ convolutions equal one $5\times5$ receptive field. Compare parameter counts.
\end{exercise}

\begin{exercise}
Design CNN for CIFAR-10 with 3 blocks, channels [64, 128, 256]. Calculate total parameters.
\end{exercise}

\section{Solutions}

\begin{solution}[Exercise 1]
Starting with input $32\times32\times3$:

\textbf{Conv1 (64 filters, $5\times5$, stride=1, padding=2):}
\begin{equation}
H_{\text{out}} = \frac{32 + 2(2) - 5}{1} + 1 = \frac{32 + 4 - 5}{1} + 1 = 32
\end{equation}
Output: $32\times32\times64$

Parameters: $(5 \times 5 \times 3 + 1) \times 64 = 76 \times 64 = 4{,}864$

\textbf{MaxPool1 ($2\times2$, stride=2):}
\begin{equation}
H_{\text{out}} = \frac{32 - 2}{2} + 1 = 16
\end{equation}
Output: $16\times16\times64$

Parameters: 0 (pooling has no learnable parameters)

\textbf{Conv2 (128 filters, $3\times3$, stride=1, padding=1):}
\begin{equation}
H_{\text{out}} = \frac{16 + 2(1) - 3}{1} + 1 = 16
\end{equation}
Output: $16\times16\times128$

Parameters: $(3 \times 3 \times 64 + 1) \times 128 = 577 \times 128 = 73{,}856$

\textbf{MaxPool2 ($2\times2$, stride=2):}
\begin{equation}
H_{\text{out}} = \frac{16 - 2}{2} + 1 = 8
\end{equation}
Output: $8\times8\times128$

\textbf{Total parameters:} $4{,}864 + 73{,}856 = 78{,}720$
\end{solution}

\begin{solution}[Exercise 2]
\textbf{Receptive field analysis:}

\textbf{Single $5\times5$ convolution:}
\begin{itemize}
    \item Receptive field: $5\times5 = 25$ pixels
    \item Parameters per output channel: $5 \times 5 \times C_{\text{in}} + 1$
    \item For $C_{\text{in}} = C_{\text{out}} = 64$: $(25 \times 64 + 1) \times 64 = 102{,}464$ parameters
\end{itemize}

\textbf{Two $3\times3$ convolutions:}
\begin{itemize}
    \item First $3\times3$ conv: receptive field $3\times3$
    \item Second $3\times3$ conv: each output pixel sees $3\times3$ region of previous layer
    \item Each pixel in previous layer sees $3\times3$ region of input
    \item Total receptive field: $3 + (3-1) = 5$ in each dimension, so $5\times5$
\end{itemize}

\textbf{Parameter count for two $3\times3$ convolutions:}
\begin{itemize}
    \item First conv: $(3 \times 3 \times 64 + 1) \times 64 = 36{,}928$ parameters
    \item Second conv: $(3 \times 3 \times 64 + 1) \times 64 = 36{,}928$ parameters
    \item Total: $73{,}856$ parameters
\end{itemize}

\textbf{Comparison:}
\begin{equation}
\text{Reduction} = \frac{102{,}464 - 73{,}856}{102{,}464} \approx 28\%
\end{equation}

Two $3\times3$ convolutions achieve the same receptive field as one $5\times5$ with 28\% fewer parameters, plus an additional nonlinearity between them, increasing representational power.
\end{solution}

\begin{solution}[Exercise 3]
\textbf{CNN architecture for CIFAR-10 (10 classes):}

Input: $32\times32\times3$

\textbf{Block 1 (64 channels):}
\begin{itemize}
    \item Conv: $3\times3$, 64 filters, stride=1, padding=1 $\to$ $32\times32\times64$
    \item Conv: $3\times3$, 64 filters, stride=1, padding=1 $\to$ $32\times32\times64$
    \item MaxPool: $2\times2$, stride=2 $\to$ $16\times16\times64$
\end{itemize}

Parameters:
\begin{itemize}
    \item Conv1: $(3 \times 3 \times 3 + 1) \times 64 = 1{,}792$
    \item Conv2: $(3 \times 3 \times 64 + 1) \times 64 = 36{,}928$
    \item Block 1 total: $38{,}720$
\end{itemize}

\textbf{Block 2 (128 channels):}
\begin{itemize}
    \item Conv: $3\times3$, 128 filters, stride=1, padding=1 $\to$ $16\times16\times128$
    \item Conv: $3\times3$, 128 filters, stride=1, padding=1 $\to$ $16\times16\times128$
    \item MaxPool: $2\times2$, stride=2 $\to$ $8\times8\times128$
\end{itemize}

Parameters:
\begin{itemize}
    \item Conv1: $(3 \times 3 \times 64 + 1) \times 128 = 73{,}856$
    \item Conv2: $(3 \times 3 \times 128 + 1) \times 128 = 147{,}584$
    \item Block 2 total: $221{,}440$
\end{itemize}

\textbf{Block 3 (256 channels):}
\begin{itemize}
    \item Conv: $3\times3$, 256 filters, stride=1, padding=1 $\to$ $8\times8\times256$
    \item Conv: $3\times3$, 256 filters, stride=1, padding=1 $\to$ $8\times8\times256$
    \item MaxPool: $2\times2$, stride=2 $\to$ $4\times4\times256$
\end{itemize}

Parameters:
\begin{itemize}
    \item Conv1: $(3 \times 3 \times 128 + 1) \times 256 = 295{,}168$
    \item Conv2: $(3 \times 3 \times 256 + 1) \times 256 = 590{,}080$
    \item Block 3 total: $885{,}248$
\end{itemize}

\textbf{Classifier:}
\begin{itemize}
    \item Global Average Pooling: $4\times4\times256 \to 1\times1\times256$
    \item Fully connected: $256 \to 10$
    \item Parameters: $256 \times 10 + 10 = 2{,}570$
\end{itemize}

\textbf{Total parameters:}
\begin{equation}
38{,}720 + 221{,}440 + 885{,}248 + 2{,}570 = 1{,}147{,}978 \approx 1.15\text{M parameters}
\end{equation}
\end{solution}

