<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 23: Best Practices - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../../deeptech.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Best Practices and Production Case Studies</h1>

<h2>Chapter Overview</h2>

<p>This final chapter synthesizes practical wisdom from deploying transformers at scale. We cover debugging strategies, hyperparameter tuning, common pitfalls, and real-world case studies from industry deployments of BERT, GPT, and other transformer models.</p>

<h3>Learning Objectives</h3>

<ol>
    <li>Apply systematic debugging for transformer training
    <li>Tune hyperparameters effectively
    <li>Avoid common pitfalls in architecture and training
    <li>Learn from real-world deployment case studies
    <li>Design robust production systems
    <li>Plan future-proof transformer architectures
</ol>

<h2>Model Selection</h2>

<p>Choosing the right transformer architecture is a critical decision that impacts both performance and resource requirements. This section provides a systematic framework for selecting among the major transformer variants based on task requirements, data availability, and computational constraints.</p>

<h3>Architecture Selection Framework</h3>

<p>The choice between BERT, GPT, T5, and other architectures depends fundamentally on the nature of your task. BERT and its variants excel at understanding tasks where bidirectional context is crucial, such as classification, named entity recognition, and question answering. The bidirectional attention mechanism allows BERT to build rich representations by attending to both past and future tokens simultaneously, making it particularly effective when the entire input is available at once.</p>

<p>GPT models, in contrast, are designed for generation tasks where autoregressive decoding is required. The unidirectional attention pattern makes GPT natural for text generation, code completion, and any task where outputs must be produced sequentially. While GPT can be adapted for understanding tasks through careful prompting, this is generally less efficient than using a bidirectional model designed for the purpose.</p>

<p>T5 represents a unified approach that frames all tasks as sequence-to-sequence problems. This architecture provides flexibility across both understanding and generation tasks, making it an excellent choice when you need a single model to handle diverse task types. The encoder-decoder structure allows T5 to leverage bidirectional attention in the encoder while maintaining autoregressive generation in the decoder.</p>

<h3>Model Size Selection</h3>

<div class="architecture-diagram">
<h3>Model Development Pipeline</h3>
<pre class="mermaid">
graph LR
    DATA["Training Data"] --> TOK["Tokenize<br/> Vocab V"]
    TOK --> BASE["Baseline Model<br/> small config"]
    BASE -->|"Scale up"| TRAIN["Full Training<br/> Memory budget:<br/> Params: 2P (FP16)<br/> Grads: 2P<br/> Adam: 8P<br/> Acts: O(L*n*d)"]
    LR["Hyperparams<br/> lr, warmup, wd"] --> TRAIN
    TRAIN --> EVAL["Evaluate<br/> Loss, Perplexity"]
    EVAL -->|"Tune"| LR

    style DATA fill:#e8f5e9,stroke:#4caf50,color:#000
    style LR fill:#fff3e0,stroke:#ff9800,color:#000
    style TRAIN fill:#e3f2fd,stroke:#2196f3,color:#000
    style EVAL fill:#f3e5f5,stroke:#9c27b0,color:#000
</pre>
<p class="diagram-caption">Model Development Pipeline</p>
</div>

<p>Selecting the appropriate model size requires balancing performance requirements against computational constraints. The relationship between model size and performance generally follows a power law, with diminishing returns as models grow larger. For most practical applications, the base-sized models provide an excellent balance between capability and efficiency.</p>

<p>BERT-base with 110 million parameters serves as the standard choice for most understanding tasks. It provides strong performance across a wide range of benchmarks while remaining tractable for fine-tuning on a single GPU. BERT-large with 340 million parameters offers modest improvements, typically 1-3 percentage points on downstream tasks, but requires significantly more memory and computation. The large variant is justified primarily when you need to extract maximum performance and have sufficient computational resources.</p>

<p>For GPT models, the size selection depends heavily on the complexity of the generation task. GPT-2 small (117M parameters) suffices for simple completion tasks and domain-specific generation after fine-tuning. GPT-2 medium (345M parameters) provides better coherence for longer generations and more complex tasks. The larger variants (GPT-2 large at 774M and GPT-2 XL at 1.5B parameters) are necessary primarily when working with limited task-specific data, as their stronger pre-trained representations enable better few-shot performance.</p>

<h3>Pre-trained versus Training from Scratch</h3>

<p>The decision to use pre-trained models versus training from scratch depends on data availability, domain specificity, and computational budget. In nearly all cases, starting from pre-trained weights is the correct choice. Pre-training on large corpora provides general language understanding that transfers effectively to downstream tasks, and the computational cost of pre-training from scratch is prohibitive for most organizations.</p>

<p>Training from scratch becomes viable only in specific circumstances. When working with highly specialized domains where general language models perform poorly, such as medical text with extensive jargon or programming languages not well-represented in pre-training data, domain-specific pre-training may be justified. However, even in these cases, continued pre-training from existing checkpoints is typically more efficient than starting from random initialization.</p>

<p>The computational cost difference is substantial. Pre-training BERT-base from scratch requires approximately 64 TPU days or equivalent GPU time, representing tens of thousands of dollars in compute costs. Fine-tuning the same model on a downstream task typically requires only hours on a single GPU, costing tens of dollars. This thousand-fold difference in cost makes pre-trained models the default choice for nearly all applications.</p>

<h3>Cost-Benefit Analysis</h3>

<p>A systematic cost-benefit analysis should consider both direct computational costs and opportunity costs. For a typical classification task with 10,000 labeled examples, fine-tuning BERT-base requires approximately 2-4 hours on a single V100 GPU, costing roughly \$10-20 in cloud compute. This investment typically yields performance improvements of 5-15 percentage points over traditional methods like logistic regression on TF-IDF features.</p>

<p>Training a smaller model from scratch on the same data might require 8-16 hours and cost \$40-80, while likely achieving inferior performance due to the lack of pre-trained representations. The pre-trained approach thus provides both better performance and lower cost, a rare combination that explains the dominance of transfer learning in modern NLP.</p>

<p>For generation tasks, the cost analysis shifts somewhat. Fine-tuning GPT-2 medium on a specific generation task requires 4-8 hours on a V100, costing \$20-40. However, inference costs become more significant for generation, as producing each token requires a full forward pass through the model. For applications requiring high-throughput generation, the ongoing inference costs may exceed training costs within weeks or months of deployment, making inference optimization critical.</p>

<h2>Training Best Practices</h2>

<p>Effective training of transformer models requires careful attention to hyperparameter selection, monitoring, and debugging. This section provides comprehensive guidance on the key decisions that impact training success.</p>

<h3>Learning Rate Selection</h3>

<div style="text-align: center;">

<table>
<tr><th><strong>Scenario</strong></th><th><strong>Range</strong></th><th><strong>Notes</strong></th></tr>
<tr><td>BERT fine-tuning</td><td>$1$--$5 \times 10^{-5}$</td><td>Lower end for small datasets</td></tr>
<tr><td>GPT fine-tuning</td><td>$2 \times 10^{-5}$--$10^{-4}$</td><td>Autoregressive is more stable</td></tr>
<tr><td>Pre-training from scratch</td><td>$10^{-4}$--$6 \times 10^{-4}$</td><td>Requires longer warmup</td></tr>
</table>

<p></div>

<p><strong>Rules of thumb:</strong> Scale LR $\sim$linearly with batch size (use LAMB for very large batches). Warmup: 5--10\% of steps for fine-tuning, 10K--50K steps for pre-training.</p>

<h3>Batch Size Selection</h3>

<div style="text-align: center;">

<table>
<tr><th><strong>Scenario</strong></th><th><strong>Typical Range</strong></th><th><strong>Notes</strong></th></tr>
<tr><td>Fine-tuning (single GPU)</td><td>16--32</td><td>Gradient accumulation for larger effective batch</td></tr>
<tr><td>Fine-tuning (multi-GPU)</td><td>32--256</td><td>Scale LR accordingly</td></tr>
<tr><td>Pre-training</td><td>256--4096</td><td>Requires LR warmup + LAMB</td></tr>
</table>

<p></div>

<p>If memory-limited, use gradient accumulation: $B_{\text{eff}} = B_{\text{micro}} \times N_{\text{accum}}$ (see Chapter~[ref] for implementation).</p>

<h3>Checkpointing and Monitoring Strategy</h3>

<p>Effective monitoring is essential for detecting problems early and understanding training dynamics. At minimum, you should track training loss, validation loss, and task-specific metrics at regular intervals. Logging every 100-500 steps provides sufficient granularity to detect issues without generating excessive data.</p>

<p>Checkpointing strategy depends on training duration and stability. For short fine-tuning runs of a few hours, saving checkpoints every epoch is sufficient. For longer training runs, save checkpoints every few thousand steps to protect against hardware failures and enable recovery from divergence. Always keep at least the three most recent checkpoints, as the most recent checkpoint may be corrupted or represent a point after training has diverged.</p>

<p>Beyond basic loss monitoring, tracking gradient norms provides early warning of training instability. Gradient norms should remain relatively stable throughout training, typically in the range of 0.1 to 10.0. Sudden spikes in gradient norm often precede loss divergence and indicate that gradient clipping or learning rate reduction may be necessary. Similarly, monitoring the ratio of update magnitude to parameter magnitude helps ensure that learning rates are appropriate.</p>

<h2>Memory Management</h2>

<p>Memory is often the primary constraint in transformer training. Rather than repeating the detailed memory analysis from Chapter~21, this section provides a quick-reference decision guide.</p>

<h3>Out-of-Memory Decision Checklist</h3>

<p>When encountering memory errors, apply these steps in order:</p>

<ol>
    <li><strong>Reduce sequence length</strong> (if task permits). Attention memory scales quadratically with sequence length---truncating from 512 to 256 tokens saves $\sim$4√ó attention memory. Many classification tasks work well at 128 tokens.
    <li><strong>Enable mixed precision</strong> (<code>torch.cuda.amp</code>). Halves activation and gradient memory with 2--3√ó speedup on tensor-core GPUs. Minimal code changes required. See Chapter~21 for implementation.
    <li><strong>Enable gradient checkpointing.</strong> Trades 20--30\% additional compute time for 40--50\% activation memory reduction. Apply via <code>torch.utils.checkpoint</code>.
    <li><strong>Reduce batch size and use gradient accumulation.</strong> Maintain effective batch size $B_{\text{eff}} = B_{\text{micro}} \times N_{\text{accum}}$ while fitting in memory. Linear memory savings.
    <li><strong>Consider model parallelism.</strong> When the model itself exceeds single-GPU memory, use pipeline parallelism (split by layers) or tensor parallelism (split within layers). Frameworks: DeepSpeed, Megatron-LM. See Chapter~22 for multi-GPU strategies.
</ol>

<h3>Memory Estimation Rule of Thumb</h3>

<p>Total training memory (GB) $\approx$ (Parameters $\times$ 16 bytes) + (Batch $\times$ SeqLen $\times$ Hidden $\times$ Layers $\times$ 40 bytes). The first term covers parameters, gradients, and optimizer states; the second covers activations. For BERT-base (110M params, batch 32, seq 512): $\sim$8 GB.</p>

<h2>Debugging Transformers</h2>

<h3>Systematic Debugging Workflow</h3>

<p><strong>Level 1: Data sanity checks</strong>
<ol>
    <li>Visualize input samples
    <li>Verify labels are correct
    <li>Check for data leakage
    <li>Validate preprocessing
</ol>

<p><strong>Level 2: Model sanity checks</strong>
<ol>
    <li>Overfit single batch (should reach near-zero loss)
    <li>Check gradient flow (no dead neurons)
    <li>Verify shapes at each layer
    <li>Test with minimal model first
</ol>

<p><strong>Level 3: Training dynamics</strong>
<ol>
    <li>Monitor loss curves (training + validation)
    <li>Track gradient norms
    <li>Visualize attention weights
    <li>Check learning rate schedule
</ol>

<div class="example"><strong>Example:</strong> 
<strong>Symptom:</strong> Loss not decreasing

<p><strong>Diagnose:</strong>
<ul>
    <li>Learning rate too low? Try 10√ó higher
    <li>Frozen layers? Check requires\_grad
    <li>Optimizer issue? Try SGD as baseline
    <li>Bad initialization? Re-initialize
    <li>Data issue? Manually inspect batches
</ul>

<p><strong>Symptom:</strong> NaN loss</p>

<p><strong>Diagnose:</strong>
<ul>
    <li>Gradient explosion? Add clipping
    <li>Numerical instability? Check mask values ($-\infty$ vs $-1e9$)
    <li>Learning rate too high? Reduce 10√ó
    <li>Mixed precision issue? Check loss scaling
</ul>
</div>

<h3>Gradient Analysis</h3>

<p>Monitor per-layer gradient norms throughout training (see Chapter~[ref] for PyTorch profiling tools). Healthy gradients have norms between $10^{-4}$ and $10^{1}$, are similar across layers, and are non-zero for all layers. Sudden spikes precede divergence; vanishing gradients indicate dead layers.</p>

<h3>Common Training Issues: Quick Reference</h3>

<div style="text-align: center;">

<table>
<tr><th><strong>Symptom</strong></th><th><strong>Likely Cause</strong></th><th><strong>Fix</strong></th></tr>
<tr><td>Out of memory</td><td>Batch/sequence too large</td><td>See memory checklist (Section~[ref])</td></tr>
<tr><td>Loss not decreasing</td><td>Learning rate too low</td><td>Increase LR 3--10$\times$; verify overfit on 1 batch</td></tr>
<tr><td>Loss diverges / NaN</td><td>LR too high or no clipping</td><td>Reduce LR; clip gradients to norm 1.0</td></tr>
<tr><td>Slow training</td><td>Low GPU utilization</td><td>Increase batch size; add DataLoader workers</td></tr>
<tr><td>Train/val gap growing</td><td>Overfitting</td><td>More dropout; data augmentation; smaller model</td></tr>
</table>

<p></div>

<h2>Inference Optimization</h2>

<p>Inference costs often exceed training costs over a model's lifetime. This section provides decision tables for choosing optimization strategies; see Chapters~21 and~22 for detailed implementations.</p>

<h3>Optimizing for Latency</h3>

<div style="text-align: center;">

<table>
<tr><th><strong>Technique</strong></th><th><strong>Speedup</strong></th><th><strong>Accuracy Cost</strong></th><th><strong>Effort</strong></th></tr>
<tr><td>FP16 inference</td><td>1.5--2$\times$</td><td>$<$0.1\%</td><td>Minimal</td></tr>
<tr><td>INT8 quantization (PTQ)</td><td>2--4$\times$</td><td>0.5--2\%</td><td>Low (calibration)</td></tr>
<tr><td>INT8 quantization (QAT)</td><td>2--4$\times$</td><td>$<$0.5\%</td><td>Medium (retraining)</td></tr>
<tr><td>KV caching (autoregressive)</td><td>5--10$\times$</td><td>None</td><td>Low</td></tr>
<tr><td>TorchScript / torch.compile</td><td>1.2--1.5$\times$</td><td>None</td><td>Low</td></tr>
<tr><td>TensorRT compilation</td><td>2--5$\times$</td><td>$<$0.5\%</td><td>Medium</td></tr>
</table>

<p></div>

<h3>Optimizing for Throughput</h3>

<ul>
    <li><strong>Dynamic batching:</strong> Accumulate requests for 10--50\,ms, process together. Improves GPU utilization from 20--30\% to 70--90\%.
    <li><strong>ONNX Runtime / TensorRT:</strong> Graph-level optimizations provide 1.5--5$\times$ throughput gains via operator fusion and kernel selection.
    <li><strong>Model distillation:</strong> Train a smaller student (e.g., 6-layer DistilBERT retains 97\% of BERT-base accuracy at 1.6$\times$ speed). Combine with quantization for 5--10$\times$ cumulative speedup.
    <li><strong>Continuous batching (vLLM):</strong> For autoregressive generation, allow new requests to join in-flight batches as others complete. See Chapter~22.
</ul>

<h3>Hardware Selection Summary</h3>

<div style="text-align: center;">

<table>
<tr><th><strong>Scenario</strong></th><th><strong>Recommended Hardware</strong></th><th><strong>Rationale</strong></th></tr>
<tr><td>$<$10 req/s, small model</td><td>CPU</td><td>Lower cost, sufficient throughput</td></tr>
<tr><td>10--100 req/s</td><td>T4 GPU + TensorRT</td><td>Good latency/cost balance</td></tr>
<tr><td>$>$100 req/s</td><td>A10/A100 GPU</td><td>Maximum throughput</td></tr>
<tr><td>Edge / mobile</td><td>INT8/INT4 on device</td><td>Memory and power constrained</td></tr>
</table>

<p></div>

<p>See Chapter~22 for detailed hardware analysis and cost breakdowns.</p>

<h2>Cost Optimization</h2>

<p>Understanding and optimizing costs is essential for sustainable deployment of transformer models. This section provides detailed analysis of training and inference costs with concrete examples.</p>

<h3>Training Cost Analysis</h3>

<p>Training costs depend on model size, dataset size, and hardware selection. For BERT-base pre-training on 16 GB of text, the original paper reports using 16 TPU chips for 4 days, equivalent to approximately 64 TPU days. At current Google Cloud pricing of roughly \$8 per TPU hour, this amounts to approximately \$12,000 for pre-training. Using equivalent GPU resources (64 V100 GPUs for 4 days) would cost approximately \$15,000 at on-demand rates.</p>

<p>Fine-tuning costs are much more modest. Training BERT-base on a typical classification task with 10,000 examples requires 2-4 hours on a single V100 GPU. At AWS on-demand pricing of approximately \$3 per hour for a p3.2xlarge instance, this amounts to \$6-12 per fine-tuning run. Even with extensive hyperparameter search involving 20-30 runs, total costs remain under \$300.</p>

<p>Cloud versus on-premise costs depend heavily on utilization. For continuous training workloads, purchasing GPUs becomes cost-effective after 12-18 months of use. A DGX A100 system costs approximately \$200,000 but provides compute equivalent to \$15,000 per month at cloud on-demand rates. For intermittent workloads or experimentation, cloud computing is more economical due to the flexibility to scale up and down.</p>

<p>Spot instances provide substantial savings for training workloads that can tolerate interruptions. AWS spot instances for p3.2xlarge typically cost 50-70\% less than on-demand rates, reducing fine-tuning costs to \$2-4 per run. Implementing checkpointing and automatic restart logic allows training to resume after spot instance interruptions, making this an attractive option for cost-conscious training.</p>

<h3>Training Time Estimation</h3>

<p>Estimating training time helps with planning and cost prediction. For fine-tuning, a useful rule of thumb is that BERT-base processes approximately 100-150 examples per second on a V100 GPU with batch size 32 and sequence length 128. For a dataset of 100,000 examples trained for 3 epochs, this translates to 2,000-3,000 seconds or roughly 1 hour of training time.</p>

<p>Pre-training time scales with dataset size and model size. BERT-base pre-training on 16 GB of text requires approximately 1 million training steps with batch size 256, processing roughly 4 billion tokens. At 1,000 tokens per second per V100 GPU, this requires 4 million GPU-seconds or approximately 1,100 GPU-hours. With 16 GPUs, this translates to roughly 70 hours or 3 days of training.</p>

<p>Larger models scale approximately linearly with parameter count for training time. GPT-2 medium with 345 million parameters takes roughly 3√ó longer to train than GPT-2 small with 117 million parameters, assuming the same dataset and batch size. However, larger models often benefit from larger batch sizes, which can partially offset the increased time per step.</p>

<h3>Inference Cost Analysis</h3>

<p>Inference costs depend on request volume, latency requirements, and model size. For a BERT-base classification service processing 1 million requests per day with average latency requirements of 100ms, a single V100 GPU can handle approximately 100 requests per second with dynamic batching, or 8.6 million requests per day. This suggests that a single GPU is sufficient, costing approximately \$200-300 per month for a cloud GPU instance.</p>

<p>For generation tasks, costs are higher due to the sequential nature of autoregressive decoding. GPT-2 medium generating 100 tokens per request can process approximately 10-20 requests per second per GPU, depending on batch size and sequence length. For 1 million requests per day, this requires 1-2 GPUs, costing \$400-600 per month. The cost per million tokens is approximately \$5-10 for self-hosted inference.</p>

<p>Comparing self-hosted to API costs reveals significant differences at scale. OpenAI's GPT-3.5 API costs approximately \$2 per million tokens for input and output combined. For applications processing 100 million tokens per month, this amounts to \$200 per month. Self-hosting a comparable model would require 4-8 GPUs costing \$1,600-3,200 per month, making the API more economical at this scale. However, at 1 billion tokens per month, self-hosting becomes competitive, and at 10 billion tokens per month, self-hosting is clearly more economical.</p>

<h3>Cost Optimization Strategies</h3>

<p>Several strategies can substantially reduce both training and inference costs. For training, using mixed precision reduces training time by 2-3√ó, directly reducing costs by the same factor. Gradient accumulation allows using smaller, cheaper GPU instances by simulating larger batch sizes. Spot instances reduce costs by 50-70\% for workloads that can tolerate interruptions.</p>

<p>For inference, quantization and distillation reduce both latency and cost. A distilled and quantized model may achieve 5-10√ó higher throughput than the original model, allowing a single GPU to handle the load that previously required 5-10 GPUs. This directly translates to 5-10√ó cost reduction. Dynamic batching improves GPU utilization from 20-30\% to 70-90\%, effectively tripling throughput without additional hardware.</p>

<p>Caching can dramatically reduce inference costs for applications with repeated queries. If 30\% of requests are duplicates or near-duplicates, caching responses eliminates 30\% of inference costs. Semantic caching using embedding similarity can extend this to near-duplicate queries, potentially caching 50-70\% of requests in some applications.</p>

<p>Autoscaling based on demand prevents paying for idle resources during low-traffic periods. For applications with diurnal traffic patterns, autoscaling can reduce costs by 40-60\% compared to provisioning for peak load. Kubernetes and cloud-native deployment platforms make autoscaling straightforward to implement.</p>

<h2>Production Deployment</h2>

<p>Production deployment involves serving infrastructure, monitoring, and safe rollout practices. Chapter~22 covers serving frameworks (TorchServe, Triton, vLLM), deployment architectures (Ray Serve, Kubernetes), and KV cache management in detail. Here we summarize the key decision points.</p>

<h3>Deployment Checklist</h3>

<p><strong>Before deployment:</strong>
<ol>
    <li>Optimize model: quantize (INT8/FP16), export to ONNX or TensorRT, enable KV caching for generation.
    <li>Benchmark under realistic conditions (expected batch sizes, sequence lengths, peak load).
    <li>Set up monitoring: latency percentiles (p50, p95, p99), throughput, error rate, GPU utilization.
    <li>Plan scaling: autoscaling rules, load balancing, maximum replica count.
</ol>

<p><strong>Safe rollout:</strong> Use canary deployment (1--5\% traffic) or shadow mode (run new model alongside production without serving results) before full rollout. Monitor key metrics for several hours before increasing traffic. Maintain instant rollback capability.</p>

<h2>Practical Checklists</h2>

<p>These checklists provide systematic guidance for common transformer workflows, helping ensure that critical steps are not overlooked.</p>

<h3>Before Training Checklist</h3>

<p>Before beginning training, verify that you have made appropriate decisions about resources and configuration. Estimate memory requirements using the formulas provided earlier, ensuring that your chosen batch size and sequence length will fit in available GPU memory with some margin for safety. Select hardware appropriate for your model size and training duration, considering the trade-offs between cost and training time.</p>

<p>Choose batch size and sequence length based on your task requirements and memory constraints. Remember that sequence length has a quadratic effect on memory, so reducing it provides substantial savings if your task permits. Set up monitoring and logging infrastructure before training begins, as debugging issues after the fact is much more difficult than catching them in real-time.</p>

<p>Estimate training time and cost using the guidelines provided earlier. This helps with planning and ensures that you have allocated sufficient budget and time for the training run. For long training runs, verify that checkpointing is configured correctly and test recovery from checkpoints before committing to the full training run.</p>

<h3>During Training Checklist</h3>

<p>While training is in progress, monitor loss and metrics regularly to detect issues early. Training loss should decrease steadily, though not necessarily monotonically. Validation loss should track training loss initially, with some divergence expected as training progresses. If validation loss increases while training loss decreases, you may be overfitting.</p>

<p>Check GPU utilization to ensure that you are using resources efficiently. Utilization should be consistently above 80\% during training. Lower utilization suggests that batch size is too small, data loading is a bottleneck, or there are inefficiencies in the training loop. Monitor memory usage to ensure you are not close to OOM errors, which can cause training to fail unexpectedly.</p>

<p>Save checkpoints regularly according to your checkpointing strategy. Verify that checkpoints are being saved successfully and that you can load them for recovery. Validate periodically on a held-out set to track generalization performance. The frequency of validation depends on training duration, but every few hundred steps or every epoch is typical.</p>

<h3>Before Deployment Checklist</h3>

<p>Before deploying a model to production, optimize it for inference using the techniques described earlier. Apply quantization if accuracy permits, as the performance benefits are substantial. Consider distillation if you need further speedup and have time for the additional training. Export the model to an optimized format like ONNX or TensorRT if using those serving frameworks.</p>

<p>Benchmark latency and throughput under realistic conditions, including the batch sizes and sequence lengths you expect in production. Test with both average-case and worst-case inputs to understand performance variability. Estimate serving costs based on expected request volume and the hardware required to meet latency requirements.</p>

<p>Set up monitoring and alerting for the production deployment. Ensure that you can track request rate, latency, error rate, and resource utilization. Configure alerts for anomalies in these metrics. Plan your scaling strategy, including autoscaling rules if using dynamic scaling.</p>

<p>Test the deployment pipeline end-to-end, including model loading, preprocessing, inference, and postprocessing. Verify that error handling works correctly and that failures are logged appropriately. Conduct load testing to ensure the system can handle expected traffic with appropriate margins for spikes.</p>

<h2>Hyperparameter Tuning</h2>

<h3>Critical Hyperparameters (Ordered by Impact)</h3>

<p><strong>1. Learning Rate (highest impact)</strong>
<ul>
    <li>Typical range: $[10^{-5}, 10^{-3}]$
    <li>BERT: $1-5 \times 10^{-5}$
    <li>GPT: $2-6 \times 10^{-4}$
    <li>Rule: Larger models need smaller LR
</ul>

<p><strong>2. Batch Size</strong>
<ul>
    <li>Trade-off: Speed vs generalization
    <li>Typical: 32-512 for fine-tuning, 256-2048 for pre-training
    <li>Scale LR linearly with batch size
</ul>

<p><strong>3. Warmup Steps</strong>
<ul>
    <li>Typical: 5-10\% of total training steps
    <li>BERT: 10,000 steps
    <li>GPT-3: 375M tokens (out of 300B)
</ul>

<p><strong>4. Weight Decay</strong>
<ul>
    <li>Typical: $0.01$ to $0.1$
    <li>AdamW: Decouple from learning rate
</ul>

<p><strong>5. Dropout</strong>
<ul>
    <li>Standard: $0.1$
    <li>Larger models: Lower dropout (0.05 or none)
    <li>Apply uniformly (attention, FFN, embeddings)
</ul>

<h3>Tuning Strategy</h3>

<p><strong>Phase 1: Coarse search</strong>
<ul>
    <li>Grid/random search over wide ranges
    <li>Short runs (10\% of full training)
    <li>Focus on learning rate first
</ul>

<p><strong>Phase 2: Fine search</strong>
<ul>
    <li>Narrow ranges around best from Phase 1
    <li>Longer runs (50\% of full training)
    <li>Tune other hyperparameters
</ul>

<p><strong>Phase 3: Validation</strong>
<ul>
    <li>Full training with best settings
    <li>Multiple seeds for robustness
    <li>Final evaluation on test set
</ul>

<div class="example"><strong>Example:</strong> 
<strong>Task:</strong> Fine-tune BERT on classification

<p><strong>Coarse search:</strong>
<ul>
    <li>Try: $[10^{-5}, 3 \times 10^{-5}, 10^{-4}, 3 \times 10^{-4}]$
    <li>Train 1 epoch each
    <li>Best: $3 \times 10^{-5}$ (85.2\% dev accuracy)
</ul>

<p><strong>Fine search:</strong>
<ul>
    <li>Try: $[2 \times 10^{-5}, 3 \times 10^{-5}, 4 \times 10^{-5}]$
    <li>Train 3 epochs each
    <li>Best: $3 \times 10^{-5}$ (86.1\% dev accuracy)
</ul>

<p><strong>Final:</strong>
<ul>
    <li>Train with $\text{LR} = 3 \times 10^{-5}$, 5 epochs
    <li>Test accuracy: 85.8\%
</ul>
</div>

<h2>Common Pitfalls and Solutions</h2>

<h3>Architecture Pitfalls</h3>

<p><strong>Pitfall 1: Forgetting positional information</strong>
<ul>
    <li>Symptom: Model treats sequence as bag-of-words
    <li>Solution: Verify position encoding is added
</ul>

<p><strong>Pitfall 2: Incorrect masking</strong>
<ul>
    <li>Symptom: Information leakage or blocked attention
    <li>Solution: Visualize attention matrices, verify mask shape
</ul>

<p><strong>Pitfall 3: Not sharing embeddings</strong>
<ul>
    <li>Symptom: Twice as many parameters as expected
    <li>Solution: Weight tying between input/output embeddings
</ul>

<h3>Training Pitfalls</h3>

<p><strong>Pitfall 4: Insufficient warmup</strong>
<ul>
    <li>Symptom: Training unstable early, doesn't recover
    <li>Solution: Increase warmup to 10\% of training
</ul>

<p><strong>Pitfall 5: Wrong learning rate scale</strong>
<ul>
    <li>Symptom: Loss not decreasing or diverging
    <li>Solution: Learning rate finder, try 10√ó up/down
</ul>

<p><strong>Pitfall 6: Overfitting small datasets</strong>
<ul>
    <li>Symptom: Large train/val gap
    <li>Solution: More dropout, data augmentation, smaller model
</ul>

<h3>Deployment Pitfalls</h3>

<p><strong>Pitfall 7: Batch size 1 in production</strong>
<ul>
    <li>Symptom: Poor GPU utilization
    <li>Solution: Dynamic batching, accumulate requests
</ul>

<p><strong>Pitfall 8: Not using mixed precision</strong>
<ul>
    <li>Symptom: Slow inference, high memory
    <li>Solution: FP16 inference, quantization
</ul>

<p><strong>Pitfall 9: No KV caching for generation</strong>
<ul>
    <li>Symptom: Slow text generation (quadratic in length)
    <li>Solution: Cache key/value tensors
</ul>

<h2>Case Study: BERT for Search Ranking</h2>

<h3>Problem Setup</h3>

<p><strong>Task:</strong> Rank search results by relevance</p>

<p><strong>Input:</strong> Query + Document pairs</p>

<p><strong>Output:</strong> Relevance score [0, 1]</p>

<h3>Architecture Decisions</h3>

<p><strong>Model:</strong> BERT-base with regression head</p>

<strong>Input format:</strong>
<pre><code>
[CLS] query tokens [SEP] document tokens [SEP]
</code></pre>

<p><strong>Output:</strong> $\text{score} = \sigma(\mW \vh_{\text{[CLS]}} + b)$</p>

<h3>Training Strategy</h3>

<p><strong>Data:</strong>
<ul>
    <li>10M query-document pairs
    <li>Labels: Click-through rate (0-1)
    <li>Hard negatives: Top results without clicks
</ul>

<p><strong>Loss:</strong> Mean squared error on CTR prediction</p>

<p><strong>Optimization:</strong>
<ul>
    <li>Learning rate: $2 \times 10^{-5}$
    <li>Batch size: 256
    <li>Warmup: 10,000 steps
    <li>Total: 100,000 steps
</ul>

<h3>Production Deployment</h3>

<p><strong>Optimizations:</strong>
<ol>
    <li>Quantize to INT8 (3√ó speedup)
    <li>Distill to 6-layer model (2√ó speedup)
    <li>Deploy with ONNX Runtime
    <li>Dynamic batching (avg batch size 32)
</ol>

<p><strong>Results:</strong>
<ul>
    <li>Latency: 15ms p99 (vs 200ms baseline)
    <li>Throughput: 2000 QPS per GPU
    <li>Relevance: +8\% improvement over TF-IDF
</ul>

<h2>Case Study: GPT for Code Generation</h2>

<h3>Problem Setup</h3>

<p><strong>Task:</strong> Generate Python code from natural language</p>

<strong>Example:</strong>
<pre><code>
Input: "Function to reverse a string"
Output: 
def reverse_string(s):
    return s[::-1]
</code></pre>

<h3>Model and Data</h3>

<p><strong>Model:</strong> GPT-2 medium (345M params)</p>

<p><strong>Data:</strong>
<ul>
    <li>GitHub public repositories (Python)
    <li>Filtered: Only files with docstrings
    <li>Format: Docstring $\to$ Implementation
    <li>Total: 50GB, 10B tokens
</ul>

<h3>Training</h3>

<p><strong>Pre-training:</strong> Start from GPT-2 checkpoint</p>

<p><strong>Fine-tuning:</strong>
<ul>
    <li>100,000 steps
    <li>Learning rate: $5 \times 10^{-5}$
    <li>Context: 1024 tokens
    <li>Batch: 128 sequences
</ul>

<h3>Evaluation</h3>

<p><strong>Metrics:</strong>
<ul>
    <li>Pass@k: \% correct in top-k samples
    <li>BLEU: Token overlap with reference
    <li>Human evaluation: Correctness + readability
</ul>

<p><strong>Results:</strong>
<ul>
    <li>Pass@1: 42\%
    <li>Pass@10: 71\%
    <li>Human preferred over baseline: 78\%
</ul>

<h2>Future Directions</h2>

<h3>Architectural Innovations</h3>

<p><strong>1. Efficient attention</strong>
<ul>
    <li>Linear complexity methods
    <li>State space models (S4, Mamba)
    <li>Hybrid CNN-attention architectures
</ul>

<p><strong>2. Multimodal integration</strong>
<ul>
    <li>Unified text-image-audio models
    <li>Better cross-modal alignment
    <li>Efficient fusion strategies
</ul>

<p><strong>3. Long context</strong>
<ul>
    <li>Million-token contexts
    <li>Hierarchical memory
    <li>Retrieval-augmented transformers
</ul>

<h3>Training Innovations</h3>

<p><strong>1. Sample efficiency</strong>
<ul>
    <li>Better pre-training objectives
    <li>Curriculum learning
    <li>Few-shot and zero-shot learning
</ul>

<p><strong>2. Scaling</strong>
<ul>
    <li>Mixture of experts
    <li>Conditional computation
    <li>Efficient parallelism strategies
</ul>

<p><strong>3. Alignment</strong>
<ul>
    <li>Better RLHF techniques
    <li>Constitutional AI
    <li>Value alignment
</ul>

<h2>Conclusion</h2>

<h3>Key Takeaways</h3>

<p><strong>Architecture:</strong>
<ul>
    <li>Attention is powerful and flexible
    <li>Position encodings crucial for sequences
    <li>Residuals + normalization enable depth
</ul>

<p><strong>Training:</strong>
<ul>
    <li>Pre-training + fine-tuning is dominant paradigm
    <li>Warmup is critical for stability
    <li>Scale requires careful optimization
</ul>

<p><strong>Deployment:</strong>
<ul>
    <li>Quantization and distillation for efficiency
    <li>Batching crucial for throughput
    <li>Monitor performance in production
</ul>

<h3>Final Advice</h3>

<p><strong>For practitioners:</strong>
<ol>
    <li>Start simple: Use pre-trained models
    <li>Debug systematically: Data, model, training
    <li>Optimize iteratively: Accuracy first, then speed
    <li>Monitor continuously: Metrics, errors, drift
</ol>

<p><strong>For researchers:</strong>
<ol>
    <li>Understand fundamentals deeply
    <li>Question assumptions: Why does this work?
    <li>Experiment rigorously: Ablations, multiple seeds
    <li>Share knowledge: Open source, papers, blogs
</ol>

<p>This concludes our comprehensive journey through deep learning and transformers. You now have the mathematical foundations, practical implementations, and real-world insights to build state-of-the-art transformer models!</p>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> Reproduce DistilBERT:
<ol>
    <li>Train 6-layer student on BERT-base teacher
    <li>Use distillation + MLM + cosine losses
    <li>Evaluate on GLUE
    <li>Measure compression ratio and speedup
</ol>
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Debug broken transformer (provided):
<ol>
    <li>Model trains but poor performance
    <li>Find 3 subtle bugs (architecture, training, data)
    <li>Fix and verify improvements
</ol>
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> Deploy BERT for production:
<ol>
    <li>Fine-tune on classification task
    <li>Quantize to INT8
    <li>Export to ONNX
    <li>Create REST API with FastAPI
    <li>Load test and optimize
</ol>
</div>

<h2>Solutions</h2>

<p>Full solutions for all exercises are available at \url{https://deeplearning.hofkensvermeulen.be}.</p>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 1: Reproduce DistilBERT</strong>

<p><strong>Training Configuration:</strong>
<ul>
    <li>Student: 6 layers, 768 hidden, 12 heads (66M params)
    <li>Teacher: BERT-base (110M params)
    <li>Loss: $\mathcal{L} = \alpha \mathcal{L}_{\text{distill}} + \beta \mathcal{L}_{\text{MLM}} + \gamma \mathcal{L}_{\text{cosine}}$
    <li>Weights: $\alpha=0.5$, $\beta=0.25$, $\gamma=0.25$
</ul>

<p><strong>Results on GLUE:</strong></p>

<table>
<tr><th><strong>Model</strong></th><th><strong>Params</strong></th><th><strong>GLUE Score</strong></th><th><strong>Speed</strong></th></tr>
<tr><td>BERT-base</td><td>110M</td><td>84.5</td><td>1.0x</td></tr>
<tr><td>DistilBERT</td><td>66M</td><td>82.8</td><td>1.6x</td></tr>
</table>

<p><strong>Compression Analysis:</strong>
<ul>
    <li><strong>Parameters:</strong> 40\% reduction (110M $\to$ 66M)
    <li><strong>Inference speed:</strong> 60\% faster
    <li><strong>Accuracy:</strong> 98\% of teacher performance (2\% drop)
    <li><strong>Memory:</strong> 40\% less
</ul>

<p><strong>Key Insights:</strong>
<ol>
    <li>Distillation preserves 98\% of teacher's knowledge
    <li>Triple loss (distill + MLM + cosine) crucial for quality
    <li>6 layers sufficient for most understanding tasks
    <li>Excellent trade-off for production deployment
</ol>

<p><strong>When to use DistilBERT:</strong>
<ul>
    <li>Latency-sensitive applications (<50ms)
    <li>Resource-constrained environments
    <li>Mobile/edge deployment
    <li>High-throughput serving
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 2: Debug Broken Transformer</strong>

<p><strong>Common Bugs Found:</strong></p>

<p><strong>Bug 1 (Architecture):</strong> Missing dropout in attention</p>

<p><strong>Symptom:</strong> Model overfits quickly, poor generalization</p>

<strong>Fix:</strong>
<pre><code># Before (broken)
attn_weights = F.softmax(scores, dim=-1)
output = torch.matmul(attn_weights, V)

# After (fixed)
attn_weights = F.softmax(scores, dim=-1)
attn_weights = F.dropout(attn_weights, p=0.1, training=self.training)
output = torch.matmul(attn_weights, V)
</code></pre>

<p><strong>Impact:</strong> Validation accuracy improves from 72\% to 84\%</p>

<p><strong>Bug 2 (Training):</strong> Learning rate too high</p>

<p><strong>Symptom:</strong> Loss oscillates, doesn't converge</p>

<strong>Fix:</strong>
<pre><code># Before (broken)
optimizer = AdamW(model.parameters(), lr=1e-3)  # Too high!

# After (fixed)
optimizer = AdamW(model.parameters(), lr=5e-5)  # Appropriate for BERT
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=1000)
</code></pre>

<p><strong>Impact:</strong> Loss converges smoothly, final accuracy 84\% $\to$ 87\%</p>

<p><strong>Bug 3 (Data):</strong> Incorrect padding token handling</p>

<p><strong>Symptom:</strong> Model attends to padding, poor performance on variable-length sequences</p>

<strong>Fix:</strong>
<pre><code># Before (broken)
attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
attn_weights = F.softmax(attn_scores, dim=-1)

# After (fixed)
attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
# Mask padding tokens
attn_scores = attn_scores.masked_fill(attention_mask == 0, float('-inf'))
attn_weights = F.softmax(attn_scores, dim=-1)
</code></pre>

<p><strong>Impact:</strong> Accuracy on variable-length sequences improves from 79\% to 87\%</p>

<p><strong>Final Results:</strong></p>

<table>
<tr><th><strong>Version</strong></th><th><strong>Accuracy</strong></th></tr>
<tr><td>Original (broken)</td><td>72\%</td></tr>
<tr><td>After Bug 1 fix</td><td>84\%</td></tr>
<tr><td>After Bug 2 fix</td><td>87\%</td></tr>
<tr><td>After Bug 3 fix</td><td>87\% (robust)</td></tr>
</table>

<p><strong>Debugging Lessons:</strong>
<ol>
    <li>Always include dropout in attention
    <li>Use appropriate learning rates (5e-5 for BERT-scale)
    <li>Properly mask padding tokens
    <li>Test on variable-length sequences
    <li>Monitor both training and validation metrics
</ol>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 3: Deploy BERT for Production</strong>

<p><strong>Deployment Pipeline:</strong></p>

<p><strong>Step 1: Fine-tune on Classification</strong>
<ul>
    <li>Task: Sentiment analysis (binary classification)
    <li>Training: 10k examples, 3 epochs
    <li>Validation accuracy: 92.3\%
</ul>

<p><strong>Step 2: Quantize to INT8</strong>
<ul>
    <li>Method: Dynamic quantization
    <li>Model size: 438 MB $\to$ 110 MB (75\% reduction)
    <li>Accuracy: 92.3\% $\to$ 91.8\% (0.5\% drop)
    <li>Inference speed: 2.4x faster
</ul>

<strong>Step 3: Export to ONNX</strong>
<pre><code>import torch.onnx

# Export model
dummy_input = torch.randint(0, 1000, (1, 128))
torch.onnx.export(
    model,
    dummy_input,
    "bert_sentiment.onnx",
    input_names=['input_ids'],
    output_names=['logits'],
    dynamic_axes={'input_ids': {0: 'batch', 1: 'sequence'}}
)

# Verify with ONNX Runtime
import onnxruntime as ort
session = ort.InferenceSession("bert_sentiment.onnx")
# 1.3x additional speedup
</code></pre>

<strong>Step 4: Create REST API</strong>
<pre><code>from fastapi import FastAPI
from pydantic import BaseModel
import onnxruntime as ort

app = FastAPI()
session = ort.InferenceSession("bert_sentiment.onnx")

class TextInput(BaseModel):
    text: str

@app.post("/predict")
async def predict(input: TextInput):
    # Tokenize
    tokens = tokenizer.encode(input.text, max_length=128, truncation=True)
    
    # Inference
    outputs = session.run(None, {"input_ids": [tokens]})
    logits = outputs[0][0]
    
    # Predict
    prediction = "positive" if logits[1] > logits[0] else "negative"
    confidence = float(max(logits))
    
    return {"prediction": prediction, "confidence": confidence}
</code></pre>

<p><strong>Step 5: Load Test and Optimize</strong></p>

<p><strong>Initial Performance:</strong>
<ul>
    <li>Latency: 45ms (p50), 78ms (p99)
    <li>Throughput: 22 requests/second
</ul>

<p><strong>Optimizations Applied:</strong>
<ol>
    <li>Dynamic batching (batch size 8): 3.2x throughput
    <li>Connection pooling: 1.2x throughput
    <li>Async processing: 1.5x throughput
</ol>

<p><strong>Final Performance:</strong>
<ul>
    <li>Latency: 38ms (p50), 62ms (p99)
    <li>Throughput: 127 requests/second
    <li>5.8x improvement over baseline
</ul>

<p><strong>Production Checklist:</strong>
<ul>
    \item[$\checkmark$] Model quantized and optimized
    \item[$\checkmark$] ONNX export for cross-platform compatibility
    \item[$\checkmark$] REST API with proper error handling
    \item[$\checkmark$] Load tested and optimized
    \item[$\checkmark$] Monitoring and logging configured
    \item[$\checkmark$] Auto-scaling based on load
    \item[$\checkmark$] Health checks and graceful shutdown
</ul>

<strong>Deployment Architecture:</strong>
<pre><code>
Load Balancer
    |
    +-- API Server 1 (ONNX Runtime)
    +-- API Server 2 (ONNX Runtime)
    +-- API Server 3 (ONNX Runtime)
    |
Monitoring (Prometheus + Grafana)
</code></pre>

<p><strong>Key Metrics to Monitor:</strong>
<ul>
    <li>Request latency (p50, p95, p99)
    <li>Throughput (requests/second)
    <li>Error rate
    <li>CPU/GPU utilization
    <li>Memory usage
    <li>Model accuracy (via A/B testing)
</ul>

<p><strong>Cost Analysis:</strong>
<ul>
    <li>Hardware: 3x T4 GPUs (\$0.35/hour each)
    <li>Total: \$1.05/hour = \$756/month
    <li>Capacity: 127 req/s √ó 3 = 381 req/s
    <li>Cost per 1M requests: \$0.55
</ul>

<p><strong>Production Best Practices:</strong>
<ol>
    <li>Always quantize for inference (2-4x speedup)
    <li>Use ONNX for deployment (cross-platform, optimized)
    <li>Implement dynamic batching (3-5x throughput)
    <li>Monitor latency percentiles (not just average)
    <li>Set up auto-scaling for variable load
    <li>Use health checks and graceful shutdown
    <li>Implement request timeouts and retries
    <li>Log predictions for model monitoring
</ol>

<p><strong>Success Criteria Met:</strong>
<ul>
    \item[$\checkmark$] <50ms p99 latency
    \item[$\checkmark$] >100 requests/second throughput
    \item[$\checkmark$] <1\% accuracy degradation
    \item[$\checkmark$] 75\% model size reduction
    \item[$\checkmark$] Production-ready API
    \item[$\checkmark$] Comprehensive monitoring
</ul>
</div>
        
        <div class="chapter-nav">
  <a href="chapter22_hardware_optimization.html">‚Üê Chapter 22: Hardware Optimization</a>
  <a href="../../deeptech.html">üìö Table of Contents</a>
  <a href="chapter24_domain_specific_models.html">Chapter 24: Domain-Specific Models ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
