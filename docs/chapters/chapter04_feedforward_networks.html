<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4: Feed-Forward Neural Networks - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../../deeptech.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Feed-Forward Neural Networks</h1>

<h2>Chapter Overview</h2>

<p>Feed-forward neural networks are the foundation of deep learning. These networks transform inputs through sequences of linear and nonlinear operations to produce outputs. This chapter develops the architecture, training, and theory of feed-forward networks, establishing concepts that extend to all modern deep learning models including transformers.</p>

<h3>Learning Objectives</h3>

<p>After completing this chapter, you will be able to:</p>

<ol>
    <li>Understand the architecture of feed-forward neural networks
    <li>Implement forward and backward passes through MLPs
    <li>Apply appropriate activation functions and understand their properties
    <li>Initialize network weights properly to enable training
    <li>Apply regularization techniques to prevent overfitting
    <li>Understand the universal approximation theorem
</ol>

<h2>From Linear Models to Neural Networks</h2>

<h3>The Perceptron</h3>

<div class="definition"><strong>Definition:</strong> 
The perceptron is a binary classifier:
<div class="equation">
$$
\hat{y} = \text{sign}(\vw\transpose \vx + b) = \begin{cases}
+1 & \text{if } \vw\transpose \vx + b > 0 \\
-1 & \text{otherwise}
\end{cases}
$$
</div>
where $\vw \in \R^n$ are weights, $b \in \R$ is bias, $\vx \in \R^n$ is input.
</div>

<h3>Multi-Class Classification: Softmax Regression</h3>

<div class="definition"><strong>Definition:</strong> 
For logits $\vz = [z_1, \ldots, z_C]\transpose \in \R^C$:
<div class="equation">
$$
\text{softmax}(\vz)_k = \frac{\exp(z_k)}{\sum_{j=1}^C \exp(z_j)}
$$
</div>
</div>

<div class="example"><strong>Example:</strong> 
For logits $\vz = [2.0, 1.0, 0.1]$: Sum of exponentials $= 11.212$, giving probabilities $[0.659, 0.242, 0.099]$. The model predicts class 1 with 65.9 percent confidence.
</div>

<h2>Multi-Layer Perceptrons</h2>

<div class="definition"><strong>Definition:</strong> 
An L-layer MLP transforms input through layers:
<div class="equation">
$$\begin{align}
\vz^{(\ell)} &= \mW^{(\ell)} \vh^{(\ell-1)} + \vb^{(\ell)} \\
\vh^{(\ell)} &= \sigma^{(\ell)}(\vz^{(\ell)})
\end{align}$$
</div>
where $\mW^{(\ell)} \in \R^{n_\ell \times n_{\ell-1}}$ is the weight matrix and $\sigma^{(\ell)}$ is the activation function.

<div class="architecture-diagram">
<h3>MLP Forward and Backward Pass</h3>
<pre class="mermaid">
graph LR
    X["Input x<br/> x ‚àà ‚Ñù^n0"] -->|"W1 ‚àà ‚Ñù^n1 x n0<br/> b1 ‚àà ‚Ñù^n1"| Z1["Pre-activation<br/> z1 = W1*x + b1<br/> z1 ‚àà ‚Ñù^n1<br/> STORED for backprop"]
    Z1 -->|"Activation œÉ"| H1["h1 = œÉ(z1)<br/> h1 ‚àà ‚Ñù^n1<br/> STORED for backprop"]
    H1 -->|"W2 ‚àà ‚Ñù^n2 x n1<br/> b2 ‚àà ‚Ñù^n2"| Z2["Pre-activation<br/> z2 = W2*h1 + b2<br/> z2 ‚àà ‚Ñù^n2<br/> STORED for backprop"]
    Z2 -->|"softmax"| Y["Output ≈∑<br/> ≈∑ ‚àà ‚Ñù^n2"]
    Y --> L["Loss L(≈∑, y)"]

    L -.->|"‚àÇL/‚àÇz2 needs: z2, ≈∑"| Z2
    Z2 -.->|"‚àÇL/‚àÇW2 needs: h1"| H1
    H1 -.->|"‚àÇL/‚àÇz1 needs: z1, œÉ prime"| Z1
    Z1 -.->|"‚àÇL/‚àÇW1 needs: x"| X

    style X fill:#e8f5e9,stroke:#4caf50,color:#000
    style Z1 fill:#fff3e0,stroke:#ff9800,color:#000
    style H1 fill:#e3f2fd,stroke:#2196f3,color:#000
    style Z2 fill:#fff3e0,stroke:#ff9800,color:#000
    style Y fill:#f3e5f5,stroke:#9c27b0,color:#000
    style L fill:#fce4ec,stroke:#e91e63,color:#000
</pre>
<p class="diagram-caption">MLP Forward and Backward Pass</p>
</div>

<p></div>

<figure>
<div class="tikz-diagram"><img src="../diagrams/chapter04_feedforward_networks_522a20fbc46b.svg" alt="TikZ Diagram" /></div>
<figcaption>Multi-layer perceptron (MLP) computational graph showing fully-connected layers. Each neuron in the hidden layer receives input from <strong>all</strong> neurons in the input layer (12 connections total), and each output neuron receives input from all hidden neurons (8 connections). The red path highlights one example: $x_1 \to h_2 \to y_1$. This dense connectivity enables MLPs to learn complex non-linear functions.</figcaption>
</figure>

<div class="example"><strong>Example:</strong> 
Architecture for MNIST digit classification:
<ul>
    <li>Input: $\vx \in \R^{784}$ (flattened $28 \times 28$ image)
    <li>Hidden 1: $\vh^{(1)} \in \R^{256}$ with ReLU
    <li>Hidden 2: $\vh^{(2)} \in \R^{128}$ with ReLU
    <li>Output: $\vz^{(3)} \in \R^{10}$ with softmax
</ul>

<p>Parameter count: $200{,}960 + 32{,}896 + 1{,}290 = 235{,}146$ parameters.
</div>

<h3>Why Depth Matters</h3>

<p>Without nonlinear activations, multiple layers collapse to single linear transformation. With nonlinearities, deep networks learn complex functions efficiently.</p>

<h2>Memory and Computation Analysis</h2>

<p>For a single fully-connected layer computing $\vy = \mW\vx + \vb$ where $\mW \in \R^{m \times n}$, the forward pass requires approximately $2mn$ FLOPs and the backward pass requires approximately $4mn$ FLOPs (computing gradients with respect to inputs, weights, and biases). This gives a useful rule of thumb: one training step requires approximately $6\times$ as many FLOPs as the model has parameters---$2\times$ for the forward pass and $4\times$ for the backward pass. This ratio holds for fully-connected layers but varies with architecture; convolutional layers have much higher FLOPs per parameter due to weight sharing, while embedding layers have zero FLOPs (table lookups only).</p>

<p>During training, intermediate activations must be stored for the backward pass, and these often consume more memory than the model parameters. Activation memory scales linearly with batch size while parameter memory remains constant, so large batch sizes eventually become memory-limited. For transformer models, attention score matrices of size $B \times h \times n \times n$ dominate activation memory due to their $O(n^2)$ scaling with sequence length, dwarfing the $O(n)$ scaling of feed-forward activations.</p>

<p>Transformer feed-forward networks use a standard two-layer architecture with $4\times$ expansion: projecting from model dimension $d$ to $4d$ with a GELU activation, then back to $d$. For BERT-base ($d = 768$, $d_{ff} = 3072$), the FFN contributes 4.7M parameters per layer---roughly twice the attention mechanism's 2.4M---and accounts for $\sim$60\% of per-layer FLOPs (see Section~[ref] for the complete breakdown).</p>

<h2>Activation Functions</h2>

<div class="definition"><strong>Definition:</strong> 
<div class="equation">
$$
\text{ReLU}(z) = \max(0, z)
$$
</div>
Derivative: $\text{ReLU}'(z) = \mathbb{1}[z > 0]$
</div>

<div class="definition"><strong>Definition:</strong> 
Gaussian Error Linear Unit (default in transformers):
<div class="equation">
$$
\text{GELU}(z) = z \cdot \Phi(z)
$$
</div>
where $\Phi$ is standard normal CDF. Approximation:
<div class="equation">
$$
\text{GELU}(z) \approx 0.5z \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}}(z + 0.044715z^3)\right]\right)
$$
</div>
</div>

<div class="keypoint">
Transformer models use GELU (BERT, GPT) or variants like Swish for feed-forward networks.
</div>

<h3>Computational Cost of Activation Functions</h3>

<p>GELU is 4$\times$ more expensive than ReLU in arithmetic operations, but in practice adds only 1--2\% to total training time because both operations are memory-bandwidth-bound on modern GPUs. Modern frameworks provide fused linear-activation kernels that eliminate intermediate memory traffic, providing 1.3--1.5$\times$ speedup for combined operations.</p>

<h3>Why GELU is Preferred in Transformers</h3>

<p>Despite its higher computational cost, GELU has become the standard activation function for transformer models, used in BERT, GPT-2, GPT-3, T5, and most modern language models. This preference is driven by empirical performance rather than computational efficiency: models trained with GELU consistently achieve better final accuracy than those trained with ReLU, particularly on language understanding tasks.</p>

<p>The theoretical motivation for GELU is that it provides a smoother approximation to the ReLU function, with non-zero gradients for negative inputs. While ReLU has gradient zero for all $z < 0$, GELU has small but non-zero gradients in this region, allowing the network to recover from neurons that have been pushed into the negative regime. This property is particularly valuable in deep networks where gradient flow through many layers can be fragile. For a 24-layer BERT-large model, the probability that a gradient signal survives through all layers is significantly higher with GELU than with ReLU, as GELU never completely blocks gradient flow.</p>

<p>Empirically, BERT-base trained with GELU achieves 84.6\% accuracy on the MNLI natural language inference task, compared to 83.9\% with ReLU‚Äîa 0.7 percentage point improvement that is statistically significant and practically meaningful. For GPT-2, the perplexity on the WebText validation set is 18.3 with GELU compared to 19.1 with ReLU, indicating better language modeling performance. These improvements justify the 1-2\% computational overhead of GELU, as the improved model quality translates to better downstream task performance and potentially reduced training time to reach a target accuracy.</p>

<p>The success of GELU has inspired variants like Swish and Mish that share the property of smooth, non-zero gradients everywhere. Swish, defined as $\text{Swish}(z) = z \cdot \sigma(z)$, has similar performance to GELU on most tasks and is used in some efficient transformer architectures like EfficientNet. Mish, defined as $\text{Mish}(z) = z \cdot \tanh(\text{softplus}(z))$, provides slightly better performance than GELU on some vision tasks but has higher computational cost. The landscape of activation functions continues to evolve, but GELU remains the standard for language models due to its strong empirical performance and reasonable computational cost.</p>

<h2>Universal Approximation Theorem</h2>

<div class="theorem"><strong>Theorem:</strong> 
A single-hidden-layer neural network with nonlinear activation can approximate any continuous function on compact domain to arbitrary precision, given sufficient hidden units.
</div>

<p>Caveat: The theorem says nothing about how many units needed, how to find weights, or generalization. Deep networks often more efficient than wide networks.</p>

<h2>Weight Initialization</h2>

<div class="definition"><strong>Definition:</strong> 
For layer with $n_{\text{in}}$ inputs and $n_{\text{out}}$ outputs:
<div class="equation">
$$
w_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
$$
</div>
Best for tanh and sigmoid activations.
</div>

<div class="definition"><strong>Definition:</strong> 
For ReLU networks:
<div class="equation">
$$
w_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
$$
</div>
Accounts for ReLU zeroing half the activations.
</div>

<h3>Variance Preservation Through Layers</h3>

<p>Proper weight initialization ensures that activations and gradients maintain reasonable magnitudes as they propagate through deep networks. Consider a linear layer $\vy = \mW\vx$ where $\vx \in \R^{n_{\text{in}}}$ has zero mean and unit variance, and weights $w_{ij}$ are independent with zero mean and variance $\sigma_w^2$. The variance of each output element is:
<div class="equation">
$$
\text{Var}(y_i) = \text{Var}\left(\sum_{j=1}^{n_{\text{in}}} w_{ij} x_j\right) = \sum_{j=1}^{n_{\text{in}}} \text{Var}(w_{ij}) \text{Var}(x_j) = n_{\text{in}} \sigma_w^2
$$
</div>

<p>To preserve variance ($\text{Var}(y_i) = 1$), we need $\sigma_w^2 = 1/n_{\text{in}}$. Xavier initialization uses $\sigma_w^2 = 2/(n_{\text{in}} + n_{\text{out}})$ to balance forward and backward pass variance preservation. For ReLU activations, which zero out half the activations on average, He initialization compensates by using $\sigma_w^2 = 2/n_{\text{in}}$, doubling the variance to maintain signal strength after the nonlinearity.</p>

<p>Without proper initialization, deep networks fail to train: with variance too large ($\sigma_w^2 = 1$), activations explode exponentially through layers; with variance too small ($\sigma_w^2 = 0.01$), they vanish to zero. For transformer models with GELU activations, He initialization or slight variants work well and are used universally in BERT, GPT, T5, and other modern architectures.</p>

<h2>Regularization</h2>

<h3>L2 Regularization</h3>

<p>Add penalty to loss:
<div class="equation">
$$
L_{\text{total}} = L_{\text{data}} + \frac{\lambda}{2} \sum_{\ell} \norm{\mW^{(\ell)}}_F^2
$$
</div>

<p>L2 regularization, also known as weight decay, penalizes large parameter values to prevent overfitting. The regularization term adds the squared Frobenius norm of all weight matrices to the loss function, encouraging the optimizer to keep weights small. The hyperparameter $\lambda$ controls the strength of regularization: larger $\lambda$ produces smaller weights and stronger regularization.</p>

<p>The computational cost of L2 regularization is modest. Computing the squared norm $\norm{\mW}_F^2 = \sum_{ij} w_{ij}^2$ requires one multiplication and one addition per parameter, totaling $2P$ operations for a model with $P$ parameters. For BERT-base with 110 million parameters, this requires 220 million operations, or 0.22 GFLOPs. Compared to the 96 GFLOPs required for a forward pass, the regularization computation adds only 0.23\% overhead. On an NVIDIA A100 GPU, computing the regularization term takes approximately 0.7 microseconds, which is negligible compared to the 50 milliseconds for a full forward-backward pass.</p>

<p>The gradient of the L2 regularization term is even simpler: $\nabla_{\mW} \left(\frac{\lambda}{2} \norm{\mW}_F^2\right) = \lambda \mW$. This adds a term proportional to the current weights to the gradient, which can be implemented as a simple scaling operation during the optimizer step. Most optimizers, including PyTorch's Adam and SGD, support weight decay as a built-in parameter that applies this scaling automatically without requiring explicit computation of the regularization term. This makes L2 regularization essentially free from a computational perspective.</p>

<p>The memory overhead of L2 regularization is zero, as it requires no additional storage beyond the parameters themselves. The regularization term is computed on-the-fly during the backward pass and does not need to be stored. This makes L2 regularization an attractive regularization technique for large models where memory is at a premium.</p>

<h3>Dropout</h3>

<div class="definition"><strong>Definition:</strong> 
During training, randomly set activations to zero with probability p. During inference, scale by $(1-p)$.
</div>

<p>Dropout is a powerful regularization technique that randomly drops (sets to zero) a fraction of activations during training. This prevents the network from relying too heavily on any single neuron and encourages learning robust features. The dropout probability $p$ is typically 0.1 to 0.5, with higher values providing stronger regularization at the cost of slower convergence.</p>

<h3>Dropout in Practice</h3>

<p>Dropout adds approximately 4--5\% overhead to training time for BERT-base, primarily from random number generation and memory traffic for the binary mask. Transformer models apply dropout at multiple points: after attention weights (attention dropout), after sublayer outputs (residual dropout), and on input embeddings. BERT uses $p=0.1$ at all locations; GPT-3 uses only residual dropout.</p>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> Design 3-layer MLP for binary classification of 100-dimensional inputs. Specify layer dimensions, activations, and parameter count.
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Compute forward pass through 2-layer network with given weights and ReLU activation.
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> For layer with 512 inputs and 256 outputs using ReLU: (1) What is He initialization variance? (2) Why different from Xavier? (3) What happens with zero initialization?
</div>

<div class="exercise" id="exercise-4"><strong>Exercise 4:</strong> Prove that without nonlinear activations, L-layer network equivalent to single layer.
</div>

<h2>Solutions</h2>

<p>Full solutions for all exercises are available at \url{https://deeplearning.hofkensvermeulen.be}.</p>

<div class="solution"><strong>Solution:</strong> <strong>3-layer MLP design for binary classification:</strong>

<p><strong>Architecture:</strong>
<ul>
    <li>Input layer: 100 dimensions
    <li>Hidden layer 1: 100 $\to$ 64 with ReLU activation
    <li>Hidden layer 2: 64 $\to$ 32 with ReLU activation
    <li>Output layer: 32 $\to$ 1 with sigmoid activation
</ul>

<p><strong>Parameter count:</strong>
<ul>
    <li>Layer 1: $\mW^{(1)} \in \R^{64 \times 100}$ has $6{,}400$ weights, $\vb^{(1)} \in \R^{64}$ has $64$ biases
    <li>Layer 2: $\mW^{(2)} \in \R^{32 \times 64}$ has $2{,}048$ weights, $\vb^{(2)} \in \R^{32}$ has $32$ biases
    <li>Layer 3: $\mW^{(3)} \in \R^{1 \times 32}$ has $32$ weights, $b^{(3)} \in \R$ has $1$ bias
    <li>Total: $6{,}400 + 64 + 2{,}048 + 32 + 32 + 1 = 8{,}577$ parameters
</ul>

<p><strong>Forward pass equations:</strong>
<div class="equation">
$$\begin{align}
\vh^{(1)} &= \text{ReLU}(\mW^{(1)}\vx + \vb^{(1)}) \\
\vh^{(2)} &= \text{ReLU}(\mW^{(2)}\vh^{(1)} + \vb^{(2)}) \\
\hat{y} &= \sigma(\mW^{(3)}\vh^{(2)} + b^{(3)})
\end{align}$$
</div>

<p>where $\sigma(z) = \frac{1}{1 + e^{-z}}$ is the sigmoid function.
</div>

<div class="solution"><strong>Solution:</strong> <strong>Forward pass computation:</strong>

<p>Given weights:
<div class="equation">
$$
\mW^{(1)} = \begin{bmatrix} 0.5 & -0.3 \\ 0.2 & 0.6 \end{bmatrix}, \quad \vb^{(1)} = \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix}, \quad \mW^{(2)} = \begin{bmatrix} 1.0 & -0.5 \end{bmatrix}, \quad b^{(2)} = 0.3
$$
</div>

<p>Input: $\vx = \begin{bmatrix} 2.0 \\ 1.0 \end{bmatrix}$</p>

<p><strong>Layer 1:</strong>
<div class="equation">
$$\begin{align}
\vz^{(1)} &= \mW^{(1)}\vx + \vb^{(1)} = \begin{bmatrix} 0.5(2.0) - 0.3(1.0) \\ 0.2(2.0) + 0.6(1.0) \end{bmatrix} + \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix} = \begin{bmatrix} 0.8 \\ 0.8 \end{bmatrix} \\
\vh^{(1)} &= \text{ReLU}(\vz^{(1)}) = \begin{bmatrix} 0.8 \\ 0.8 \end{bmatrix}
\end{align}$$
</div>

<p><strong>Layer 2:</strong>
<div class="equation">
$$\begin{align}
z^{(2)} &= \mW^{(2)}\vh^{(1)} + b^{(2)} = 1.0(0.8) - 0.5(0.8) + 0.3 = 0.7 \\
\hat{y} &= z^{(2)} = 0.7 \quad \text{(no activation for regression)}
\end{align}$$
</div>

<p>Final output: $\hat{y} = 0.7$
</div>

<div class="solution"><strong>Solution:</strong> For layer with 512 inputs and 256 outputs using ReLU:

<p><strong>(1) He initialization variance:</strong>
<div class="equation">
$$
\text{Var}(w_{ij}) = \frac{2}{n_{\text{in}}} = \frac{2}{512} = 0.00391
$$
</div>

<p>Standard deviation: $\sigma = \sqrt{0.00391} \approx 0.0625$</p>

<p><strong>(2) Why different from Xavier:</strong>
<ul>
    <li>Xavier initialization: $\text{Var}(w) = \frac{1}{n_{\text{in}}}$ (for tanh/sigmoid)
    <li>He initialization: $\text{Var}(w) = \frac{2}{n_{\text{in}}}$ (for ReLU)
    <li>ReLU zeros out half the activations, reducing variance by factor of 2
    <li>He initialization compensates by doubling the initial variance
    <li>This maintains signal variance through deep networks with ReLU
</ul>

<p><strong>(3) Zero initialization problem:</strong>
If all weights are initialized to zero:
<ul>
    <li>All neurons in a layer compute identical outputs
    <li>All gradients are identical (symmetry)
    <li>Neurons never differentiate during training
    <li>Network effectively has only one neuron per layer
    <li>Learning fails completely
</ul>

<p>Random initialization breaks symmetry, allowing neurons to learn different features.
</div>

<div class="solution"><strong>Solution:</strong> <strong>Proof that L-layer linear network equals single layer:</strong>

<p>Consider an $L$-layer network without nonlinear activations:
<div class="equation">
$$\begin{align}
\vh^{(1)} &= \mW^{(1)}\vx + \vb^{(1)} \\
\vh^{(2)} &= \mW^{(2)}\vh^{(1)} + \vb^{(2)} \\
&\vdots \\
\vh^{(L)} &= \mW^{(L)}\vh^{(L-1)} + \vb^{(L)}
\end{align}$$
</div>

<p>Substituting recursively:
<div class="equation">
$$\begin{align}
\vh^{(2)} &= \mW^{(2)}(\mW^{(1)}\vx + \vb^{(1)}) + \vb^{(2)} \\
&= \mW^{(2)}\mW^{(1)}\vx + \mW^{(2)}\vb^{(1)} + \vb^{(2)}
\end{align}$$
</div>

<p>Continuing to layer $L$:
<div class="equation">
$$\begin{align}
\vh^{(L)} &= \mW^{(L)}\mW^{(L-1)}\cdots\mW^{(1)}\vx + \text{(bias terms)} \\
&= \mW_{\text{eff}}\vx + \vb_{\text{eff}}
\end{align}$$
</div>

<p>where:
<div class="equation">
$$\begin{align}
\mW_{\text{eff}} &= \mW^{(L)}\mW^{(L-1)}\cdots\mW^{(1)} \\
\vb_{\text{eff}} &= \sum_{i=1}^{L} \left(\prod_{j=i+1}^{L} \mW^{(j)}\right) \vb^{(i)}
\end{align}$$
</div>

<p>This is equivalent to a single linear layer with weights $\mW_{\text{eff}}$ and bias $\vb_{\text{eff}}$. Therefore, without nonlinear activations, depth provides no additional representational power‚Äîthe network can only learn linear functions regardless of depth.
</div>
        
        <div class="chapter-nav">
  <a href="chapter03_probability_information.html">‚Üê Chapter 3: Probability and Information Theory</a>
  <a href="../../deeptech.html">üìö Table of Contents</a>
  <a href="chapter05_convolutional_networks.html">Chapter 5: Convolutional Neural Networks ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
