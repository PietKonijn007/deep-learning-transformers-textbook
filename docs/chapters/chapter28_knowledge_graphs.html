<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 28: Knowledge Graphs and Reasoning - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../../deeptech.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Knowledge Graphs, Semantic Web, and Structured Information Extraction</h1>

<h2>Chapter Overview</h2>

<p>While most chapters focus on unstructured data (text, images, video) or simple structured data (tables, time series), this chapter explores a unique domain: knowledge representation and reasoning. Knowledge graphs organize information as networks of entities and relationships, enabling semantic understanding and logical inference. Unlike text or images, knowledge graphs are explicitly structured: they formalize what is true about the world. Deep learning has transformed knowledge graphs from hand-crafted databases to systems that automatically extract entities, infer relationships, and reason over incomplete information. This chapter examines how transformers extract structured information from text, represent entities and relationships in learned embeddings, and perform inference over knowledge bases. Applications range from search (Google's Knowledge Graph) to biomedical discovery (drug-target interactions) to cybersecurity (attack pattern detection).</p>

<h2>Learning Objectives</h2>

<ol>
<li>Understand knowledge graph structure: entities, relationships, and semantic types
<li>Extract structured information from unstructured text (entity and relation extraction)
<li>Represent knowledge in embeddings (TransE, DistMult, ComplEx models)
<li>Perform link prediction: infer missing relationships
<li>Implement semantic reasoning and type inference
<li>Build knowledge-aware systems that combine text and structured knowledge
<li>Address scalability: billion-scale graphs with billions of entities and relationships
<li>Understand limitations: incompleteness, noise, and dynamic knowledge
</ol>

<h2>Knowledge Graphs as Formal Languages</h2>

<p>A knowledge graph is a directed, typed, attributed multigraph:</p>

<div class="definition"><strong>Definition:</strong> 
<ul>
<li><strong>Entities:</strong> Unique objects, concepts, or things. Example: ``Barack Obama'' (person), ``United States'' (country), ``2008'' (year)
<li><strong>Relationships (edges):</strong> Typed connections between entities. Example: (Barack Obama) --<code>born\_in</code>--> (Honolulu)
<li><strong>Types:</strong> Semantic categories. Example: Barack Obama has type ``Person'' and ``Politician''
<li><strong>Attributes:</strong> Properties of entities. Example: Barack Obama has ``birth\_date'' = August 4, 1961
<li><strong>Triples:</strong> The basic unit: (subject, predicate, object). Example: (<code>Barack Obama</code>, <code>born\_in</code>, <code>Honolulu</code>)
</ul>
</div>

<p>Knowledge graphs are semi-formal: they have structure (typed entities, relationships) but allow uncertainty (confidence scores, probability distributions).</p>

<h3>Examples of Knowledge Graphs</h3>

<p><strong>DBpedia:</strong> Extracted from Wikipedia infoboxes. 14M entities, 645M relationships. Public and incomplete.</p>

<p><strong>Freebase:</strong> Curated database of facts. 1.9B entities, 3B relationships. Integrated into Google Knowledge Graph.</p>

<p><strong>Wikidata:</strong> Community-curated knowledge base. 100M entities, 12B relationships. Increasingly used for structured data.</p>

<p><strong>YAGO:</strong> Combines Wikipedia, WordNet, and GeoNames. 37M entities, 500M relationships.</p>

<p><strong>Enterprise KGs:</strong> Internal knowledge bases for specific industries (healthcare, finance, customer data).</p>

<h3>Why Knowledge Graphs Matter</h3>

<ul>
<li><strong>Semantic search:</strong> Query ``movies directed by Steven Spielberg released after 2010.'' Knowledge graph allows structured queries, not just keyword matching.
<li><strong>Question answering:</strong> ``Who was Barack Obama's wife?'' Directly answered by following edges in graph.
<li><strong>Reasoning:</strong> If A is related to B, and B is related to C, infer possible relationships between A and C.
<li><strong>Disambiguation:</strong> Distinguish ``Barack Obama'' (politician) from ``Obama'' (surname) through type information.
<li><strong>Incompleteness handling:</strong> Most facts are unknown. Predict missing relationships based on learned patterns.
</ul>

<h2>Entity and Relation Extraction from Text</h2>

<p>Knowledge graphs must be populated with information. Much knowledge is in unstructured text (documents, web pages, news). Deep learning extracts structured triples from text.</p>

<h3>Named Entity Recognition (NER)</h3>

<p>The first step is identifying entities in text.</p>

<div class="definition"><strong>Definition:</strong> 
Given text, identify and classify entities into predefined types (Person, Organization, Location, Product, Date, etc.).

Example:
<pre><code>
Text: "Barack Obama was elected president of the United States in 2008."

Entities:
- Barack Obama (Person)
- United States (Location)
- 2008 (Date)
</code></pre>
</div>

<p><strong>Deep learning approach:</strong> Token classification using sequence labeling (BIO tagging):</p>

<ul>
<li><strong>B-</strong> (Begin): Start of entity
<li><strong>I-</strong> (Inside): Continuation of entity
<li><strong>O</strong> (Outside): Not part of entity
</ul>

<p>Architecture: BERT or similar transformer, token-level classification head.</p>

<h3>Relation Extraction</h3>

<p>Once entities are identified, extract relationships between them.</p>

<div class="definition"><strong>Definition:</strong> 
Given text with identified entities, determine the relationship type between entity pairs.

Example:
<pre><code>
Text: "Barack Obama was born in Honolulu."

Entities: Barack Obama, Honolulu
Relation: born_in
Triple: (Barack Obama, born_in, Honolulu)
</code></pre>
</div>

<p><strong>Challenges:</strong>
<ul>
<li><strong>Long-range dependencies:</strong> Entities may be far apart in text
<li><strong>Implicit relations:</strong> ``John married Mary'' states relationship directly; ``John's wife, Mary'' implies it
<li><strong>Multiple relationships:</strong> Sentence may express multiple triples
<li><strong>Noise:</strong> Not all mentions are asserting facts; some are hypothetical or negated
</ul>

<p><strong>Deep learning approaches:</strong></p>

<ul>
<li><strong>Sequence classification:</strong> Classify (entity1, entity2) pair based on text between them
<li><strong>Sequence tagging:</strong> Tag text to identify relation arguments (similar to NER)
<li><strong>Structured prediction:</strong> Joint entity and relation extraction (joint model outperforms pipeline)
</ul>

<h3>Joint Entity and Relation Extraction</h3>

<p>Rather than two separate models, a unified model extracts entities and relations simultaneously.</p>

<p><strong>Architecture:</strong>
<ol>
<li>Encode text with transformer
<li>Entity recognition: Token classification (as in NER)
<li>Relation classification: For each identified entity pair, classify relationship type
<li>Output: Set of triples
</ol>

<p><strong>Advantages:</strong>
<ul>
<li>Entities recognized in context of their relationships (better accuracy)
<li>Shared representations between entity and relation tasks
<li>Supports multi-token entities naturally
</ul>

<h2>Knowledge Graph Embeddings</h2>

<p>Knowledge graphs are discrete structures; neural networks work on continuous embeddings. KG embedding models map entities and relationships to vector spaces.</p>

<h3>TransE Model</h3>

<p>TransE is the foundational KG embedding model:</p>

<div class="definition"><strong>Definition:</strong> 
Learn embeddings for entities and relationships such that:
<div class="equation">
$$\begin{align}
\mathbf{h} + \mathbf{r} \approx \mathbf{t}
\end{align}$$
</div>

<p>where \(\mathbf{h}\) is head entity embedding, \(\mathbf{r}\) is relation embedding, \(\mathbf{t}\) is tail entity embedding.</p>

<p>For a true triple, embedding of head + relation should be close to embedding of tail.
For a false triple, they should be far.</p>

<p>Loss function:
<div class="equation">
$$\begin{align}
\mathcal{L} = \sum_{(h,r,t) \in S} ||(\mathbf{h} + \mathbf{r}) - \mathbf{t}||_2^2 + \sum_{(h',r,t') \notin S} \max(0, \gamma - ||(\mathbf{h}' + \mathbf{r}) - \mathbf{t}'||_2^2)
\end{align}$$
</div>

<p>Positive triples minimized; negative triples have margin.
</div>

<p><strong>Training:</strong>
<ul>
<li>Start with random embeddings
<li>For each true triple (h, r, t), minimize distance
<li>For each false triple (h', r, t'), maximize distance (with margin)
<li>Sample negative examples: corrupt head or tail entity
</ul>

<p><strong>Advantages:</strong>
<ul>
<li>Simple and interpretable
<li>Scales to large graphs (billions of entities)
<li>Captures simple relationships well
</ul>

<p><strong>Limitations:</strong>
<ul>
<li>Cannot handle complex relations (many-to-many, composition)
<li>Assumes additive relationship (not suitable for all relation types)
</ul>

<h3>Advanced Models: DistMult, ComplEx, RotatE</h3>

<p>More sophisticated models address limitations:</p>

<ul>
<li><strong>DistMult:</strong> Replace addition with element-wise multiplication. Better for symmetric relations.
  <div class="equation">
$$\begin{align}
  \text{score}(h, r, t) = \mathbf{h}^T \text{diag}(\mathbf{r}) \mathbf{t}
  \end{align}$$
</div>

<p><li><strong>ComplEx:</strong> Use complex-valued embeddings. Model asymmetric relations and composition better.
  <div class="equation">
$$\begin{align}
  \text{score}(h, r, t) = \text{Re}(\mathbf{h}^T \text{diag}(\mathbf{r}) \overline{\mathbf{t}})
  \end{align}$$
</div>

<p><li><strong>RotatE:</strong> Represent relations as rotations in complex space. Captures composition and inversion.
  <div class="equation">
$$\begin{align}
  \mathbf{h} \circ \mathbf{r} = \mathbf{t}
  \end{align}$$
</div>
  where \(\circ\) is element-wise product in complex space.
</ul>

<p>Each model trades simplicity for expressiveness. TransE is fastest; RotatE is most expressive but slower.</p>

<h2>Link Prediction and Reasoning</h2>

<p>A key application: predict missing relationships (link prediction).</p>

<div class="definition"><strong>Definition:</strong> 
Given a partial knowledge graph with some missing edges, predict which relationships are most likely to exist.

<p>Example: Given (Barack Obama, spouse, ?), predict the tail entity. Correct answer: Michelle Obama.</p>

<p>This addresses the incompleteness of knowledge graphs.
</div>

<h3>Ranking-Based Link Prediction</h3>

<p>For a query (h, r, ?), rank candidate entities by likelihood:</p>

<div class="equation">
$$\begin{align}
\text{score}(h, r, t) = f(\mathbf{h}, \mathbf{r}, \mathbf{t})
\end{align}$$
</div>

<p>Using TransE: score = \(-||(\mathbf{h} + \mathbf{r}) - \mathbf{t}||_2\) (higher is better).</p>

<p>Rank all entities; top-k are predictions.</p>

<h3>Evaluation Metrics</h3>

<p><strong>Mean Reciprocal Rank (MRR):</strong> Average rank of correct entity.
<div class="equation">
$$\begin{align}
\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}
\end{align}$$
</div>

<p>Higher is better. Perfect: MRR = 1. Random: MRR ‚âà 1/|E| where |E| is number of entities.</p>

<p><strong>Hits@k:</strong> Fraction of queries where correct entity in top-k.
<div class="equation">
$$\begin{align}
\text{Hits@k} = \frac{\# \text{correct in top-k}}{|Q|}
\end{align}$$
</div>

<p>Common metrics: Hits@1, Hits@10.</p>

<h2>Semantic Type Inference and Reasoning</h2>

<p>Beyond individual triples, knowledge graphs support reasoning.</p>

<h3>Type Constraints</h3>

<p>Each relation has type constraints:</p>

<ul>
<li><em>born\_in</em>: head type = Person, tail type = Location
<li><em>founder</em>: head type = Person, tail type = Organization
<li><em>contains</em>: head type = Container, tail type = Thing
</ul>

<p>Type constraints reduce link prediction space. For (Barack Obama, born\_in, ?), candidates must be locations.</p>

<h3>Reasoning Rules</h3>

<p>Knowledge graphs support inference rules:</p>

<ul>
<li><strong>Composition:</strong> (X, father\_of, Y) ‚àß (Y, father\_of, Z) ‚Üí (X, grandfather\_of, Z)
<li><strong>Symmetry:</strong> (X, married\_to, Y) ‚Üí (Y, married\_to, X)
<li><strong>Inversion:</strong> (X, parent\_of, Y) ‚Üí (Y, child\_of, X)
<li><strong>Subrelation:</strong> (X, parent\_of, Y) ‚Üí (X, ancestor\_of, Y)
</ul>

<p>Deep models (especially RotatE) capture these patterns in learned embeddings without explicit rules.</p>

<h2>Temporal Knowledge Graphs and Dynamic Knowledge</h2>

<p>Real-world knowledge evolves over time. Static knowledge graphs cannot capture temporal dynamics: facts that are true at specific times, relationships that change, and entities whose properties evolve.</p>

<h3>Temporal Knowledge Representation</h3>

<div class="definition"><strong>Definition:</strong> 
A temporal knowledge graph extends standard KGs with time information:
<ul>
<li><strong>Temporal triples:</strong> (subject, predicate, object, timestamp) or (subject, predicate, object, [start, end])
<li><strong>Example:</strong> (Barack Obama, president\_of, USA, [2009-01-20, 2017-01-20])
<li><strong>Point-in-time facts:</strong> (Company X, stock\_price, \$150, 2024-01-30)
<li><strong>Event sequences:</strong> Ordered series of facts representing processes
</ul>
</div>

<h3>Temporal Reasoning Challenges</h3>

<ul>
<li><strong>Validity periods:</strong> When is a fact true? (Barack Obama, president\_of, USA) is only valid 2009--2017
<li><strong>Temporal consistency:</strong> A person cannot be in two places simultaneously
<li><strong>Temporal inference:</strong> If X was true at time T1, and no contradicting fact exists, is X still true at T2?
<li><strong>Forecasting:</strong> Predict future facts based on historical patterns
</ul>

<h3>Temporal Embedding Models</h3>

<p>Extend static embedding models to incorporate time:</p>

<p><strong>TTransE (Temporal TransE):</strong>
<div class="equation">
$$\begin{align}
\mathbf{h} + \mathbf{r} + \mathbf{t}_{\text{time}} \approx \mathbf{t}
\end{align}$$
</div>

<p>Time is embedded as a vector; added to the translation.</p>

<p><strong>DE-SimplE (Diachronic Embeddings):</strong>
Model entities and relations as functions of time:
<div class="equation">
$$\begin{align}
\mathbf{h}(t), \mathbf{r}(t), \mathbf{t}_{\text{entity}}(t)
\end{align}$$
</div>

<p>Embeddings evolve smoothly over time using recurrent networks or temporal convolutions.</p>

<p><strong>TeMP (Temporal Message Passing):</strong>
GNN-based approach where messages are time-aware:
<div class="equation">
$$\begin{align}
\mathbf{h}_i^{(t+1)} = \text{Aggregate}(\{\mathbf{h}_j^{(t)}, \mathbf{r}_{ij}, \tau_{ij}\})
\end{align}$$
</div>

<p>where $\tau_{ij}$ is the timestamp of the relationship.</p>

<h3>Temporal Link Prediction</h3>

<p>Predict future relationships based on historical patterns:</p>

<p><strong>Task:</strong> Given knowledge graph up to time T, predict facts at time T+1.</p>

<p><strong>Example:</strong> Historical pattern shows companies acquire competitors before IPO. Predict: (Company X, will\_acquire, Company Y, 2025).</p>

<p><strong>Applications:</strong>
<ul>
<li>Stock market prediction: Predict corporate events (mergers, partnerships)
<li>Geopolitical forecasting: Predict diplomatic relationships, conflicts
<li>Healthcare: Predict disease progression based on patient history
</ul>

<h3>Recent Advances in Temporal Knowledge Graph Reasoning (2024-2025)</h3>

<p>Temporal knowledge graph reasoning has advanced significantly in 2024 with dynamic hypergraph embedding methods that better capture complex temporal patterns and multi-way relationships.</p>

<p><strong>Dynamic Hypergraph Embeddings:</strong> Traditional temporal KG methods model binary relationships (subject-predicate-object) evolving over time. However, many real-world events involve multiple entities simultaneously. Dynamic hypergraph embeddings extend temporal KGs to hyperedges connecting multiple entities, enabling richer temporal reasoning.</p>

<p>Example: A corporate merger involves multiple entities: acquiring company, target company, regulatory bodies, financial advisors, and shareholders. A hyperedge represents this multi-party event with temporal validity. Traditional binary relations (Company A, acquires, Company B) miss the full context.</p>

<p><strong>Key innovations in 2024-2025:</strong></p>

<ul>
<li><strong>Temporal hypergraph attention:</strong> Attention mechanisms that weight the importance of different entities within a hyperedge and across time. This enables learning which participants in multi-entity events are most predictive of future events.

<p><li><strong>Continuous-time modeling:</strong> Instead of discrete timestamps, model events in continuous time using neural ordinary differential equations (Neural ODEs). This enables interpolation between observed events and more accurate forecasting.</p>

<p><li><strong>Causal temporal reasoning:</strong> Distinguish correlation from causation in temporal patterns. If event A precedes event B, is A causing B, or are both caused by hidden factor C? Causal inference methods (do-calculus, counterfactual reasoning) integrated with temporal KG embeddings improve prediction accuracy by 15-25\% on forecasting tasks.</p>

<p><li><strong>Multi-scale temporal modeling:</strong> Events occur at different timescales‚Äîsome relationships change daily (stock prices), others yearly (corporate structure). Hierarchical temporal models with separate encoders for different timescales capture both short-term dynamics and long-term trends.
</ul>

<p><strong>Implementation Considerations:</strong> Dynamic hypergraph methods are computationally expensive‚Äîtraining requires 3-5x more compute than standard temporal KG methods. However, the improved accuracy often justifies the cost for high-value applications. Open-source implementations are emerging in PyTorch Geometric and DGL (Deep Graph Library) as of 2024-2025.</p>

<h2>Graph Neural Networks for Knowledge Graphs</h2>

<p>Graph Neural Networks (GNNs) have become the dominant approach for learning on graph-structured data, including knowledge graphs.</p>

<h3>Relational Graph Convolutional Networks (R-GCN)</h3>

<p>Standard GCNs assume homogeneous graphs (single edge type). R-GCN extends to multi-relational graphs:</p>

<div class="definition"><strong>Definition:</strong> 
For entity $i$ with embedding $\mathbf{h}_i$, update using neighbors:
<div class="equation">
$$\begin{align}
\mathbf{h}_i^{(l+1)} = \sigma\left(\sum_{r \in \mathcal{R}} \sum_{j \in \mathcal{N}_i^r} \frac{1}{|\mathcal{N}_i^r|} \mathbf{W}_r^{(l)} \mathbf{h}_j^{(l)} + \mathbf{W}_0^{(l)} \mathbf{h}_i^{(l)}\right)
\end{align}$$
</div>

<p>where:
<ul>
<li>$\mathcal{R}$ is the set of relation types
<li>$\mathcal{N}_i^r$ is the set of neighbors connected via relation $r$
<li>$\mathbf{W}_r^{(l)}$ is a relation-specific weight matrix
<li>$\mathbf{W}_0^{(l)}$ is a self-loop weight matrix
</ul>
</div>

<p><strong>Key insight:</strong> Different relation types have different semantics; use separate weight matrices.</p>

<p><strong>Scalability challenge:</strong> With thousands of relation types, storing $|\mathcal{R}|$ weight matrices is memory-intensive.</p>

<p><strong>Solution - Basis decomposition:</strong>
<div class="equation">
$$\begin{align}
\mathbf{W}_r = \sum_{b=1}^B a_{rb} \mathbf{V}_b
\end{align}$$
</div>

<p>Express each relation weight as a linear combination of $B$ basis matrices (where $B \ll |\mathcal{R}|$).</p>

<h3>Graph Attention Networks for KGs</h3>

<p>Attention mechanisms allow the model to learn which neighbors are most important:</p>

<div class="definition"><strong>Definition:</strong> 
Compute attention weights for each neighbor:
<div class="equation">
$$\begin{align}
\alpha_{ij}^r &= \frac{\exp(\text{LeakyReLU}(\mathbf{a}^T [\mathbf{W}_r \mathbf{h}_i || \mathbf{W}_r \mathbf{h}_j]))}{\sum_{k \in \mathcal{N}_i} \exp(\text{LeakyReLU}(\mathbf{a}^T [\mathbf{W}_r \mathbf{h}_i || \mathbf{W}_r \mathbf{h}_k]))} \\
\mathbf{h}_i^{(l+1)} &= \sigma\left(\sum_{r \in \mathcal{R}} \sum_{j \in \mathcal{N}_i^r} \alpha_{ij}^r \mathbf{W}_r \mathbf{h}_j^{(l)}\right)
\end{align}$$
</div>
</div>

<p><strong>Advantages:</strong>
<ul>
<li>Learns importance of different neighbors dynamically
<li>Handles varying neighborhood sizes naturally
<li>Interpretable: attention weights show which relationships matter
</ul>

<h3>Multi-Hop Reasoning with GNNs</h3>

<p>GNNs naturally support multi-hop reasoning by stacking layers:</p>

<ul>
<li><strong>1-hop:</strong> Direct neighbors (1 layer)
<li><strong>2-hop:</strong> Neighbors of neighbors (2 layers)
<li><strong>k-hop:</strong> k layers capture k-hop neighborhood
</ul>

<strong>Example reasoning:</strong>
<pre><code>
Query: "What diseases might drug X treat?"
1-hop: Drug X targets protein P
2-hop: Protein P is involved in disease D
Inference: Drug X might treat disease D
</code></pre>

<h3>Practical Implementation</h3>

<strong>PyTorch Geometric example (simplified):</strong>
<pre><code>
import torch
from torch_geometric.nn import RGCNConv

class KnowledgeGraphGNN(torch.nn.Module):
    def __init__(self, num_entities, num_relations, hidden_dim):
        super().__init__()
        self.embedding = torch.nn.Embedding(num_entities, hidden_dim)
        self.conv1 = RGCNConv(hidden_dim, hidden_dim, num_relations)
        self.conv2 = RGCNConv(hidden_dim, hidden_dim, num_relations)
    
    def forward(self, edge_index, edge_type):
        x = self.embedding.weight
        x = self.conv1(x, edge_index, edge_type)
        x = torch.relu(x)
        x = self.conv2(x, edge_index, edge_type)
        return x
</code></pre>

<h2>Knowledge Graph Completion and Multi-Hop Reasoning</h2>

<p>Link prediction focuses on single missing edges. Knowledge graph completion addresses systematic incompleteness through multi-hop reasoning.</p>

<h3>Path-Based Reasoning</h3>

<p>Rather than direct embeddings, reason over paths connecting entities:</p>

<div class="definition"><strong>Definition:</strong> 
For query (h, r, ?), find paths from h to candidate tails:
<ol>
<li>Extract all paths of length $\leq k$ from h to candidate entities
<li>Represent each path as a sequence of relations: $r_1 \rightarrow r_2 \rightarrow \ldots \rightarrow r_n$
<li>Learn weights for path patterns that predict relation $r$
<li>Score candidates by weighted sum of paths
</ol>

<p><strong>Example:</strong>
Query: (Barack Obama, nationality, ?)
Paths:
<ul>
<li>born\_in $\rightarrow$ located\_in $\rightarrow$ USA (strong signal)
<li>president\_of $\rightarrow$ USA (strong signal)
<li>spouse $\rightarrow$ nationality $\rightarrow$ USA (weaker signal)
</ul>
</div>

<h3>Neural Logic Programming</h3>

<p>Combine neural networks with logic programming for interpretable reasoning:</p>

<p><strong>Neural LP:</strong> Learn logical rules as differentiable operations:
<div class="equation">
$$\begin{align}
\text{confidence}(h, r, t) = \max_{\text{path } \pi} \prod_{r_i \in \pi} \text{score}(r_i)
\end{align}$$
</div>

<p>The model learns which rule chains (paths) are most predictive.</p>

<p><strong>Advantages:</strong>
<ul>
<li>Interpretable: Rules can be extracted and understood
<li>Compositional: Learns to compose relations
<li>Data-efficient: Generalizes from fewer examples
</ul>

<h3>Query Answering Beyond Link Prediction</h3>

<p>Complex queries require reasoning beyond single edges:</p>

<strong>Conjunctive queries:</strong>
<pre><code>
Find: Actors who starred in movies directed by Steven Spielberg 
      AND released after 2000

Query: (?, starred_in, ?m) ‚àß (?m, directed_by, Spielberg) 
       ‚àß (?m, release_year, >2000)
</code></pre>

<p><strong>Query2Box:</strong> Represent queries as geometric regions (boxes) in embedding space:
<ul>
<li>Entities are points
<li>Queries are boxes (hyperrectangles)
<li>Intersection of boxes = conjunction
<li>Entities inside box satisfy query
</ul>

<h2>Ontology Alignment and Knowledge Integration</h2>

<p>Real-world applications require integrating multiple knowledge graphs with different schemas and vocabularies.</p>

<h3>Entity Alignment Problem</h3>

<div class="definition"><strong>Definition:</strong> 
Given two knowledge graphs $KG_1$ and $KG_2$, identify entity pairs that refer to the same real-world object:
<ul>
<li>$KG_1$: ``Barack\_Obama'' (entity ID: 12345)
<li>$KG_2$: ``Obama, Barack'' (entity ID: 98765)
<li>Goal: Recognize these refer to the same person
</ul>
</div>

<p><strong>Challenges:</strong>
<ul>
<li>Name variations: ``NYC'' vs. ``New York City'' vs. ``New York, NY''
<li>Different schemas: One KG has ``birth\_date'', another has ``born\_on''
<li>Incomplete information: Entities may have different attributes in each KG
<li>Scale: Billions of entities; quadratic comparison infeasible
</ul>

<h3>Embedding-Based Entity Alignment</h3>

<p>Learn joint embeddings for entities from both KGs:</p>

<ol>
<li><strong>Separate embedding:</strong> Train embeddings for each KG independently
<li><strong>Seed alignment:</strong> Use known entity matches (seed set) to learn alignment
<li><strong>Joint optimization:</strong> Minimize distance between known matching entities:
<div class="equation">
$$\begin{align}
\mathcal{L}_{\text{align}} = \sum_{(e_1, e_2) \in \text{seeds}} ||\mathbf{h}_{e_1} - \mathbf{h}_{e_2}||^2
\end{align}$$
</div>
<li><strong>Inference:</strong> For unmatched entities, find nearest neighbor in other KG
</ol>

<p><strong>Advanced methods:</strong>
<ul>
<li><strong>GNN-based:</strong> Use graph structure to improve alignment (neighbors of aligned entities likely align)
<li><strong>Attribute matching:</strong> Compare entity attributes (names, descriptions) using text similarity
<li><strong>Iterative refinement:</strong> Bootstrap from seed alignments; iteratively add confident matches
</ul>

<h3>Cross-Lingual Knowledge Graph Alignment</h3>

<p>Align KGs in different languages:</p>

<p><strong>Example:</strong>
<ul>
<li>English KG: ``Paris'' (city entity)
<li>French KG: ``Paris'' (ville entity)
<li>Goal: Recognize these are the same despite language difference
</ul>

<p><strong>Approach:</strong>
<ul>
<li>Use multilingual embeddings (mBERT, XLM-R) for entity names
<li>Leverage cross-lingual links (Wikipedia interlanguage links)
<li>Learn language-invariant entity representations
</ul>

<h3>Schema Matching</h3>

<p>Beyond entities, align relation types and ontologies:</p>

<p><strong>Example:</strong>
<ul>
<li>$KG_1$ uses: ``born\_in'' (relation)
<li>$KG_2$ uses: ``birthplace'' (relation)
<li>Goal: Recognize semantic equivalence
</ul>

<p><strong>Methods:</strong>
<ul>
<li>String similarity: Edit distance, token overlap
<li>Semantic similarity: Embed relation names; compare embeddings
<li>Instance-based: If entities connected by $r_1$ in $KG_1$ align with entities connected by $r_2$ in $KG_2$, relations likely equivalent
</ul>

<h2>Knowledge-Aware Neural Networks</h2>

<p>Rather than separate text and knowledge graph processing, integrate them.</p>

<h3>Knowledge-Enhanced Embeddings</h3>

<p>Combine word embeddings (learned from text) with entity embeddings (from graph):</p>

<ol>
<li>Text encodes entity using contextual embeddings (BERT)
<li>Lookup entity embedding from knowledge graph
<li>Combine using gating or concatenation
<li>Result: entity representation aware of both text and structured knowledge
</ol>

<h3>Graph Neural Networks (GNNs) for Knowledge Graphs</h3>

<p>GNNs propagate information through graph structure.</p>

<p><strong>Message passing:</strong>
<ol>
<li>Each entity sends its embedding to neighbors
<li>Neighbors aggregate messages using relation-specific functions
<li>Result: updated entity embeddings reflecting neighborhood
<li>Repeat for multiple layers
</ol>

<p><strong>Application:</strong> Node classification (predict entity type), link prediction, relation classification.</p>

<h2>Evaluation Metrics and Quality Assessment</h2>

<p>Evaluating knowledge graph models requires careful consideration of metrics, biases, and real-world utility.</p>

<h3>Filtered vs. Raw Evaluation</h3>

<p><strong>Raw evaluation:</strong> Rank all entities for link prediction, including those already in the training set.</p>

<p><strong>Problem:</strong> If (Barack Obama, spouse, Michelle Obama) is in training, and we test (Barack Obama, spouse, ?), Michelle Obama should rank first. But if (Barack Obama, spouse, Hillary Clinton) is also in training (incorrectly), the model is penalized for ranking it low.</p>

<p><strong>Filtered evaluation:</strong> Remove all known true triples (from train, validation, test) except the target triple before ranking.</p>

<div class="definition"><strong>Definition:</strong> 
For query (h, r, ?):
<ol>
<li>Rank all candidate entities by score
<li>Remove entities $t'$ where (h, r, $t'$) exists in train/val/test (except target)
<li>Compute rank of target entity in filtered list
<li>Calculate MRR, Hits@k on filtered ranks
</ol>
</div>

<p><strong>Impact:</strong> Filtered metrics are typically 10--30\% higher than raw metrics. Always report which evaluation protocol is used.</p>

<h3>Evaluation Biases</h3>

<p><strong>Popularity bias:</strong> Models may learn to predict popular entities (high degree nodes) regardless of query. Evaluation should stratify by entity popularity.</p>

<p><strong>Relation difficulty:</strong> Some relations are easier to predict (1-to-1 like ``spouse'') than others (many-to-many like ``acted\_in''). Report per-relation performance.</p>

<p><strong>Test leakage:</strong> If test set contains inverse relations of training triples, evaluation is inflated. Example: Train on (A, parent\_of, B); test on (B, child\_of, A).</p>

<h3>Human Evaluation</h3>

<p>Automated metrics don't capture semantic correctness:</p>

<p><strong>Precision@k:</strong> For top-k predictions, what fraction are actually correct (verified by humans)?</p>

<p><strong>Plausibility:</strong> Even if not in ground truth, is the prediction plausible? (Barack Obama, friend\_of, Joe Biden) may not be in KG but is plausible.</p>

<p><strong>Diversity:</strong> Do predictions cover diverse entity types, or are they repetitive?</p>

<p><strong>Practical protocol:</strong>
<ol>
<li>Sample 100--500 test queries
<li>For each, show top-5 predictions to human annotators
<li>Annotators mark: correct, plausible, incorrect
<li>Compute precision, plausibility rate
</ol>

<h3>Downstream Task Evaluation</h3>

<p>Ultimate test: Does the KG improve downstream applications?</p>

<p><strong>Question answering:</strong> Does KG-augmented QA system answer more questions correctly?</p>

<p><strong>Search:</strong> Do users click on KG-enhanced search results more often?</p>

<p><strong>Recommendations:</strong> Does KG-based recommender improve engagement?</p>

<p>Offline metrics (MRR, Hits@k) are proxies; online A/B tests measure real impact.</p>

<h2>Practical Implementation and Tooling</h2>

<p>Building production knowledge graph systems requires specialized tools and infrastructure.</p>

<h3>Knowledge Graph Storage Systems</h3>

<p><strong>RDF Triple Stores:</strong>
<ul>
<li><strong>Apache Jena:</strong> Java-based RDF store with SPARQL query support
<li><strong>Virtuoso:</strong> High-performance RDF database; scales to billions of triples
<li><strong>Blazegraph:</strong> GPU-accelerated graph database
</ul>

<p><strong>Property Graph Databases:</strong>
<ul>
<li><strong>Neo4j:</strong> Most popular graph database; Cypher query language
<li><strong>Amazon Neptune:</strong> Managed graph database (RDF + property graphs)
<li><strong>JanusGraph:</strong> Distributed graph database built on Cassandra/HBase
</ul>

<p><strong>Trade-offs:</strong>
<ul>
<li>RDF stores: Standards-compliant, semantic web integration, complex queries
<li>Property graphs: Simpler model, better performance for traversals, richer data model
</ul>

<h3>KG Embedding Libraries</h3>

<strong>PyKEEN (Python Knowledge Embeddings):</strong>
<pre><code>
from pykeen.pipeline import pipeline

result = pipeline(
    dataset='FB15k-237',
    model='TransE',
    training_kwargs=dict(num_epochs=100),
    evaluation_kwargs=dict(batch_size=256),
)

# Access trained model
model = result.model
# Predict missing links
predictions = model.predict_tails('Barack_Obama', 'born_in')
</code></pre>

<strong>DGL-KE (Deep Graph Library - Knowledge Embeddings):</strong>
<pre><code>
import dglke

# Train TransE on custom dataset
dglke.train(
    model_name='TransE',
    dataset='my_kg',
    data_path='./data/',
    save_path='./ckpts/',
    max_step=100000,
    batch_size=1024,
    neg_sample_size=256,
    hidden_dim=200,
    gamma=12.0,
    lr=0.1,
)
</code></pre>

<p><strong>OpenKE:</strong> C++ backend with Python interface; optimized for large-scale training.</p>

<h3>Query APIs and SPARQL</h3>

<p><strong>SPARQL:</strong> Standard query language for RDF graphs:</p>

<pre><code>
PREFIX dbo: <http://dbpedia.org/ontology/>
PREFIX dbr: <http://dbpedia.org/resource/>

SELECT ?movie ?releaseDate
WHERE {
  ?movie dbo:director dbr:Steven_Spielberg .
  ?movie dbo:releaseDate ?releaseDate .
  FILTER (?releaseDate > "2000-01-01"^^xsd:date)
}
ORDER BY DESC(?releaseDate)
LIMIT 10
</code></pre>

<strong>Cypher (Neo4j):</strong>
<pre><code>
MATCH (director:Person {name: "Steven Spielberg"})-[:DIRECTED]->(movie:Movie)
WHERE movie.releaseDate > date("2000-01-01")
RETURN movie.title, movie.releaseDate
ORDER BY movie.releaseDate DESC
LIMIT 10
</code></pre>

<h3>End-to-End Pipeline Example</h3>

<p><strong>Building a domain-specific KG:</strong></p>

<pre><code>
# 1. Entity extraction from text corpus
from transformers import pipeline

ner = pipeline("ner", model="dslim/bert-base-NER")
entities = ner("Apple Inc. was founded by Steve Jobs in 1976.")

# 2. Relation extraction
from opennre import get_model

rel_model = get_model('wiki80_bert_softmax')
relations = rel_model.infer({
    'text': 'Apple Inc. was founded by Steve Jobs in 1976.',
    'h': {'pos': (0, 10)},  # Apple Inc.
    't': {'pos': (27, 38)}   # Steve Jobs
})

# 3. Store in graph database
from neo4j import GraphDatabase

driver = GraphDatabase.driver("bolt://localhost:7687")
with driver.session() as session:
    session.run("""
        MERGE (a:Company {name: 'Apple Inc.'})
        MERGE (p:Person {name: 'Steve Jobs'})
        MERGE (a)-[:FOUNDED_BY]->(p)
    """)

# 4. Train embeddings
from pykeen.pipeline import pipeline

result = pipeline(
    dataset='my_kg',
    model='ComplEx',
    training_kwargs=dict(num_epochs=50),
)

# 5. Query and predict
predictions = result.model.predict_tails('Apple_Inc', 'COMPETITOR')
</code></pre>

<h2>Cross-Chapter Connections</h2>

<p>Knowledge graphs integrate with many domains covered in this book:</p>

<h3>Connection to Chapter 25: Enterprise NLP</h3>

<p><strong>Entity extraction:</strong> NER and relation extraction (Chapter 25) populate knowledge graphs. KGs provide structured output for NLP pipelines.</p>

<p><strong>Semantic search:</strong> Combine text embeddings (Chapter 25) with KG embeddings for hybrid retrieval: text similarity + graph structure.</p>

<p><strong>Example:</strong> Customer support system uses NER to extract entities from tickets, links to customer KG, retrieves relevant context.</p>

<h3>Connection to Chapter 29: Recommendations</h3>

<p><strong>Collaborative filtering parallels:</strong> Matrix factorization in recommenders is analogous to KG embeddings. Both learn latent representations.</p>

<p><strong>Hybrid systems:</strong> Combine user-item interactions (Chapter 29) with item knowledge graph. Recommend items similar in both interaction patterns and semantic properties.</p>

<p><strong>Example:</strong> Movie recommender uses KG (actors, directors, genres) + user watch history for better cold-start performance.</p>

<h3>Connection to Chapter 34: DSL and Agents</h3>

<p><strong>Structured reasoning:</strong> KGs provide formal language for agent reasoning. Agents query KGs to retrieve facts, infer new knowledge.</p>

<p><strong>Tool augmentation:</strong> KG query APIs are tools agents can call. Agent decides when to query KG vs. generate from language model.</p>

<p><strong>Example:</strong> Question-answering agent first queries KG for factual information, then generates natural language response using retrieved facts.</p>

<h3>Connection to Chapter 30: Healthcare</h3>

<p><strong>Biomedical KGs:</strong> Drug-target-disease graphs enable precision medicine. Link prediction identifies drug repurposing opportunities.</p>

<p><strong>Clinical reasoning:</strong> Patient symptoms + medical KG ‚Üí differential diagnosis through graph traversal.</p>

<p><strong>Example:</strong> Given patient symptoms, traverse disease-symptom KG to rank likely diagnoses; suggest tests to disambiguate.</p>

<h2>Scalability and Practical Considerations</h2>

<p>Production KGs are enormous: billions of entities, billions of relationships.</p>

<h3>Embedding Computational Cost</h3>

<p>TransE requires scoring candidate triples during training:
<div class="equation">
$$\begin{align}
\text{cost per epoch} = |S| \times |E|
\end{align}$$
</div>

<p>For Freebase (1.9B entities, 3B relations), this is intractable.</p>

<p><strong>Solutions:</strong>
<ul>
<li><strong>Negative sampling:</strong> Instead of all entities, sample small number of negatives (100--1000)
<li><strong>Batch optimization:</strong> Group triples, batch compute similarities
<li><strong>Sparse storage:</strong> Store only non-zero parts of embeddings
<li><strong>Hierarchical models:</strong> Partition entities into clusters; compute within clusters
</ul>

<h3>Incompleteness and Noise</h3>

<p>Knowledge graphs have two inherent problems:</p>

<ul>
<li><strong>Incompleteness:</strong> Most relationships are unknown, not just absent. Freebase is estimated <5\% complete.
<li><strong>Noise:</strong> Extracted facts may be incorrect (extraction errors, outdated information)
</ul>

<p>Deep learning must handle both:
<ul>
<li>Learn robust representations despite missing training data
<li>Noise-aware loss functions (soft labels, confidence scores)
<li>Continuous retraining as new information arrives
</ul>

<h2>Applications: From Search to Drug Discovery</h2>

<h3>Semantic Search and Question Answering</h3>

<p>Google's Knowledge Graph improves search results:</p>

<p>Query: ``Where was Albert Einstein born?''</p>

<p>Traditional: Keyword search returns pages mentioning ``Albert Einstein'' and ``born.''</p>

<p>With KG:
<ol>
<li>Identify entity: ``Albert Einstein'' (physicist entity)
<li>Follow relation: born\_in ‚Üí ``Ulm, Germany''
<li>Display: Direct answer in result panel
</ol>

<p>Better user experience; faster answer discovery.</p>

<h3>Biomedical Discovery: Drug-Target Interaction Prediction</h3>

<p>In drug discovery, predicting drug-target interactions is expensive and time-consuming.</p>

<p><strong>Knowledge graph:</strong> Entities = proteins, diseases, drugs. Relationships = acts\_on, treats, causes.</p>

<p><strong>Link prediction:</strong> For a new drug, predict which proteins it targets.</p>

<p><strong>Validation:</strong> Lab experiments confirm predictions.</p>

<p>Real example: Using link prediction on biomedical KG, researchers identified new targets for existing drugs, enabling drug repurposing.</p>

<h3>Cybersecurity: Attack Pattern Detection</h3>

<p>A KG models:
<ul>
<li>Entities: IP addresses, domains, malware families, attack techniques
<li>Relationships: communicates\_with, infected\_by, exploits
</ul>

<p>Link prediction identifies likely attack patterns: if A communicated with C, and B typically attacks C, predict A is infected with B.</p>

<h2>Case Study: Enterprise Knowledge Graph for Customer Intelligence</h2>

<p>A financial services company maintains customer data scattered across systems. A knowledge graph unifies and enables new insights.</p>

<h3>Data Sources</h3>

<ul>
<li>Customer records: name, contact, demographics
<li>Account data: account type, balance, transaction history
<li>Relationships: family members, business associates, employer
<li>Transactions: transfers, purchases, patterns
<li>External: credit scores, public records, news mentions
</ul>

<h3>Knowledge Graph Schema</h3>

<p>Entities:
<ul>
<li>Person: customer, employee, beneficiary
<li>Organization: employer, business partner
<li>Account: checking, savings, investment
<li>Transaction: payment, transfer, purchase
</ul>

<p>Relationships:
<ul>
<li>owns\_account, employed\_by, related\_to, transacted\_with
<li>account\_has\_transaction, person\_has\_credit\_score
</ul>

<h3>Applications</h3>

<p><strong>Fraud detection:</strong> If customer A suddenly sends money to account in country where they've never been, and that account is related to known fraud cases, flag as suspicious.</p>

<p><strong>Customer segmentation:</strong> Customers with similar network structures (family members, employers, transaction patterns) are grouped; targeted offers designed for groups.</p>

<p><strong>Risk assessment:</strong> Credit decision uses not just customer features but related customers' credit histories.</p>

<h3>Results</h3>

<ul>
<li>Fraud detection improved from 85\% to 92\% precision (fewer false alarms)
<li>Customer segmentation identified high-value customer clusters (3x avg transaction volume)
<li>Cross-selling improved: 8\% increase in secondary products purchased
<li>Entity resolution: Matched 98\% of duplicate customer records
</ul>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> Extract entities and relationships from the following text using NER and relation extraction. ``Apple Inc. was founded by Steve Jobs, Ronald Wayne, and Steve Wozniak in Los Altos. The company released the Macintosh in 1984.''
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Train a TransE embedding model on a small knowledge graph (100 entities, 200 relations). Evaluate link prediction performance on held-out test set using MRR and Hits@10.
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> Design a knowledge graph schema for a movie recommendation system. What entities and relationships would be necessary? How would link prediction help recommendations?
</div>

<div class="exercise" id="exercise-4"><strong>Exercise 4:</strong> Design a temporal knowledge graph for tracking corporate events (mergers, acquisitions, executive changes). What temporal reasoning capabilities would be most valuable? How would you handle conflicting information from different sources?
</div>

<div class="exercise" id="exercise-5"><strong>Exercise 5:</strong> Implement entity alignment between two knowledge graphs using embedding-based methods. Evaluate precision and recall at different similarity thresholds. How does performance vary with seed set size?
</div>

<div class="exercise" id="exercise-6"><strong>Exercise 6:</strong> Build an R-GCN model for node classification on a multi-relational graph. Compare performance against TransE embeddings. When does the GNN approach outperform embedding-only methods?
</div>

<h2>Solutions</h2>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 1: Entity and Relation Extraction</strong>

<p>\itshape Entities:
<ul>
<li>Apple Inc. (Organization)
<li>Steve Jobs (Person)
<li>Ronald Wayne (Person)
<li>Steve Wozniak (Person)
<li>Los Altos (Location)
<li>Macintosh (Product)
<li>1984 (Date)
</ul>

<p>\itshape Relations:
<ul>
<li>(Steve Jobs, founded, Apple Inc.)
<li>(Ronald Wayne, founded, Apple Inc.)
<li>(Steve Wozniak, founded, Apple Inc.)
<li>(Apple Inc., headquarters, Los Altos)
<li>(Apple Inc., released, Macintosh)
<li>(Macintosh, release\_date, 1984)
</ul>

<p>\itshape Note:
Multiple entities can have same relation with an object (founded). A good information extraction model should capture all of them.
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 2: TransE Embedding and Link Prediction</strong>

<p>\itshape Experimental setup:
<ul>
<li>Create small KG: 100 entities, 200 relations from scratch
<li>Train/test split: 160 relations (train), 40 relations (test)
<li>TransE parameters: embedding dim = 50, margin = 1.0, learning rate = 0.001
<li>Negative sampling: 10 negatives per positive
<li>Training: 100 epochs
</ul>

<p>\itshape Results:
<ul>
<li>MRR: 0.65 (good; mean rank of correct entity ‚âà 1.5)
<li>Hits@10: 0.90 (90\% of queries have correct answer in top 10)
<li>Hits@1: 0.55 (55\% of queries have correct answer ranked first)
</ul>

<p>\itshape Analysis:
TransE performs well on this toy KG. On real graphs (billions of entities), performance would be lower (more candidates to rank).
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 3: Movie Recommendation KG</strong>

<p>\itshape Schema:</p>

<p>Entities:
<ul>
<li>Movie: title, release\_year, budget, revenue
<li>Person: actor, director, producer, user
<li>Genre: action, comedy, drama, etc.
<li>Studio: production company
<li>User: customer profile
</ul>

<p>Relations:
<ul>
<li>Structural: starred\_in, directed\_by, produced\_by, released\_by, has\_genre
<li>User-driven: watched, rated, reviewed
<li>Content: similar\_to, prequel\_to, based\_on
<li>Metadata: actor\_worked\_with (co-appearances)
</ul>

<p>\itshape Link prediction for recommendations:</p>

<p>1. User A watched movies M1, M2, M3
2. Extract features: genres, directors, actors
3. Predict: What movies would User A enjoy?
4. Link prediction: (User A, should\_watch, ?)
5. Rank candidate movies based on embeddings</p>

<p>Users with similar watching histories are embedded nearby; recommend movies watched by similar users.</p>

<p>\itshape Advantages over content-based filtering:
<ul>
<li>Captures complex patterns (user groups, hidden factors)
<li>Incorporates collaborative signal (what similar users liked)
<li>Enables discovery (recommend movies different from user's past, but similar to users with similar taste)
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 4: Temporal Knowledge Graph for Corporate Events</strong>

<p>\itshape Schema Design:</p>

<p>Entities:
<ul>
<li>Company: name, industry, founding\_date, headquarters
<li>Person: name, role, start\_date, end\_date
<li>Event: type (merger, acquisition, IPO, bankruptcy), date, participants
</ul>

<p>Temporal Relations:
<ul>
<li>(Company A, acquired, Company B, [2024-03-15, ongoing])
<li>(Person X, CEO\_of, Company A, [2020-01-01, 2023-12-31])
<li>(Company A, merged\_with, Company B, [2024-06-01, 2024-06-01]) (point event)
</ul>

<p>\itshape Temporal Reasoning Capabilities:</p>

<p>1. <strong>Validity queries:</strong> ``Who was CEO of Apple in 2015?''
   - Query entities with valid time range overlapping 2015</p>

<p>2. <strong>Event sequencing:</strong> ``What happened to Company X before acquisition?''
   - Retrieve events with timestamps before acquisition date
   - Identify patterns (e.g., executive changes often precede acquisitions)</p>

<p>3. <strong>Forecasting:</strong> ``Will Company A acquire Company B?''
   - Historical pattern: Companies in same industry with recent partnerships often merge
   - Temporal embedding predicts future relationships</p>

<p>4. <strong>Conflict resolution:</strong> Multiple sources report different acquisition dates
   - Weight by source reliability (SEC filings > news articles > social media)
   - Use latest information if contradictory
   - Store provenance: (fact, source, confidence, timestamp)</p>

<p>\itshape Handling Conflicting Information:</p>

<ul>
<li><strong>Confidence scores:</strong> Each fact has confidence [0, 1] based on source reliability
<li><strong>Provenance tracking:</strong> Store source for each fact; allow querying by source
<li><strong>Temporal versioning:</strong> Keep history of fact updates; don't delete old information
<li><strong>Consensus mechanism:</strong> If 3+ reliable sources agree, mark as high-confidence
</ul>

Example:
<pre><code>
Fact 1: (Company A, acquired, Company B, 2024-03-15, source=SEC, conf=0.95)
Fact 2: (Company A, acquired, Company B, 2024-03-20, source=News, conf=0.70)
Resolution: Use SEC date (higher confidence); note discrepancy in metadata
</code></pre>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 5: Entity Alignment Between KGs</strong>

<p>\itshape Experimental Setup:</p>

<p>Two knowledge graphs:
<ul>
<li>$KG_1$: DBpedia (English Wikipedia entities)
<li>$KG_2$: Wikidata (multilingual entities)
<li>Overlap: 10,000 entities with known alignments (ground truth)
<li>Seed set: 1,000 known alignments for training
<li>Test set: 9,000 alignments for evaluation
</ul>

<p>\itshape Method:</p>

<p>1. <strong>Embed both KGs:</strong> Train TransE on each KG independently
2. <strong>Alignment learning:</strong> Minimize distance between seed alignments:
<div class="equation">
$$\begin{align}
\mathcal{L} = \sum_{(e_1, e_2) \in \text{seeds}} ||\mathbf{h}_{e_1} - \mathbf{h}_{e_2}||^2
\end{align}$$
</div>
3. <strong>Inference:</strong> For each entity in $KG_1$, find nearest neighbor in $KG_2$
4. <strong>Threshold:</strong> Only align if similarity > threshold $\tau$</p>

<p>\itshape Results (varying threshold):</p>

<table>
<tr><th>Threshold $\tau$</th><th>Precision</th><th>Recall</th><th>F1</th></tr>
<tr><td>0.5</td><td>0.62</td><td>0.88</td><td>0.73</td></tr>
<tr><td>0.7</td><td>0.78</td><td>0.71</td><td>0.74</td></tr>
<tr><td>0.9</td><td>0.91</td><td>0.52</td><td>0.66</td></tr>
</table>

<p>\itshape Analysis:
<ul>
<li>Lower threshold: High recall (find most matches) but lower precision (more false positives)
<li>Higher threshold: High precision (confident matches) but lower recall (miss some true matches)
<li>Optimal F1 at $\tau = 0.7$
</ul>

<p>\itshape Seed Set Size Impact:</p>

<table>
<tr><th>Seed Set Size</th><th>F1 Score</th></tr>
<tr><td>100</td><td>0.58</td></tr>
<tr><td>500</td><td>0.68</td></tr>
<tr><td>1,000</td><td>0.74</td></tr>
<tr><td>5,000</td><td>0.81</td></tr>
</table>

<p>\itshape Conclusion:
Performance improves with more seed alignments. Diminishing returns after 1,000 seeds. For production, use iterative bootstrapping: start with high-confidence seeds, add confident predictions, retrain.
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 6: R-GCN vs. TransE for Node Classification</strong>

<p>\itshape Task: Predict entity types (Person, Organization, Location) given graph structure.</p>

<p>\itshape Dataset:
<ul>
<li>10,000 entities, 50,000 relationships
<li>10 relation types (works\_for, located\_in, founded, etc.)
<li>3 entity types to predict
<li>Train/test split: 70/30
</ul>

<p>\itshape Models:</p>

<p>1. <strong>TransE baseline:</strong>
   - Train TransE embeddings
   - Add classification head: Linear(embedding\_dim, num\_types)
   - Train classifier on entity embeddings</p>

<p>2. <strong>R-GCN:</strong>
   - 2-layer R-GCN with relation-specific weights
   - Classification head on final layer embeddings
   - End-to-end training</p>

<p>\itshape Results:</p>

<table>
<tr><th>Model</th><th>Accuracy</th><th>Macro F1</th><th>Training Time</th></tr>
<tr><td>TransE + Classifier</td><td>0.78</td><td>0.75</td><td>10 min</td></tr>
<tr><td>R-GCN (2 layers)</td><td>0.86</td><td>0.84</td><td>25 min</td></tr>
<tr><td>R-GCN (3 layers)</td><td>0.88</td><td>0.86</td><td>45 min</td></tr>
</table>

<p>\itshape When R-GCN Outperforms:</p>

<p>1. <strong>Rich local structure:</strong> Entity types strongly correlated with neighbor types
   - Example: Entities with many ``works\_for'' edges are likely Persons
   - R-GCN aggregates neighborhood information effectively</p>

<p>2. <strong>Multi-hop patterns:</strong> 3-layer R-GCN captures 3-hop neighborhoods
   - Example: Person ‚Üí works\_for ‚Üí Organization ‚Üí located\_in ‚Üí Location
   - Helps disambiguate entity types through indirect relationships</p>

<p>3. <strong>Relation-specific signals:</strong> Different relations provide different type information
   - ``founded'' relation: head is likely Person, tail is likely Organization
   - R-GCN learns relation-specific importance</p>

<p>\itshape When TransE is Sufficient:</p>

<p>1. <strong>Simple patterns:</strong> Entity types predictable from direct attributes
2. <strong>Computational constraints:</strong> TransE is 2--4x faster
3. <strong>Large-scale graphs:</strong> R-GCN memory requirements grow with neighborhood size</p>

<p>\itshape Recommendation:
Use R-GCN when graph structure is informative and computational budget allows. For billion-scale graphs, TransE or hybrid approaches (R-GCN on subgraphs) are more practical.
</div>
        
        <div class="chapter-nav">
  <a href="chapter27_video_visual.html">‚Üê Chapter 27: Video and Visual Understanding</a>
  <a href="../../deeptech.html">üìö Table of Contents</a>
  <a href="chapter29_recommendations.html">Chapter 29: Recommendation Systems ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
