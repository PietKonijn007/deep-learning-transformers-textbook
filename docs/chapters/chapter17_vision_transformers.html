<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 17: Vision Transformers - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../css/style.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Vision Transformers</h1>

<h2>Chapter Overview</h2>

<p>Vision Transformers (ViT) apply transformer architecture to computer vision, replacing convolutional neural networks. This chapter covers patch embeddings, position encodings for 2D images, ViT architecture variants, and hybrid CNN-transformer models.</p>

<h3>Learning Objectives</h3>

<ol>
    <li>Understand how to apply transformers to images
    <li>Implement patch embedding and position encoding
    <li>Compare ViT to CNNs (ResNet, EfficientNet)
    <li>Apply data augmentation and regularization for ViT
    <li>Understand ViT variants (DeiT, Swin, CoAtNet)
    <li>Implement masked autoencoding (MAE) for vision
</ol>

<h2>From Images to Sequences</h2>

<h3>The Patch Embedding Approach</h3>

<p><strong>Challenge:</strong> Image is 2D array, transformer expects 1D sequence.</p>

<p><strong>Solution:</strong> Divide image into patches, flatten each patch.</p>

<div class="definition"><strong>Definition:</strong> 
For image $\mI \in \R^{H \times W \times C}$ with patch size $P$:

<p><strong>Step 1:</strong> Divide into $N = HW/P^2$ patches
<div class="equation">
$$
\mI_{\text{patches}} \in \R^{N \times (P^2 \cdot C)}
$$
</div>

<p><strong>Step 2:</strong> Linear projection
<div class="equation">
$$
\mX = \mI_{\text{patches}} \mW_{\text{patch}} + \vb \quad \text{where } \mW_{\text{patch}} \in \R^{(P^2C) \times d}
$$
</div>

<p><strong>Step 3:</strong> Add position embeddings
<div class="equation">
$$
\mX = \mX + \mE_{\text{pos}}
$$
</div>
</div>

<div class="example"><strong>Example:</strong> 
Image: $224 \times 224 \times 3$ (ImageNet standard)

<p>Patch size: $P = 16$</p>

<figure>
<div class="tikz-diagram"><img src="diagrams/chapter17_vision_transformers_515c6cbf259c.svg" alt="TikZ Diagram" /></div>
<figcaption>Vision Transformer patch embedding process. A $224 \times 224$ image is divided into $16 \times 16$ patches (196 total), each patch is flattened and linearly projected to embedding dimension $d$, a CLS token is prepended, position embeddings are added, and the sequence is processed by a standard transformer encoder.</figcaption>
</figure>

<p><strong>Number of patches:</strong>
<div class="equation">
$$
N = \frac{224 \times 224}{16^2} = \frac{50176}{256} = 196 \text{ patches}
$$
</div>

<p><strong>Each patch:</strong> $16 \times 16 \times 3 = 768$ values</p>

<p><strong>Linear projection to </strong> $d = 768$:
<div class="equation">
$$
\mW_{\text{patch}} \in \R^{768 \times 768}
$$
</div>

<p><strong>Sequence length:</strong> 196 tokens (much shorter than full image 50,176 pixels!)</p>

<p><strong>With [CLS] token:</strong> 197 total sequence length
</div>

<h3>Position Encodings for 2D</h3>

<p><strong>Option 1: 1D Position Embeddings</strong>
<div class="equation">
$$
\mE_{\text{pos}} \in \R^{N \times d}
$$
</div>
Learned absolute positions, treats as 1D sequence.</p>

<p><strong>Option 2: 2D Position Embeddings</strong>
<div class="equation">
$$
\mE_{\text{pos}}(i,j) = \mE_{\text{row}}(i) + \mE_{\text{col}}(j)
$$
</div>
Separate embeddings for row and column.</p>

<p><strong>Original ViT uses 1D:</strong> Simpler, works well in practice!</p>

<h2>Vision Transformer (ViT) Architecture</h2>

<h3>Complete ViT Model</h3>

<div class="definition"><strong>Definition:</strong> 
<strong>Input:</strong> Image $\mI \in \R^{H \times W \times C}$

<p><strong>Step 1:</strong> Patch embedding
<div class="equation">
$$
\vx_{\text{patches}} = \text{PatchEmbed}(\mI) \in \R^{N \times d}
$$
</div>

<p><strong>Step 2:</strong> Add [CLS] token
<div class="equation">
$$
\vx_0 = [\vx_{\text{cls}}, \vx_{\text{patches}}] \in \R^{(N+1) \times d}
$$
</div>

<p><strong>Step 3:</strong> Add position embeddings
<div class="equation">
$$
\vx_0 = \vx_0 + \mE_{\text{pos}}
$$
</div>

<p><strong>Step 4:</strong> Transformer encoder (L layers)
<div class="equation">
$$
\vx_L = \text{Transformer}(\vx_0)
$$
</div>

<p><strong>Step 5:</strong> Classification head on [CLS]
<div class="equation">
$$
y = \text{softmax}(\mW_{\text{head}} \vx_L^{\text{cls}} + \vb)
$$
</div>
</div>

<h3>ViT Model Variants</h3>

<p>The Vision Transformer comes in three standard configurations that scale from moderate to extremely large models. ViT-Base uses 12 layers with hidden dimension $d = 768$ and 12 attention heads, resulting in 86 million parameters. This configuration is comparable in size to BERT-base and serves as the standard baseline for vision transformer research. The patch size is typically set to $P = 16$ for ImageNet-resolution images, producing 196 patches from a $224 \times 224$ input.</p>

<p>ViT-Large scales up to 24 layers with $d = 1024$ and 16 attention heads, totaling 307 million parameters. This represents a roughly 3.5√ó increase in parameters compared to ViT-Base, with the additional capacity enabling stronger performance when sufficient training data is available. The larger hidden dimension increases both the expressiveness of each layer and the computational cost per token.</p>

<p>ViT-Huge pushes the architecture to 32 layers with $d = 1280$ and 16 heads, reaching 632 million parameters. This massive model requires enormous datasets like JFT-300M for effective training and demonstrates the scalability of the transformer architecture to vision tasks. However, the computational and memory requirements make ViT-Huge impractical for many applications, with inference on a single image requiring several gigabytes of GPU memory and hundreds of milliseconds even on modern accelerators.</p>

<div class="example"><strong>Example:</strong> 
Configuration: $L=12$, $d=768$, $h=12$, $P=16$, ImageNet ($N=196$)

<p><strong>Patch embedding:</strong>
<div class="equation">
$$
768 \times 768 = 589{,}824
$$
</div>

<p><strong>Position embeddings:</strong>
<div class="equation">
$$
197 \times 768 = 151{,}296
$$
</div>

<p><strong>Transformer encoder (12 layers):</strong>
<div class="equation">
$$
12 \times 7{,}084{,}800 = 85{,}017{,}600
$$
</div>

<p><strong>Classification head (ImageNet, 1000 classes):</strong>
<div class="equation">
$$
768 \times 1000 = 768{,}000
$$
</div>

<p><strong>Total:</strong> $\approx 86{,}527{,}000 \approx$ <strong>86M parameters</strong>
</div>

<h3>Memory Requirements and Computational Analysis</h3>

<p>The memory footprint of Vision Transformers scales with both the model size and the input image resolution. For ViT-Base with 86 million parameters, storing the model weights in FP32 requires $86 \times 10^6 \times 4 = 344$ MB. During training, we must also store optimizer states (momentum and variance for Adam), which doubles this to approximately 1 GB for the model alone. Additionally, activations must be stored for backpropagation, and their memory consumption depends critically on the sequence length.</p>

<p>For a standard $224 \times 224$ image with patch size 16, the sequence length is 196 tokens (plus one CLS token for 197 total). The activation memory for a single layer includes the attention scores matrix of size $h \times n \times n$ where $h = 12$ heads and $n = 197$, requiring $12 \times 197^2 \times 4 = 1.86$ MB in FP32. Across 12 layers with batch size 32, attention matrices alone consume approximately 714 MB. The feed-forward network activations add another $32 \times 197 \times 768 \times 4 \times 12 = 2.3$ GB for intermediate representations. In total, training ViT-Base with batch size 32 on $224 \times 224$ images requires approximately 8-10 GB of GPU memory, comfortably fitting on modern GPUs like the NVIDIA RTX 3090 or A100.</p>

<p>However, increasing the image resolution dramatically impacts memory requirements due to the quadratic scaling of attention. For $384 \times 384$ images with the same patch size of 16, the number of patches increases to $(384/16)^2 = 576$ tokens. The attention matrices now require $12 \times 577^2 \times 4 = 16.0$ MB per layer, or 6.1 GB across 12 layers with batch size 32. This represents an 8.5√ó increase in attention memory compared to $224 \times 224$ resolution. The total memory requirement grows to approximately 18-22 GB, necessitating high-end GPUs or gradient checkpointing techniques to fit in memory.</p>

<div class="example"><strong>Example:</strong> 
Compare memory and computation for different resolutions with ViT-Base ($L=12$, $d=768$, $h=12$, $P=16$):

<p><strong>Resolution $224 \times 224$:</strong>
<div class="equation">
$$
n = \frac{224^2}{16^2} = 196 \text{ patches}
$$
</div>
Attention memory per layer: $12 \times 197^2 \times 4 = 1.86$ MB</p>

<p>FLOPs per attention layer: $4n^2d = 4 \times 197^2 \times 768 = 119$ MFLOPs</p>

<p><strong>Resolution $384 \times 384$:</strong>
<div class="equation">
$$
n = \frac{384^2}{16^2} = 576 \text{ patches}
$$
</div>
Attention memory per layer: $12 \times 577^2 \times 4 = 16.0$ MB (8.6√ó increase)</p>

<p>FLOPs per attention layer: $4 \times 577^2 \times 768 = 1.03$ GFLOPs (8.6√ó increase)</p>

<p><strong>Key insight:</strong> Memory and computation scale quadratically with image resolution when patch size is fixed. Doubling resolution increases cost by approximately 4√ó.
</div>

<p>The patch size provides another lever for controlling computational cost. Using larger patches reduces the sequence length, thereby decreasing both memory and computation. For a $224 \times 224$ image, patch size $P = 32$ produces only $(224/32)^2 = 49$ patches compared to 196 for $P = 16$. This 4√ó reduction in sequence length translates to a 16√ó reduction in attention memory and computation due to the quadratic scaling. However, larger patches also reduce the model's ability to capture fine-grained visual details, creating a fundamental trade-off between efficiency and representational capacity.</p>

<div class="example"><strong>Example:</strong> 
For $224 \times 224$ images with ViT-Base:

<p><strong>Patch size $P = 16$:</strong>
<div class="equation">
$$
n = 196, \quad \text{Attention FLOPs} = 119 \text{ MFLOPs per layer}
$$
</div>

<p><strong>Patch size $P = 32$:</strong>
<div class="equation">
$$
n = 49, \quad \text{Attention FLOPs} = 7.4 \text{ MFLOPs per layer}
$$
</div>

<p>The 16√ó reduction in attention cost makes $P = 32$ attractive for efficiency, but the coarser granularity typically reduces accuracy by 2-3\% on ImageNet. The optimal patch size depends on the application: real-time systems may prefer $P = 32$, while accuracy-critical applications use $P = 16$ or even $P = 14$ for ViT-Huge.
</div>

<h2>Training Vision Transformers</h2>

<h3>Pre-training Strategies</h3>

<p><strong>Supervised Pre-training (Original ViT):</strong>
<ul>
    <li>Large datasets: JFT-300M (300M images, 18K classes)
    <li>Standard classification loss
    <li>Then fine-tune on ImageNet
</ul>

<p><strong>Key finding:</strong> ViT requires massive data to outperform CNNs!
<ul>
    <li>On ImageNet alone: ResNet > ViT
    <li>Pre-trained on JFT-300M: ViT > ResNet
</ul>

<h3>Data Augmentation and Regularization</h3>

<p><strong>Essential for ViT (lacks CNN inductive biases):</strong></p>

<p><strong>Augmentation:</strong>
<ul>
    <li>RandAugment: Random augmentation policies
    <li>Mixup: $\tilde{x} = \lambda x_i + (1-\lambda) x_j$
    <li>CutMix: Cut and paste patches between images
    <li>Random erasing
</ul>

<p><strong>Regularization:</strong>
<ul>
    <li>Dropout: 0.1
    <li>Stochastic depth: Drop entire layers randomly
    <li>Weight decay: $10^{-4}$ to $10^{-2}$
</ul>

<h3>DeiT: Data-efficient Image Transformers</h3>

<p>Improvements for training without massive datasets:</p>

<p><strong>1. Knowledge Distillation</strong>
<ul>
    <li>Teacher: CNN (RegNetY) or ViT
    <li>Student: ViT
    <li>Distillation token alongside [CLS]
</ul>

<p><strong>2. Strong Augmentation</strong>
<ul>
    <li>Aggressive RandAugment
    <li>Repeated augmentation
</ul>

<p><strong>Result:</strong> DeiT-Base achieves 81.8\% on ImageNet trained only on ImageNet (1.3M images)!</p>

<h2>Masked Autoencoders (MAE)</h2>

<h3>Self-Supervised Pre-training for Vision</h3>

<div class="definition"><strong>Definition:</strong> 
BERT-style masking for images:

<p><strong>Step 1:</strong> Randomly mask 75\% of patches</p>

<p><strong>Step 2:</strong> Encoder processes only visible patches</p>

<p><strong>Step 3:</strong> Decoder reconstructs all patches (including masked)</p>

<p><strong>Loss:</strong> Pixel-level MSE on masked patches
<div class="equation">
$$
\mathcal{L} = \frac{1}{|\mathcal{M}|} \sum_{i \in \mathcal{M}} \|\hat{\vx}_i - \vx_i\|^2
$$
</div>
</div>

<div class="example"><strong>Example:</strong> 
<strong>Image:</strong> $224 \times 224$, patches $16 \times 16$ ($N=196$)

<p><strong>Masking:</strong> Keep 25\% = 49 patches, mask 147 patches</p>

<p><strong>Encoder:</strong>
<ul>
    <li>Input: 49 visible patches only
    <li>Architecture: ViT-Large (24 layers, $d=1024$)
    <li>Much faster (process 1/4 of patches)
</ul>

<p><strong>Decoder:</strong>
<ul>
    <li>Input: Encoder output + mask tokens
    <li>Architecture: Smaller (8 layers, $d=512$)
    <li>Reconstruct all 196 patches
</ul>

<p><strong>Benefits:</strong>
<ul>
    <li>Self-supervised (no labels needed)
    <li>Learns strong representations
    <li>Fine-tune on ImageNet: 87.8\% accuracy
</ul>
</div>

<h2>Hierarchical Vision Transformers</h2>

<h3>Motivation for Hierarchical Architectures</h3>

<p>The original Vision Transformer processes images at a single scale, dividing the input into fixed-size patches and maintaining the same spatial resolution throughout all layers. While this uniform approach simplifies the architecture, it has significant limitations for computer vision tasks. Many vision problems benefit from multi-scale representations: low-level features like edges and textures are best captured at high resolution with small receptive fields, while high-level semantic concepts require large receptive fields that aggregate information across the entire image. CNNs naturally provide this hierarchical structure through pooling layers that progressively reduce spatial resolution while increasing channel capacity.</p>

<p>Additionally, the quadratic complexity of self-attention with respect to sequence length makes standard ViT impractical for high-resolution images or dense prediction tasks like object detection and semantic segmentation. For a $512 \times 512$ image with patch size 16, the sequence length reaches 1,024 tokens, requiring attention matrices of size $1024 \times 1024$ per head. With 12 heads across 12 layers, this consumes over 600 MB just for attention weights in a single forward pass. The computational cost of $O(n^2d)$ attention becomes prohibitive, limiting ViT's applicability to tasks requiring fine-grained spatial reasoning.</p>

<p>Hierarchical Vision Transformers address these limitations by introducing multi-scale processing and localized attention mechanisms. These architectures progressively reduce spatial resolution while increasing feature dimensions, mimicking the pyramid structure of CNNs while retaining the flexibility of transformer layers. By restricting attention to local windows rather than the full image, they achieve linear or near-linear complexity in the number of pixels, enabling efficient processing of high-resolution inputs.</p>

<h3>Swin Transformer</h3>

<p>The Swin Transformer (Shifted Window Transformer) introduces a hierarchical architecture with shifted window-based attention that achieves linear complexity while maintaining the ability to model long-range dependencies. The architecture consists of four stages, each operating at a different spatial resolution. The first stage processes the image at high resolution with small patches (typically $4 \times 4$), producing a large number of tokens. Subsequent stages merge adjacent patches to reduce the spatial dimensions by 2√ó while doubling the feature dimension, creating a pyramid structure similar to ResNet.</p>

<div class="definition"><strong>Definition:</strong> 
For input image $\mI \in \R^{H \times W \times 3}$:

<p><strong>Stage 1:</strong> Patch size $4 \times 4$, dimension $C$
<div class="equation">
$$
\text{Resolution: } \frac{H}{4} \times \frac{W}{4}, \quad \text{Channels: } C
$$
</div>

<p><strong>Stage 2:</strong> Patch merging, dimension $2C$
<div class="equation">
$$
\text{Resolution: } \frac{H}{8} \times \frac{W}{8}, \quad \text{Channels: } 2C
$$
</div>

<p><strong>Stage 3:</strong> Patch merging, dimension $4C$
<div class="equation">
$$
\text{Resolution: } \frac{H}{16} \times \frac{W}{16}, \quad \text{Channels: } 4C
$$
</div>

<p><strong>Stage 4:</strong> Patch merging, dimension $8C$
<div class="equation">
$$
\text{Resolution: } \frac{H}{32} \times \frac{W}{32}, \quad \text{Channels: } 8C
$$
</div>

<p>For Swin-Base: $C = 128$, producing feature maps at resolutions $\frac{H}{4}, \frac{H}{8}, \frac{H}{16}, \frac{H}{32}$ with dimensions 128, 256, 512, 1024 respectively.
</div>

<p>The key innovation of Swin Transformer is shifted window attention, which restricts self-attention to non-overlapping local windows while enabling cross-window connections through window shifting. In even-numbered layers, the image is partitioned into regular $M \times M$ windows (typically $M = 7$), and attention is computed independently within each window. In odd-numbered layers, the windows are shifted by $\lfloor M/2 \rfloor$ pixels in both horizontal and vertical directions, causing the windows to overlap with different regions than in the previous layer. This shifting mechanism allows information to flow between windows while maintaining the computational efficiency of local attention.</p>

<p>The computational complexity of window-based attention is $O(M^2 \cdot HW)$ where $M$ is the window size and $HW$ is the image resolution. For $M = 7$ and a $224 \times 224$ image at stage 1 resolution ($56 \times 56$ tokens), each window contains $7 \times 7 = 49$ tokens. The attention computation within a window requires $49^2 = 2,401$ operations per head, compared to $3,136^2 = 9.8$ million operations for global attention over all $56 \times 56$ tokens. This 4,000√ó reduction in attention complexity enables Swin Transformer to process high-resolution images efficiently while still capturing long-range dependencies through the hierarchical structure and window shifting.</p>

<div class="example"><strong>Example:</strong> 
Compare attention complexity for $224 \times 224$ image at stage 1 ($56 \times 56$ tokens):

<p><strong>Global attention (standard ViT):</strong>
<div class="equation">
$$
\text{Complexity: } O(n^2d) = O(3136^2 \times 128) = 1.26 \text{ GFLOPs per layer}
$$
</div>

<p><strong>Window attention (Swin, $M=7$):</strong>
<div class="equation">
$$
\text{Windows: } \frac{56}{7} \times \frac{56}{7} = 64 \text{ windows}
$$
</div>
<div class="equation">
$$
\text{Complexity: } O(M^2 \cdot HW \cdot d) = O(49 \times 3136 \times 128) = 19.7 \text{ MFLOPs per layer}
$$
</div>

<p>The window-based approach reduces attention cost by 64√ó, making high-resolution processing practical. The shifted window mechanism ensures that information still propagates globally through the network depth.
</div>

<p>Swin Transformer achieves state-of-the-art performance across multiple vision tasks while maintaining computational efficiency. On ImageNet classification, Swin-Base reaches 83.5\% top-1 accuracy with 88 million parameters and 15.4 GFLOPs‚Äîcomparable to ViT-Base in parameters but with better accuracy due to the hierarchical structure. For object detection on COCO, Swin-Base achieves 51.9 box AP, surpassing previous transformer-based detectors by significant margins. The multi-scale feature maps produced by the hierarchical architecture are particularly well-suited for dense prediction tasks, making Swin Transformer a versatile backbone for various computer vision applications.</p>

<h3>Pyramid Vision Transformer (PVT)</h3>

<p>Pyramid Vision Transformer takes a different approach to hierarchical vision transformers by introducing spatial-reduction attention that progressively decreases the key and value sequence lengths. Unlike Swin's window-based attention, PVT maintains global attention but reduces computational cost by downsampling the keys and values before computing attention. This design preserves the ability to attend to the entire image while achieving sub-quadratic complexity.</p>

<p>In PVT, each stage reduces the spatial resolution through patch merging, similar to Swin Transformer. However, within each stage, the attention mechanism uses a spatial reduction operation on keys and values. For a reduction ratio $R$, the keys and values are reshaped and downsampled by $R \times R$, reducing their sequence length by a factor of $R^2$. The queries maintain the original resolution, allowing each token to attend to a downsampled representation of the entire image. This approach reduces attention complexity from $O(n^2d)$ to $O(n^2d/R^2)$, providing a tunable trade-off between computational cost and attention granularity.</p>

<p>The hierarchical structure of PVT produces feature maps at multiple scales, making it suitable as a backbone for dense prediction tasks. PVT-Medium with 44 million parameters achieves 82.0\% ImageNet accuracy while requiring only 6.7 GFLOPs‚Äîsignificantly more efficient than ViT-Base. For object detection, PVT-based detectors achieve competitive performance with CNN-based methods while offering the benefits of transformer architectures, including better transfer learning and attention-based interpretability.</p>

<h3>Hybrid Architectures: CoAtNet</h3>

<p>Hybrid architectures combine convolutional layers and transformer layers to leverage the complementary strengths of both approaches. Convolutional layers provide efficient local feature extraction with built-in translation equivariance, while transformer layers enable global reasoning and flexible attention patterns. CoAtNet (Convolution and Attention Network) systematically explores this design space, identifying an optimal combination that achieves state-of-the-art performance with improved efficiency.</p>

<p>The CoAtNet architecture consists of five stages with progressively decreasing spatial resolution. The first two stages use convolutional blocks based on the MBConv (Mobile Inverted Bottleneck Convolution) design from EfficientNet, which efficiently extracts local features at high resolution. These convolutional stages capture low-level visual patterns like edges, textures, and simple shapes with strong inductive bias and minimal computational cost. The spatial resolution is reduced by 2√ó at each stage through strided convolutions.</p>

<p>The final three stages employ transformer blocks with relative attention, enabling global reasoning over the extracted features. By this point in the network, the spatial resolution has been reduced by 8√ó or more, making global attention computationally feasible. The transformer stages learn high-level semantic representations and long-range dependencies that benefit from the flexibility of self-attention. The final stage uses attention pooling to aggregate spatial information into a global representation for classification.</p>

<div class="example"><strong>Example:</strong> 
CoAtNet-3 configuration for $224 \times 224$ input:

<p><strong>Stage 0 (Stem):</strong> Convolution, $112 \times 112$ resolution, 64 channels</p>

<p><strong>Stage 1:</strong> MBConv blocks, $112 \times 112$ resolution, 96 channels</p>

<p><strong>Stage 2:</strong> MBConv blocks, $56 \times 56$ resolution, 192 channels</p>

<p><strong>Stage 3:</strong> Transformer blocks, $28 \times 28$ resolution, 384 channels</p>

<p><strong>Stage 4:</strong> Transformer blocks, $14 \times 14$ resolution, 768 channels</p>

<p><strong>Stage 5:</strong> Attention pooling, global representation</p>

<p>Total parameters: 168M, FLOPs: 34.7G</p>

<p>This hybrid design achieves 87.9\% ImageNet accuracy, outperforming pure CNN and pure transformer architectures of similar size.
</div>

<p>The success of CoAtNet demonstrates that the choice between convolution and attention need not be binary. By using convolutions where they excel (local feature extraction at high resolution) and transformers where they excel (global reasoning at lower resolution), hybrid architectures achieve better accuracy-efficiency trade-offs than either approach alone. CoAtNet-7, the largest variant with 2.4 billion parameters, achieved 90.88\% ImageNet accuracy and state-of-the-art results on multiple vision benchmarks at the time of its release, validating the hybrid approach at scale.</p>

<h2>ViT vs CNN Comparison</h2>

<h3>Parameter Efficiency</h3>

<p>Vision Transformers and Convolutional Neural Networks differ fundamentally in their parameter efficiency and data requirements. ResNet-50, a standard CNN baseline, contains approximately 25 million parameters distributed across convolutional layers with small kernel sizes (typically $3 \times 3$ or $7 \times 7$). In contrast, ViT-Base requires 86 million parameters‚Äîmore than 3√ó the size of ResNet-50‚Äîto achieve comparable performance. This parameter gap reflects the different inductive biases: CNNs build in locality and translation equivariance through their convolutional structure, while transformers must learn these properties from data through their flexible attention mechanism.</p>

<p>The parameter distribution also differs significantly between the architectures. In ResNet-50, the majority of parameters reside in the later convolutional layers and the final fully-connected layer. For ViT-Base, the parameters are more evenly distributed across the 12 transformer layers, with each layer containing approximately 7 million parameters in the attention and feed-forward components. The patch embedding layer contributes only 590K parameters, while position embeddings add another 151K‚Äîboth negligible compared to the transformer layers themselves.</p>

<p>Despite having more parameters, ViT-Base is not necessarily slower than ResNet-50 for inference. The transformer's matrix multiplications are highly optimized on modern GPUs, and the lack of spatial convolutions can actually improve throughput. On an NVIDIA A100 GPU, ViT-Base processes approximately 1,200 images per second at $224 \times 224$ resolution with batch size 128, compared to 1,400 images per second for ResNet-50. The 15\% throughput difference is much smaller than the 3√ó parameter gap would suggest, demonstrating the efficiency of transformer operations on modern hardware.</p>

<h3>Computational Complexity Analysis</h3>

<p>The computational complexity of Vision Transformers scales differently than CNNs, leading to different performance characteristics across image resolutions. For a CNN like ResNet-50, the computational cost is approximately $O(C \times k^2 \times H \times W)$ where $C$ is the number of channels, $k$ is the kernel size, and $H \times W$ is the spatial resolution. This linear scaling in spatial dimensions means that doubling the image resolution increases computation by 4√ó. For ResNet-50 processing a $224 \times 224$ image, the total computation is approximately 4.1 GFLOPs.</p>

<p>Vision Transformers have complexity $O(n^2d + nd^2)$ where $n = (H/P)^2$ is the number of patches and $d$ is the hidden dimension. The $n^2d$ term comes from attention, while $nd^2$ comes from the feed-forward network. For ViT-Base with $224 \times 224$ images and patch size 16, we have $n = 196$ and $d = 768$. The attention computation across 12 layers totals $12 \times 4 \times 196^2 \times 768 = 1.4$ GFLOPs, while the feed-forward network contributes $12 \times 2 \times 196 \times 768^2 = 2.8$ GFLOPs, for a total of approximately 4.2 GFLOPs‚Äînearly identical to ResNet-50.</p>

<p>However, the scaling behavior differs dramatically. When we increase resolution to $384 \times 384$ with the same patch size, the number of patches grows to $n = 576$, increasing by a factor of $(384/224)^2 = 2.94$. The attention cost grows quadratically to $12 \times 4 \times 576^2 \times 768 = 12.3$ GFLOPs (8.6√ó increase), while the feed-forward cost grows linearly to $12 \times 2 \times 576 \times 768^2 = 8.1$ GFLOPs (2.9√ó increase). The total ViT computation reaches 20.4 GFLOPs, compared to 12.0 GFLOPs for ResNet-50 at the same resolution. This crossover point illustrates why efficient attention mechanisms become critical for high-resolution vision tasks.</p>

<div class="example"><strong>Example:</strong> 
Compare FLOPs for ResNet-50 and ViT-Base across resolutions:

<div style="text-align: center;">

<table>
<tr><th><strong>Resolution</strong></th><th><strong>ResNet-50</strong></th><th><strong>ViT-Base</strong></th></tr>
<tr><td>$224 \times 224$</td><td>4.1 GFLOPs</td><td>4.2 GFLOPs</td></tr>
<tr><td>$384 \times 384$</td><td>12.0 GFLOPs</td><td>20.4 GFLOPs</td></tr>
<tr><td>$512 \times 512$</td><td>21.3 GFLOPs</td><td>48.7 GFLOPs</td></tr>
</table>

<p></div>

<p>At standard ImageNet resolution, ViT and ResNet have similar computational cost. However, ViT's quadratic attention scaling makes it increasingly expensive at higher resolutions, motivating hierarchical architectures like Swin Transformer that reduce attention to local windows.
</div>

<h3>Data Requirements and Inductive Bias</h3>

<p>The most striking difference between Vision Transformers and CNNs lies in their data requirements, which stem from their different inductive biases. CNNs encode strong priors about images: locality (nearby pixels are related), translation equivariance (a cat is a cat regardless of position), and hierarchical structure (edges ‚Üí textures ‚Üí objects). These built-in assumptions allow CNNs to learn effectively from moderate-sized datasets like ImageNet with 1.3 million images. ResNet-50 trained only on ImageNet achieves 76.5\% top-1 accuracy, demonstrating that the convolutional structure provides useful inductive bias for natural images.</p>

<p>Vision Transformers, by contrast, have minimal inductive bias. The self-attention mechanism can attend to any patch regardless of spatial distance, and the model must learn locality and translation properties from data. When trained only on ImageNet, ViT-Base achieves only 72.3\% accuracy‚Äî4.2 percentage points below ResNet-50 despite having 3√ó more parameters. This performance gap reveals that the flexibility of attention becomes a liability when training data is limited: the model has too much capacity and insufficient constraints to learn good representations.</p>

<p>The situation reverses dramatically with large-scale pre-training. When ViT-Base is pre-trained on JFT-300M (300 million images with 18,000 classes) and then fine-tuned on ImageNet, it achieves 84.2\% accuracy, surpassing ResNet-50's 76.5\% by a substantial margin. The massive pre-training dataset provides enough examples for the transformer to learn the visual priors that CNNs encode by design. Moreover, the learned representations transfer better to downstream tasks: ViT-Base pre-trained on JFT-300M achieves higher accuracy than ResNet-50 on 19 out of 20 transfer learning benchmarks, with improvements ranging from 2-7 percentage points.</p>

<p>This data-efficiency trade-off has important practical implications. For applications with limited training data or computational budgets, CNNs remain the better choice. For large-scale systems with access to massive datasets and compute, Vision Transformers offer superior performance and transfer learning capabilities. The development of data-efficient training methods like DeiT (Data-efficient Image Transformers) has partially bridged this gap, enabling ViT-Base to achieve 81.8\% on ImageNet without external data through aggressive augmentation and distillation techniques.</p>

<h3>When to Use Each Architecture</h3>

<table>
<tr><th><strong>Aspect</strong></th><th><strong>CNN (ResNet)</strong></th><th><strong>ViT</strong></th></tr>
<tr><td>Inductive bias</td><td>Strong (locality, translation)</td><td>Weak</td></tr>
<tr><td>Data requirement</td><td>Moderate (ImageNet)</td><td>Large (JFT-300M)</td></tr>
<tr><td>Parameters</td><td>25M (ResNet-50)</td><td>86M (ViT-Base)</td></tr>
<tr><td>Computation</td><td>$O(HW)$</td><td>$O((HW/P)^2)$</td></tr>
<tr><td>Memory</td><td>5-7 GB training</td><td>8-10 GB training</td></tr>
<tr><td>Interpretability</td><td>Filter visualization</td><td>Attention maps</td></tr>
<tr><td>Transfer</td><td>Good</td><td>Excellent (large-scale)</td></tr>
<tr><td>Best use</td><td>Small/medium data</td><td>Large-scale pre-training</td></tr>
</table>

<p>The choice between CNNs and Vision Transformers depends on the specific application constraints. CNNs are preferable when training data is limited (fewer than 10 million images), when computational efficiency is critical (mobile or edge deployment), or when strong spatial priors are known to be appropriate for the task. ResNet and EfficientNet variants remain the standard choice for many production computer vision systems due to their reliability and efficiency.</p>

<p>Vision Transformers excel when massive pre-training data is available, when transfer learning to diverse downstream tasks is important, or when state-of-the-art performance justifies the additional computational cost. The superior scaling properties of transformers‚Äîboth in terms of model size and dataset size‚Äîmake them the architecture of choice for foundation models in vision. Hybrid architectures like CoAtNet attempt to combine the strengths of both approaches, using convolutional layers for early feature extraction and transformer layers for high-level reasoning.</p>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> Implement patch embedding for image $224 \times 224 \times 3$ with patch size 16:
<ol>
    <li>Reshape image to patches
    <li>Apply linear projection
    <li>Add position embeddings
    <li>Verify output shape: $(196, 768)$
</ol>
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Compare ViT-Base and ResNet-50:
<ol>
    <li>Parameter count
    <li>FLOPs for $224 \times 224$ image
    <li>Memory footprint
    <li>Which is more efficient?
</ol>
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> Implement MAE masking:
<ol>
    <li>Randomly mask 75\% of 196 patches
    <li>Keep 49 visible patches
    <li>Add mask tokens for decoder
    <li>Compute reconstruction loss
</ol>
</div>

<div class="exercise" id="exercise-4"><strong>Exercise 4:</strong> Train ViT-Tiny on CIFAR-10:
<ol>
    <li>Use patch size 4 (for $32 \times 32$ images)
    <li>6 layers, $d=192$, 3 heads
    <li>Apply RandAugment
    <li>Compare to small ResNet
</ol>
</div>

<h2>Solutions</h2>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 1: Patch Embedding Implementation</strong>

<pre><code>import torch
import torch.nn as nn
import numpy as np

class PatchEmbedding(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.n_patches = (img_size // patch_size) ** 2
        
        # Linear projection of flattened patches
        self.proj = nn.Conv2d(
            in_channels, 
            embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )
        
        # Position embeddings
        self.pos_embed = nn.Parameter(
            torch.randn(1, self.n_patches + 1, embed_dim)
        )
        
        # CLS token
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
    
    def forward(self, x):
        # x: (B, C, H, W)
        B = x.shape[0]
        
        # Part 1: Reshape image to patches and project
        # Conv2d with stride=patch_size extracts non-overlapping patches
        x = self.proj(x)  # (B, embed_dim, H/P, W/P)
        x = x.flatten(2)  # (B, embed_dim, n_patches)
        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)
        
        # Part 2: Add CLS token
        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)
        x = torch.cat([cls_tokens, x], dim=1)  # (B, n_patches+1, embed_dim)
        
        # Part 3: Add position embeddings
        x = x + self.pos_embed
        
        return x

# Example usage
img_size = 224
patch_size = 16
in_channels = 3
embed_dim = 768

# Create model
patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)

# Create sample image
batch_size = 4
image = torch.randn(batch_size, in_channels, img_size, img_size)

# Forward pass
output = patch_embed(image)

print(f"Input image shape: {image.shape}")
print(f"Number of patches: {(img_size // patch_size) ** 2}")
print(f"Output shape: {output.shape}")
print(f"Expected: (batch_size, n_patches+1, embed_dim)")
print(f"Actual: ({batch_size}, {(img_size//patch_size)**2 + 1}, {embed_dim})")
</code></pre>

<p><strong>Detailed Breakdown:</strong></p>

<p><strong>Part (a): Reshape Image to Patches</strong></p>

<p>Original image: $224 \times 224 \times 3$</p>

<p>Patch size: $16 \times 16$</p>

<p>Number of patches: $\frac{224}{16} \times \frac{224}{16} = 14 \times 14 = 196$ patches</p>

<p>Each patch: $16 \times 16 \times 3 = 768$ values</p>

<p>Reshaped: $(196, 768)$</p>

<p><strong>Part (b): Linear Projection</strong></p>

<p>Using Conv2d with kernel\_size=16, stride=16:
<ul>
    <li>Input: $(B, 3, 224, 224)$
    <li>Output: $(B, 768, 14, 14)$
    <li>Flatten spatial dimensions: $(B, 768, 196)$
    <li>Transpose: $(B, 196, 768)$
</ul>

<p>This is equivalent to:
<ol>
    <li>Extract $14 \times 14 = 196$ non-overlapping patches
    <li>Flatten each patch: $16 \times 16 \times 3 = 768$ values
    <li>Apply linear projection: $\mathbb{R}^{768} \to \mathbb{R}^{768}$
</ol>

<p><strong>Part (c): Add Position Embeddings</strong></p>

<p>Position embeddings: learnable parameters of shape $(1, 197, 768)$
<ul>
    <li>197 = 196 patches + 1 CLS token
    <li>Each position has unique 768-dimensional embedding
    <li>Added element-wise to patch embeddings
</ul>

<p>$$\vx_i = \text{PatchEmbed}(\text{patch}_i) + \vpos_i$$</p>

<p><strong>Part (d): Output Shape Verification</strong></p>

<pre><code>
Input image shape: torch.Size([4, 3, 224, 224])
Number of patches: 196
Output shape: torch.Size([4, 197, 768])
Expected: (batch_size, n_patches+1, embed_dim)
Actual: (4, 197, 768)
</code></pre>

<p>Output shape: $(B, 197, 768)$ where:
<ul>
    <li>$B = 4$: batch size
    <li>$197 = 196 + 1$: patches + CLS token
    <li>$768$: embedding dimension
</ul>

<p><strong>Key Design Choices:</strong></p>

<ol>
    <li><strong>Conv2d for patching:</strong> More efficient than manual reshaping
    <li><strong>CLS token:</strong> Special token for classification (like BERT)
    <li><strong>Learnable position embeddings:</strong> Unlike sinusoidal in original Transformer
    <li><strong>No overlap:</strong> Patches don't overlap (stride = patch\_size)
</ol>

<p><strong>Alternative Implementation (Manual):</strong></p>

<pre><code>def manual_patch_embedding(image, patch_size=16):
    """Manual patch extraction without Conv2d"""
    B, C, H, W = image.shape
    P = patch_size
    
    # Reshape to patches
    patches = image.unfold(2, P, P).unfold(3, P, P)  # (B, C, H/P, W/P, P, P)
    patches = patches.contiguous().view(B, C, -1, P, P)  # (B, C, n_patches, P, P)
    patches = patches.permute(0, 2, 1, 3, 4)  # (B, n_patches, C, P, P)
    patches = patches.reshape(B, -1, C * P * P)  # (B, n_patches, C*P*P)
    
    return patches

# Verify equivalence
manual_patches = manual_patch_embedding(image, patch_size)
print(f"Manual patches shape: {manual_patches.shape}")  # (4, 196, 768)
</code></pre>

<p>Both methods produce identical results, but Conv2d is more efficient and commonly used in practice.
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 2: ViT-Base vs ResNet-50 Comparison</strong>

<p><strong>Part (a): Parameter Count</strong></p>

<p><strong>ViT-Base:</strong>
<ul>
    <li>Layers: $L = 12$
    <li>Hidden size: $d = 768$
    <li>Attention heads: $h = 12$
    <li>MLP size: $d_{mlp} = 3072$
    <li>Image size: $224 \times 224$
    <li>Patch size: $16 \times 16$
    <li>Number of patches: $196$
</ul>

<p><strong>Parameters:</strong>
<ul>
    <li>Patch embedding: $3 \times 16 \times 16 \times 768 = 589{,}824$
    <li>Position embeddings: $197 \times 768 = 151{,}296$
    <li>CLS token: $768$
    <li>Per transformer layer:
    <ul>
        <li>Attention: $4 \times 768^2 = 2{,}359{,}296$
        <li>MLP: $768 \times 3072 + 3072 \times 768 = 4{,}718{,}592$
        <li>Layer norm: $2 \times 2 \times 768 = 3{,}072$
        <li>Total per layer: $7{,}080{,}960$
    </ul>
    <li>12 layers: $12 \times 7{,}080{,}960 = 84{,}971{,}520$
    <li>Classification head: $768 \times 1000 = 768{,}000$
</ul>

<p><strong>Total ViT-Base: $86{,}481{,}408 \approx 86$M parameters</strong></p>

<p><strong>ResNet-50:</strong>
<ul>
    <li>Conv1: $7 \times 7 \times 3 \times 64 = 9{,}408$
    <li>Layer 1 (3 blocks): $\approx 215{,}808$
    <li>Layer 2 (4 blocks): $\approx 1{,}219{,}648$
    <li>Layer 3 (6 blocks): $\approx 7{,}098{,}880$
    <li>Layer 4 (3 blocks): $\approx 14{,}964{,}800$
    <li>FC layer: $2048 \times 1000 = 2{,}048{,}000$
</ul>

<p><strong>Total ResNet-50: $25{,}556{,}032 \approx 25.6$M parameters</strong></p>

<p><strong>Ratio: ViT-Base has $3.4\times$ more parameters than ResNet-50</strong></p>

<p><strong>Part (b): FLOPs for $224 \times 224$ Image</strong></p>

<p><strong>ViT-Base FLOPs:</strong></p>

<p><strong>1. Patch Embedding:</strong>
<ul>
    <li>Conv2d: $3 \times 16 \times 16 \times 768 \times 14 \times 14 = 115{,}605{,}504$ FLOPs
</ul>

<p><strong>2. Per Transformer Layer (12 layers):</strong></p>

<p><em>Multi-Head Attention:</em>
<ul>
    <li>$Q, K, V$ projections: $3 \times 197 \times 768 \times 768 = 347{,}054{,}592$ FLOPs
    <li>Attention scores: $197 \times 197 \times 768 = 29{,}859{,}712$ FLOPs
    <li>Attention weights $\times V$: $197 \times 197 \times 768 = 29{,}859{,}712$ FLOPs
    <li>Output projection: $197 \times 768 \times 768 = 115{,}684{,}864$ FLOPs
    <li><strong>Total attention: $522{,}458{,}880$ FLOPs</strong>
</ul>

<p><em>MLP:</em>
<ul>
    <li>First linear: $197 \times 768 \times 3072 = 464{,}739{,}456$ FLOPs
    <li>Second linear: $197 \times 3072 \times 768 = 464{,}739{,}456$ FLOPs
    <li><strong>Total MLP: $929{,}478{,}912$ FLOPs</strong>
</ul>

<p><strong>Total per layer: $1{,}451{,}937{,}792$ FLOPs</strong></p>

<p><strong>12 layers: $12 \times 1{,}451{,}937{,}792 = 17{,}423{,}253{,}504$ FLOPs</strong></p>

<p><strong>3. Classification Head:</strong>
<ul>
    <li>$768 \times 1000 = 768{,}000$ FLOPs
</ul>

<p><strong>Total ViT-Base: $\approx 17.5$ GFLOPs</strong></p>

<p><strong>ResNet-50 FLOPs:</strong></p>

<ul>
    <li>Conv1 + BN: $\approx 118$ MFLOPs
    <li>Layer 1: $\approx 1{,}219$ MFLOPs
    <li>Layer 2: $\approx 1{,}627$ MFLOPs
    <li>Layer 3: $\approx 3{,}254$ MFLOPs
    <li>Layer 4: $\approx 1{,}627$ MFLOPs
    <li>FC: $\approx 2$ MFLOPs
</ul>

<p><strong>Total ResNet-50: $\approx 4.1$ GFLOPs</strong></p>

<p><strong>Ratio: ViT-Base requires $4.3\times$ more FLOPs than ResNet-50</strong></p>

<p><strong>Part (c): Memory Footprint</strong></p>

<p><strong>ViT-Base Memory (Inference):</strong></p>

<p><strong>1. Activations per layer:</strong>
<ul>
    <li>Input: $197 \times 768 = 151{,}296$ values
    <li>Attention scores: $12 \times 197 \times 197 = 465{,}228$ values
    <li>MLP intermediate: $197 \times 3072 = 605{,}184$ values
</ul>

<p><strong>Peak activation memory per layer:</strong> $\approx 1.2$M values $\times$ 4 bytes = 4.8 MB</p>

<p><strong>Total for 12 layers:</strong> $\approx 58$ MB</p>

<p><strong>Parameters:</strong> $86$M $\times$ 4 bytes = 344 MB</p>

<p><strong>Total ViT-Base inference: $\approx 402$ MB</strong></p>

<p><strong>ResNet-50 Memory (Inference):</strong></p>

<p><strong>Peak activation memory:</strong>
<ul>
    <li>Layer 1 output: $56 \times 56 \times 256 = 802{,}816$ values
    <li>Layer 2 output: $28 \times 28 \times 512 = 401{,}408$ values
    <li>Layer 3 output: $14 \times 14 \times 1024 = 200{,}704$ values
    <li>Layer 4 output: $7 \times 7 \times 2048 = 100{,}352$ values
</ul>

<p><strong>Peak: $\approx 3.2$ MB</strong></p>

<p><strong>Parameters:</strong> $25.6$M $\times$ 4 bytes = 102 MB</p>

<p><strong>Total ResNet-50 inference: $\approx 105$ MB</strong></p>

<p><strong>Ratio: ViT-Base uses $3.8\times$ more memory than ResNet-50</strong></p>

<p><strong>Part (d): Which is More Efficient?</strong></p>

<p><strong>Efficiency Analysis:</strong></p>

<table>
<tr><th><strong>Metric</strong></th><th><strong>ViT-Base</strong></th><th><strong>ResNet-50</strong></th><th><strong>Ratio</strong></th></tr>
<tr><td>Parameters</td><td>86M</td><td>25.6M</td><td>$3.4\times$</td></tr>
<tr><td>FLOPs</td><td>17.5 GFLOPs</td><td>4.1 GFLOPs</td><td>$4.3\times$</td></tr>
<tr><td>Memory</td><td>402 MB</td><td>105 MB</td><td>$3.8\times$</td></tr>
</table>

<p><strong>Conclusion:</strong></p>

<p><strong>ResNet-50 is more computationally efficient</strong> in terms of:
<ul>
    <li>Fewer parameters ($3.4\times$ less)
    <li>Lower FLOPs ($4.3\times$ less)
    <li>Smaller memory footprint ($3.8\times$ less)
</ul>

<p><strong>However, ViT-Base has advantages:</strong></p>

<ol>
    <li><strong>Better scaling:</strong> Performance improves more with larger datasets
    <li><strong>Transfer learning:</strong> Pre-trained ViT generalizes better
    <li><strong>Parallelization:</strong> Self-attention is more parallelizable than convolutions
    <li><strong>Long-range dependencies:</strong> Global receptive field from layer 1
    <li><strong>Interpretability:</strong> Attention maps show what model focuses on
</ol>

<p><strong>Trade-off:</strong>
<ul>
    <li><strong>Small datasets:</strong> ResNet-50 is better (more efficient, better inductive bias)
    <li><strong>Large datasets (ImageNet-21k, JFT-300M):</strong> ViT-Base is better (superior accuracy)
    <li><strong>Edge deployment:</strong> ResNet-50 preferred (lower resource requirements)
    <li><strong>Cloud deployment:</strong> ViT-Base viable (resources available, better accuracy)
</ul>

<p><strong>Practical Recommendation:</strong></p>

<p>For ImageNet-1k from scratch: <strong>ResNet-50</strong></p>

<p>For transfer learning with pre-training: <strong>ViT-Base</strong></p>

<p>For production with limited compute: <strong>ResNet-50</strong></p>

<p>For research and maximum accuracy: <strong>ViT-Base</strong>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 3: MAE Masking Implementation</strong>

<pre><code>import torch
import torch.nn as nn
import numpy as np

class MAEMasking(nn.Module):
    def __init__(self, n_patches=196, embed_dim=768, mask_ratio=0.75):
        super().__init__()
        self.n_patches = n_patches
        self.embed_dim = embed_dim
        self.mask_ratio = mask_ratio
        self.n_visible = int(n_patches * (1 - mask_ratio))
        
        # Mask token for decoder
        self.mask_token = nn.Parameter(torch.randn(1, 1, embed_dim))
    
    def random_masking(self, x):
        """
        Randomly mask patches
        Args:
            x: (B, N, D) where N = n_patches + 1 (including CLS)
        Returns:
            x_visible: (B, n_visible+1, D) visible patches + CLS
            mask: (B, N) binary mask (0 = keep, 1 = remove)
            ids_restore: (B, N) indices to restore original order
        """
        B, N, D = x.shape
        N_patches = N - 1  # Exclude CLS token
        
        # Generate random noise for shuffling
        noise = torch.rand(B, N_patches, device=x.device)
        
        # Sort noise to get shuffle indices
        ids_shuffle = torch.argsort(noise, dim=1)
        ids_restore = torch.argsort(ids_shuffle, dim=1)
        
        # Keep first n_visible patches
        ids_keep = ids_shuffle[:, :self.n_visible]
        
        # Extract CLS token
        cls_token = x[:, :1, :]  # (B, 1, D)
        
        # Extract patch tokens (exclude CLS)
        x_patches = x[:, 1:, :]  # (B, N_patches, D)
        
        # Gather visible patches
        x_visible = torch.gather(
            x_patches, 
            dim=1, 
            index=ids_keep.unsqueeze(-1).expand(-1, -1, D)
        )  # (B, n_visible, D)
        
        # Concatenate CLS token
        x_visible = torch.cat([cls_token, x_visible], dim=1)  # (B, n_visible+1, D)
        
        # Generate binary mask: 0 = keep, 1 = remove
        mask = torch.ones(B, N_patches, device=x.device)
        mask[:, :self.n_visible] = 0
        mask = torch.gather(mask, dim=1, index=ids_restore)
        
        return x_visible, mask, ids_restore
    
    def add_mask_tokens(self, x_visible, ids_restore):
        """
        Add mask tokens for decoder
        Args:
            x_visible: (B, n_visible+1, D)
            ids_restore: (B, N_patches)
        Returns:
            x_full: (B, N, D) with mask tokens
        """
        B, _, D = x_visible.shape
        
        # Extract CLS token
        cls_token = x_visible[:, :1, :]
        
        # Extract visible patches
        x_patches = x_visible[:, 1:, :]  # (B, n_visible, D)
        
        # Create mask tokens
        n_mask = self.n_patches - self.n_visible
        mask_tokens = self.mask_token.expand(B, n_mask, -1)
        
        # Concatenate visible and mask tokens
        x_combined = torch.cat([x_patches, mask_tokens], dim=1)  # (B, N_patches, D)
        
        # Restore original order
        x_restored = torch.gather(
            x_combined,
            dim=1,
            index=ids_restore.unsqueeze(-1).expand(-1, -1, D)
        )  # (B, N_patches, D)
        
        # Add CLS token back
        x_full = torch.cat([cls_token, x_restored], dim=1)  # (B, N, D)
        
        return x_full
    
    def forward(self, x):
        """
        Complete MAE masking pipeline
        """
        # Part 1: Random masking
        x_visible, mask, ids_restore = self.random_masking(x)
        
        # Part 2: Add mask tokens for decoder
        x_full = self.add_mask_tokens(x_visible, ids_restore)
        
        return x_visible, x_full, mask, ids_restore



# Example usage
n_patches = 196
embed_dim = 768
mask_ratio = 0.75
batch_size = 4

# Create MAE masking module
mae_mask = MAEMasking(n_patches, embed_dim, mask_ratio)

# Simulate patch embeddings (including CLS token)
x = torch.randn(batch_size, n_patches + 1, embed_dim)

# Apply masking
x_visible, x_full, mask, ids_restore = mae_mask(x)

print(f"Input shape: {x.shape}")
print(f"Visible patches shape: {x_visible.shape}")
print(f"Full with mask tokens shape: {x_full.shape}")
print(f"Mask shape: {mask.shape}")
print(f"Number of masked patches: {mask.sum(dim=1)[0].item()}")
print(f"Number of visible patches: {(1 - mask).sum(dim=1)[0].item()}")
</code></pre>

<strong>Output:</strong>
<pre><code>
Input shape: torch.Size([4, 197, 768])
Visible patches shape: torch.Size([4, 50, 768])
Full with mask tokens shape: torch.Size([4, 197, 768])
Mask shape: torch.Size([4, 196])
Number of masked patches: 147.0
Number of visible patches: 49.0
</code></pre>

<p><strong>Part (a): Randomly Mask 75\% of 196 Patches</strong></p>

<p><strong>Masking Strategy:</strong>
<ol>
    <li>Generate random noise: $\text{noise} \sim \mathcal{U}(0, 1)^{196}$
    <li>Sort noise to get shuffle indices
    <li>Keep first $49$ patches (25\% of 196)
    <li>Mask remaining $147$ patches (75\% of 196)
</ol>

<p><strong>Mathematical Formulation:</strong></p>

<p>Let $\vx = [\vx_{\text{cls}}, \vx_1, \vx_2, \ldots, \vx_{196}]$ be patch embeddings.</p>

<p>Random permutation: $\pi: \{1, \ldots, 196\} \to \{1, \ldots, 196\}$</p>

<p>Visible set: $\mathcal{V} = \{\pi(1), \ldots, \pi(49)\}$</p>

<p>Masked set: $\mathcal{M} = \{\pi(50), \ldots, \pi(196)\}$</p>

<p>Binary mask: $m_i = \begin{cases} 0 & \text{if } i \in \mathcal{V} \\ 1 & \text{if } i \in \mathcal{M} \end{cases}$</p>

<p><strong>Part (b): Keep 49 Visible Patches</strong></p>

<p><strong>Encoder Input:</strong></p>

<p>$\vx_{\text{visible}} = [\vx_{\text{cls}}, \vx_{\pi(1)}, \vx_{\pi(2)}, \ldots, \vx_{\pi(49)}]$</p>

<p>Shape: $(B, 50, 768)$ where $50 = 49 + 1$ (CLS token)</p>

<p><strong>Computational Savings:</strong></p>

<p>Encoder processes only 25\% of patches:
<ul>
    <li>Attention complexity: $O(50^2 \cdot 768)$ vs $O(197^2 \cdot 768)$
    <li>Speedup: $\frac{197^2}{50^2} = 15.5\times$ faster
    <li>Memory reduction: $15.5\times$ less
</ul>

<p>This is the key efficiency gain of MAE!</p>

<p><strong>Part (c): Add Mask Tokens for Decoder</strong></p>

<p><strong>Decoder Input Construction:</strong></p>

<ol>
    <li>Take encoder output: $\vz_{\text{visible}} = \text{Encoder}(\vx_{\text{visible}})$
    <li>Create mask tokens: $\vm_{\text{mask}} \in \mathbb{R}^{147 \times 768}$ (learnable)
    <li>Concatenate: $[\vz_{\pi(1)}, \ldots, \vz_{\pi(49)}, \vm_1, \ldots, \vm_{147}]$
    <li>Restore original order using $\pi^{-1}$
    <li>Add position embeddings
</ol>

<p><strong>Decoder Input:</strong></p>

<p>$\vx_{\text{decoder}} = [\vx_{\text{cls}}, \vz_1, \vz_2, \ldots, \vz_{196}]$</p>

<p>where $\vz_i = \begin{cases} \text{Encoder output} & \text{if } i \in \mathcal{V} \\ \vm_{\text{mask}} & \text{if } i \in \mathcal{M} \end{cases}$</p>

<p>Shape: $(B, 197, 768)$ - full sequence restored</p>

<p><strong>Part (d): Compute Reconstruction Loss</strong></p>

<p><strong>Decoder Output:</strong></p>

<p>$\hat{\vx}_i = \text{Decoder}(\vx_{\text{decoder}})_i$ for $i = 1, \ldots, 196$</p>

<p><strong>Reconstruction Loss (MSE on masked patches only):</strong></p>

<p>$\mathcal{L}_{\text{MAE}} = \frac{1}{|\mathcal{M}|} \sum_{i \in \mathcal{M}} \|\hat{\vx}_i - \vx_i\|_2^2$</p>

<p>Only compute loss on masked patches (147 patches):</p>

<pre><code>def compute_mae_loss(original_patches, reconstructed_patches, mask):
    """
    Compute MAE reconstruction loss
    Args:
        original_patches: (B, N, D) original patch embeddings
        reconstructed_patches: (B, N, D) decoder output
        mask: (B, N) binary mask (1 = masked, 0 = visible)
    Returns:
        loss: scalar
    """
    # Compute MSE
    mse = (reconstructed_patches - original_patches) ** 2
    mse = mse.mean(dim=-1)  # (B, N) - mean over embedding dim
    
    # Apply mask - only compute loss on masked patches
    loss = (mse * mask).sum() / mask.sum()
    
    return loss

# Example
original = torch.randn(4, 196, 768)
reconstructed = torch.randn(4, 196, 768)
mask = torch.zeros(4, 196)
mask[:, 49:] = 1  # Mask last 147 patches

loss = compute_mae_loss(original, reconstructed, mask)
print(f"MAE Loss: {loss.item():.4f}")
</code></pre>

<p><strong>Why Only Masked Patches?</strong></p>

<ul>
    <li>Visible patches are already seen by encoder
    <li>Predicting visible patches is trivial (identity mapping)
    <li>Masked patches require understanding context
    <li>Forces model to learn semantic representations
</ul>

<p><strong>Complete MAE Training Loop:</strong></p>

<pre><code># 1. Patch embedding
patches = patch_embed(images)  # (B, 197, 768)

# 2. Random masking
visible_patches, mask, ids_restore = random_masking(patches)  # (B, 50, 768)

# 3. Encoder (only on visible patches)
encoded = encoder(visible_patches)  # (B, 50, 768)

# 4. Add mask tokens and restore order
decoder_input = add_mask_tokens(encoded, ids_restore)  # (B, 197, 768)

# 5. Decoder
reconstructed = decoder(decoder_input)  # (B, 197, 768)

# 6. Compute loss (only on masked patches)
loss = compute_mae_loss(patches[:, 1:], reconstructed[:, 1:], mask)

# 7. Backpropagation
loss.backward()
</code></pre>

<p><strong>Key Insights:</strong></p>

<ol>
    <li><strong>High masking ratio (75\%):</strong> Forces model to learn global structure
    <li><strong>Random masking:</strong> Prevents trivial solutions (interpolation)
    <li><strong>Asymmetric encoder-decoder:</strong> Encoder is large, decoder is small
    <li><strong>Pixel-level reconstruction:</strong> Simpler than contrastive learning
    <li><strong>Efficiency:</strong> $15.5\times$ faster than processing all patches
</ol>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 4: Train ViT-Tiny on CIFAR-10</strong>

<pre><code>import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

class ViTTiny(nn.Module):
    def __init__(self, img_size=32, patch_size=4, in_channels=3, 
                 num_classes=10, embed_dim=192, depth=6, num_heads=3,
                 mlp_ratio=4.0, dropout=0.1):
        super().__init__()
        self.patch_size = patch_size
        self.n_patches = (img_size // patch_size) ** 2  # 64 patches
        
        # Patch embedding
        self.patch_embed = nn.Conv2d(
            in_channels, embed_dim, 
            kernel_size=patch_size, stride=patch_size
        )
        
        # CLS token and position embeddings
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1, embed_dim))
        self.dropout = nn.Dropout(dropout)
        
        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=int(embed_dim * mlp_ratio),
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)
        
        # Classification head
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)
    
    def forward(self, x):
        B = x.shape[0]
        
        # Patch embedding
        x = self.patch_embed(x)  # (B, embed_dim, H/P, W/P)
        x = x.flatten(2).transpose(1, 2)  # (B, n_patches, embed_dim)
        
        # Add CLS token
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        
        # Add position embeddings
        x = x + self.pos_embed
        x = self.dropout(x)
        
        # Transformer
        x = self.transformer(x)
        
        # Classification
        x = self.norm(x[:, 0])  # CLS token
        x = self.head(x)
        
        return x



# Part (a): Patch size 4 for 32x32 images
print(f"Image size: 32x32")
print(f"Patch size: 4x4")
print(f"Number of patches: {(32 // 4) ** 2} = 64")
print(f"Each patch: 4x4x3 = 48 values")

# Part (b): Model configuration
model = ViTTiny(
    img_size=32,
    patch_size=4,
    in_channels=3,
    num_classes=10,
    embed_dim=192,
    depth=6,
    num_heads=3,
    mlp_ratio=4.0
)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
print(f"\nViT-Tiny parameters: {total_params:,}")

# Part (c): RandAugment data augmentation
from torchvision.transforms import RandAugment

train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    RandAugment(num_ops=2, magnitude=9),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

# Load CIFAR-10
train_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=train_transform
)
test_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=True, transform=test_transform
)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)

# Training setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

# Training loop
def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
    
    return total_loss / len(loader), 100. * correct / total

def evaluate(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    return 100. * correct / total

# Train for 200 epochs
num_epochs = 200
best_acc = 0

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    test_acc = evaluate(model, test_loader, device)
    scheduler.step()
    
    if test_acc > best_acc:
        best_acc = test_acc
    
    if (epoch + 1) 
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}
        print(f"Test Acc: {test_acc:.2f}

print(f"\nFinal Best Test Accuracy: {best_acc:.2f}
</code></pre>

<p><strong>Part (d): Compare to Small ResNet</strong></p>

<pre><code># Small ResNet for CIFAR-10
class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = torch.relu(out)
        return out

class SmallResNet(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        
        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)
    
    def _make_layer(self, in_channels, out_channels, num_blocks, stride):
        layers = [BasicBlock(in_channels, out_channels, stride)]
        for _ in range(1, num_blocks):
            layers.append(BasicBlock(out_channels, out_channels, 1))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and compare models
resnet = SmallResNet(num_classes=10)
vit_tiny = ViTTiny()

resnet_params = sum(p.numel() for p in resnet.parameters())
vit_params = sum(p.numel() for p in vit_tiny.parameters())

print("Model Comparison:")
print(f"ViT-Tiny parameters: {vit_params:,}")
print(f"Small ResNet parameters: {resnet_params:,}")
print(f"Ratio: {vit_params / resnet_params:.2f}x")
</code></pre>

<p><strong>Expected Results:</strong></p>

<table>
<tr><th><strong>Model</strong></th><th><strong>Parameters</strong></th><th><strong>Test Acc</strong></th><th><strong>Training Time</strong></th></tr>
<tr><td>ViT-Tiny</td><td>$\sim$5.7M</td><td>85-87\%</td><td>Slower</td></tr>
<tr><td>Small ResNet</td><td>$\sim$2.8M</td><td>88-90\%</td><td>Faster</td></tr>
</table>

<p><strong>Analysis:</strong></p>

<p><strong>Part (a): Patch Size 4 for 32√ó32 Images</strong></p>

<ul>
    <li>Image: $32 \times 32 \times 3$
    <li>Patch size: $4 \times 4$
    <li>Number of patches: $\frac{32}{4} \times \frac{32}{4} = 8 \times 8 = 64$
    <li>Each patch: $4 \times 4 \times 3 = 48$ values
    <li>Sequence length: $64 + 1 = 65$ (including CLS token)
</ul>

<p>This is appropriate for CIFAR-10 because:
<ul>
    <li>Smaller images need smaller patches
    <li>64 patches provide sufficient spatial resolution
    <li>Comparable to 196 patches for ImageNet (224√ó224, patch 16)
</ul>

<p><strong>Part (b): Model Configuration</strong></p>

<p><strong>ViT-Tiny Architecture:</strong>
<ul>
    <li>Layers: $L = 6$
    <li>Hidden size: $d = 192$
    <li>Attention heads: $h = 3$
    <li>MLP ratio: $4.0$ (MLP size = $192 \times 4 = 768$)
    <li>Dropout: $0.1$
</ul>

<p><strong>Parameter Count:</strong>
<ul>
    <li>Patch embedding: $3 \times 4 \times 4 \times 192 = 9{,}216$
    <li>Position embeddings: $65 \times 192 = 12{,}480$
    <li>Per layer: $4 \times 192^2 + 2 \times 192 \times 768 \approx 443{,}000$
    <li>6 layers: $6 \times 443{,}000 = 2{,}658{,}000$
    <li>Classification head: $192 \times 10 = 1{,}920$
    <li><strong>Total: $\approx 5.7$M parameters</strong>
</ul>

<p><strong>Part (c): RandAugment</strong></p>

<p><strong>RandAugment Strategy:</strong>
<ul>
    <li>Randomly select $N$ augmentation operations
    <li>Apply with magnitude $M$
    <li>Operations: rotation, shear, color jitter, contrast, etc.
    <li>Typical: $N=2$, $M=9$ for CIFAR-10
</ul>

<p><strong>Why RandAugment for ViT?</strong></p>

<ol>
    <li><strong>Data augmentation is crucial:</strong> ViT lacks inductive bias
    <li><strong>Prevents overfitting:</strong> CIFAR-10 is small (50k images)
    <li><strong>Improves generalization:</strong> +2-3\% accuracy improvement
    <li><strong>Simpler than AutoAugment:</strong> No search required
</ol>

<p><strong>Training Recipe:</strong>
<ul>
    <li>Optimizer: AdamW with weight decay 0.05
    <li>Learning rate: $10^{-3}$ with cosine annealing
    <li>Batch size: 128
    <li>Epochs: 200
    <li>Warmup: 10 epochs (optional)
</ul>

<p><strong>Part (d): Comparison with Small ResNet</strong></p>

<p><strong>Quantitative Comparison:</strong></p>

<table>
<tr><th><strong>Metric</strong></th><th><strong>ViT-Tiny</strong></th><th><strong>Small ResNet</strong></th></tr>
<tr><td>Parameters</td><td>5.7M</td><td>2.8M</td></tr>
<tr><td>FLOPs</td><td>$\sim$0.5 GFLOPs</td><td>$\sim$0.3 GFLOPs</td></tr>
<tr><td>Test Accuracy</td><td>85-87\%</td><td>88-90\%</td></tr>
<tr><td>Training Time</td><td>$\sim$3 hours</td><td>$\sim$2 hours</td></tr>
<tr><td>Convergence</td><td>Slower</td><td>Faster</td></tr>
</table>

<p><strong>Why ResNet Performs Better on CIFAR-10:</strong></p>

<ol>
    <li><strong>Inductive bias:</strong> Convolutions encode spatial locality
    <li><strong>Translation equivariance:</strong> Built into convolutions
    <li><strong>Parameter efficiency:</strong> Fewer parameters, better generalization
    <li><strong>Small dataset:</strong> CIFAR-10 (50k) is too small for ViT
    <li><strong>Low resolution:</strong> $32 \times 32$ images have limited spatial information
</ol>

<p><strong>When ViT Would Win:</strong></p>

<ul>
    <li><strong>Pre-training:</strong> Train on ImageNet-21k, fine-tune on CIFAR-10
    <li><strong>Larger dataset:</strong> More training data (e.g., 500k images)
    <li><strong>Higher resolution:</strong> Upscale CIFAR-10 to $224 \times 224$
    <li><strong>Transfer learning:</strong> Use pre-trained ViT weights
</ul>

<p><strong>Practical Recommendations:</strong></p>

<ol>
    <li><strong>From scratch on CIFAR-10:</strong> Use ResNet (better accuracy, faster)
    <li><strong>With pre-training:</strong> Use ViT (transfer learning advantage)
    <li><strong>Research purposes:</strong> Try both, compare carefully
    <li><strong>Production:</strong> ResNet for efficiency, ViT for maximum accuracy with pre-training
</ol>

<p><strong>Key Takeaways:</strong></p>

<ul>
    <li>ViT requires large-scale pre-training to excel
    <li>Convolutional inductive bias helps on small datasets
    <li>Data augmentation is critical for ViT
    <li>Patch size should scale with image resolution
    <li>ResNet is more sample-efficient on CIFAR-10
</ul>

<p><strong>Experiment Variations to Try:</strong></p>

<ol>
    <li>Increase ViT depth to 12 layers
    <li>Try different patch sizes (2, 4, 8)
    <li>Add more aggressive augmentation
    <li>Use mixup or cutmix
    <li>Pre-train on CIFAR-100, fine-tune on CIFAR-10
    <li>Compare with hybrid models (ResNet + Transformer)
</ol>
</div>
        
        <div class="chapter-nav">
  <a href="chapter16_efficient_transformers.html">‚Üê Chapter 16: Efficient Transformers</a>
  <a href="../index.html">üìö Table of Contents</a>
  <a href="chapter18_multimodal_transformers.html">Chapter 18: Multimodal Transformers ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
