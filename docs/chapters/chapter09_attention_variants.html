<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 9: Attention Variants and Mechanisms - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../css/style.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                vh: '{\\mathbf{h}}',
                vw: '{\\mathbf{w}}',
                vb: '{\\mathbf{b}}',
                vq: '{\\mathbf{q}}',
                vk: '{\\mathbf{k}}',
                vv: '{\\mathbf{v}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mQ: '{\\mathbf{Q}}',
                mK: '{\\mathbf{K}}',
                mV: '{\\mathbf{V}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mU: '{\\mathbf{U}}',
                mM: '{\\mathbf{M}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
    </nav>

    <main>
        <h1>Attention Variants and Mechanisms</h1>

<h2>Chapter Overview</h2>

<p>Beyond standard scaled dot-product attention, numerous variants have been developed for specific use cases and improved efficiency. This chapter explores cross-attention for encoder-decoder models, soft vs hard attention, attention with relative position representations, and practical considerations for implementing attention mechanisms.</p>

<h3>Learning Objectives</h3>

<ol>
    <li>Distinguish between self-attention and cross-attention
    <li>Understand relative position representations
    <li>Implement attention with different scoring functions
    <li>Apply attention masking for various scenarios
    <li>Understand attention dropout and layer normalization
    <li>Visualize and interpret attention patterns
</ol>

<h2>Cross-Attention</h2>

<div class="definition"><strong>Definition:</strong> 
In encoder-decoder architectures, decoder attends to encoder output via cross-attention:
<div class="equation">
$$\begin{align}
\mQ &= \mX_{\text{dec}} \mW^Q \quad \text{(queries from decoder)} \\
\mK &= \mX_{\text{enc}} \mW^K \quad \text{(keys from encoder)} \\
\mV &= \mX_{\text{enc}} \mW^V \quad \text{(values from encoder)} \\
\text{CrossAttn}(\mX_{\text{dec}}, \mX_{\text{enc}}) &= \text{softmax}\left(\frac{\mQ \mK\transpose}{\sqrt{d_k}}\right) \mV
\end{align}$$
</div>
</div>

<p><strong>Dimensions:</strong>
<ul>
    <li>Decoder input: $\mX_{\text{dec}} \in \R^{m \times d}$ ($m$ decoder positions)
    <li>Encoder output: $\mX_{\text{enc}} \in \R^{n \times d}$ ($n$ encoder positions)
    <li>Attention matrix: $\mA \in \R^{m \times n}$ (decoder $\times$ encoder)
    <li>Output: $\R^{m \times d_v}$ (same decoder length)
</ul>

<div class="example"><strong>Example:</strong> 
English source: "The cat sat" (3 tokens encoded to $\mX_{\text{enc}} \in \R^{3 \times 512}$)

<p>French target: "Le chat" (2 tokens so far, $\mX_{\text{dec}} \in \R^{2 \times 512}$)</p>

<p>Cross-attention computes:
<div class="equation">
$$
\mA = \begin{bmatrix}
\alpha_{1,1} & \alpha_{1,2} & \alpha_{1,3} \\
\alpha_{2,1} & \alpha_{2,2} & \alpha_{2,3}
\end{bmatrix} \in \R^{2 \times 3}
$$
</div>

<p>where $\alpha_{1,j}$ = attention from decoder position 1 ("Le") to encoder position $j$.</p>

<p>When generating "Le" (the), model should attend strongly to "The" in source.</p>

<p>When generating "chat" (cat), model should attend strongly to "cat" in source.
</div>

<h3>Transformer Decoder Attention Layers</h3>

<p>A transformer decoder block contains <strong>three</strong> attention mechanisms:</p>

<ol>
    <li><strong>Masked self-attention:</strong> Decoder attends to previous decoder positions
    <div class="equation">
$$
    \mQ = \mK = \mV = \mX_{\text{dec}} \quad \text{(with causal mask)}
    $$
</div>

<p><li><strong>Cross-attention:</strong> Decoder attends to encoder output
    <div class="equation">
$$
    \mQ = \mX_{\text{dec}}, \quad \mK = \mV = \mX_{\text{enc}}
    $$
</div>

<p><li><strong>Feed-forward:</strong> Position-wise MLP (not attention)
</ol>

<div class="keypoint">
Encoder-only models (BERT) use only self-attention. Decoder-only models (GPT) use only masked self-attention. Encoder-decoder models (T5, BART) use all three mechanisms.
</div>

<h2>Relative Position Representations</h2>

<p><strong>Problem with absolute positions:</strong> Model learns positions 0-512 during training. How to handle position 600 at inference?</p>

<p><strong>Solution:</strong> Relative position representations‚Äîencode distance between positions, not absolute positions.</p>

<h3>Shaw et al. Relative Attention</h3>

<div class="definition"><strong>Definition:</strong> 
Modify attention scores to include relative position information:
<div class="equation">
$$
e_{ij} = \frac{\vq_i\transpose \vk_j}{\sqrt{d_k}} + \vq_i\transpose \vr^{K}_{i-j}
$$
</div>
where $\vr^{K}_{i-j} \in \R^{d_k}$ encodes relative position $i-j$ (clipped to maximum distance).
\end{equation}
</div>

<p><strong>Advantages:</strong>
<ul>
    <li>Generalize to longer sequences
    <li>Model learns distance-based patterns
    <li>More parameter efficient
</ul>

<h3>T5 Relative Position Bias</h3>

<p>T5 uses even simpler approach‚Äîadd learned bias based on relative position:
<div class="equation">
$$
\mA_{ij} = \text{softmax}\left(\frac{\mQ \mK\transpose}{\sqrt{d_k}} + \mB\right)_{ij}
$$
</div>
where $B_{ij}$ depends only on $|i-j|$ (bucketed by distance).</p>

<h2>Alternative Attention Scoring Functions</h2>

<p>Beyond scaled dot-product, various scoring functions exist:</p>

<h3>Additive (Bahdanau)</h3>
<div class="equation">
$$
\text{score}(\vq, \vk) = \mathbf{v}\transpose \tanh(\mW_1 \vq + \mW_2 \vk)
$$
</div>

<h3>Multiplicative (Luong)</h3>
<div class="equation">
$$
\text{score}(\vq, \vk) = \vq\transpose \mW \vk
$$
</div>

<h3>Scaled Dot-Product (Transformers)</h3>
<div class="equation">
$$
\text{score}(\vq, \vk) = \frac{\vq\transpose \vk}{\sqrt{d_k}}
$$
</div>

<h3>General</h3>
<div class="equation">
$$
\text{score}(\vq, \vk) = \vq\transpose \mW \vk
$$
</div>

<p><strong>Comparison:</strong>
<ul>
    <li><strong>Additive:</strong> More parameters, handles different dimensions
    <li><strong>Dot-product:</strong> Efficient, used in transformers
    <li><strong>General:</strong> Flexible but more parameters
</ul>

<h2>Attention Masking</h2>

<h3>Padding Mask</h3>

<p>For variable-length sequences in batch, mask padding tokens:
<div class="equation">
$$
M_{ij} = \begin{cases}
0 & \text{if position } j \text{ is valid} \\
-\infty & \text{if position } j \text{ is padding}
\end{cases}
$$
</div>

<div class="example"><strong>Example:</strong> 
Batch with sequences of length [5, 7, 4], padded to length 7:
<div class="equation">
$$\begin{align}
\text{Seq 1:} & \quad [w_1, w_2, w_3, w_4, w_5, \text{PAD}, \text{PAD}] \\
\text{Seq 2:} & \quad [w_1, w_2, w_3, w_4, w_5, w_6, w_7] \\
\text{Seq 3:} & \quad [w_1, w_2, w_3, w_4, \text{PAD}, \text{PAD}, \text{PAD}]
\end{align}$$
</div>

<p>Mask for Seq 1:
<div class="equation">
$$
[0, 0, 0, 0, 0, -\infty, -\infty]
$$
</div>

<p>Prevents attending to padding tokens.
</div>

<h3>Combined Masks</h3>

<p>For decoder, combine causal mask and padding mask:
<div class="equation">
$$
\mM_{\text{total}} = \mM_{\text{causal}} + \mM_{\text{padding}}
$$
</div>

<p>Element-wise, use most restrictive: if either mask blocks, result blocks.</p>

<h2>Attention Dropout</h2>

<p>Apply dropout to attention weights for regularization:
<div class="equation">
$$
\mA = \text{Dropout}\left(\text{softmax}\left(\frac{\mQ \mK\transpose}{\sqrt{d_k}}\right)\right)
$$
</div>

<p>Typical dropout rate: 0.1 (10\%)</p>

<p><strong>Effect:</strong> Randomly zero out some attention connections, preventing over-reliance on specific positions.</p>

<h2>Layer Normalization with Attention</h2>

<p>Two architectures for combining attention with layer norm:</p>

<h3>Post-Norm (Original Transformer)</h3>
<div class="equation">
$$\begin{align}
\vh &= \mX + \text{MultiHeadAttn}(\mX) \\
\mZ &= \text{LayerNorm}(\vh)
\end{align}$$
</div>

<h3>Pre-Norm (More Common Now)</h3>
<div class="equation">
$$\begin{align}
\vh &= \mX + \text{MultiHeadAttn}(\text{LayerNorm}(\mX)) \\
\mZ &= \vh
\end{align}$$
</div>

<p><strong>Pre-norm advantages:</strong>
<ul>
    <li>More stable training
    <li>Easier gradient flow
    <li>Used in GPT-2, GPT-3, modern transformers
</ul>

<h2>Visualizing Attention</h2>

<p>Attention weights $\mA \in \R^{n \times n}$ reveal what model attends to:</p>

<h3>Attention Heatmaps</h3>

<p>For sentence "The cat sat on the mat":
<ul>
    <li>Row $i$: attention distribution when processing token $i$
    <li>Bright cell $(i,j)$: token $i$ strongly attends to token $j$
</ul>

<p><strong>Patterns observed:</strong>
<ul>
    <li>Diagonal: Attending to self
    <li>Vertical lines: Attending to specific important words (e.g., subject, verb)
    <li>Symmetric patterns: Mutual attention between related words
    <li>Head-specific patterns: Different heads learn different relationships
</ul>

<h3>Interpreting Multiple Heads</h3>

<p>In 12-head attention, different heads specialize:
<ul>
    <li>Some heads attend to adjacent words (local syntax)
    <li>Some heads attend to distant words (long-range dependencies)
    <li>Some heads attend to specific parts of speech
    <li>Some heads attend based on semantic similarity
</ul>

<div class="caution">
Attention weights are NOT necessarily model explanations! High attention doesn't always mean high importance for prediction. Attention shows where model looks, not why decisions are made.
</div>

<h2>Practical Implementation Considerations</h2>

<h3>Memory-Efficient Attention</h3>

<p>For very long sequences, store attention matrix in chunks:
<ol>
    <li>Compute $\mQ \mK\transpose$ for chunk of queries
    <li>Apply softmax
    <li>Multiply by $\mV$ chunk
    <li>Accumulate results
</ol>

<p>Reduces peak memory from $O(n^2)$ to $O(nc)$ where $c$ is chunk size.</p>

<h3>Fused Attention Kernels</h3>

<p>Modern implementations fuse operations:
<div class="equation">
$$
\text{QK}^T \to \text{Scale} \to \text{Mask} \to \text{Softmax} \to \text{Dropout} \to \text{multiply } \mV
$$
</div>

<p>Single fused kernel faster than separate operations (fewer memory transfers).</p>

<p>Example: FlashAttention achieves 2-4x speedup through fused operations and memory hierarchy optimization.</p>

<h2>Exercises</h2>

<p>\begin{exercise}
Implement cross-attention layer in PyTorch. Test with encoder output (length 10, dim 128) and decoder input (length 7, dim 128). Verify attention matrix shape is $7 \times 10$.
</div>

<p>\begin{exercise}
For sequence length 1000 with 8 attention heads and $d_{\text{model}} = 512$: (1) Calculate attention matrix memory for all heads, (2) Estimate speedup if using sparse attention (10\% sparsity), (3) What is memory reduction?
</div>

<p>\begin{exercise}
Implement relative position bias as in T5. Use buckets: [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32+]. Show how attention scores change with relative distance.
</div>

<p>\begin{exercise}
Create visualization showing: (1) Self-attention patterns for sentence "The quick brown fox jumps", (2) Effect of causal masking, (3) Difference between heads 1 and 12 in multi-head attention. What patterns emerge?
</div>

<p>\begin{exercise}
Compare computational cost of: (1) Additive (Bahdanau) attention, (2) Multiplicative attention, (3) Scaled dot-product attention. For $n = 512$, $d_k = 64$, which is most efficient?
</div>
        
        <div class="chapter-nav">
  <a href="chapter08_self_attention.html">‚Üê Chapter 8: Self-Attention and Multi-Head Attention</a>
  <a href="../index.html">üìö Table of Contents</a>
  <a href="chapter10_transformer_model.html">Chapter 10: The Transformer Model ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
