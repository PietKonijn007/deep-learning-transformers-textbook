\chapter{Solutions to Exercises}
\label{chap:solutions}

\section*{How to Use This Chapter}

This chapter provides complete solutions to all exercises in the book. Each solution includes:
\begin{itemize}
    \item Step-by-step mathematical derivations
    \item Numerical calculations with intermediate steps
    \item Implementation code where applicable
    \item Explanations of key concepts and common pitfalls
\end{itemize}

Solutions are organized by chapter and exercise number for easy reference. When working through exercises, try to solve them independently first, then use these solutions to check your work and deepen your understanding.

\section{Chapter 1: Linear Algebra for Deep Learning}

\subsection*{Exercise 1.1}
\textbf{Problem:} Given $\vx = [2, -1, 3]\transpose$ and $\vy = [1, 4, -2]\transpose$, compute: (1) The dot product $\vx\transpose \vy$, (2) The L2 norms $\norm{\vx}_2$ and $\norm{\vy}_2$, (3) The cosine similarity between $\vx$ and $\vy$.

\textbf{Solution:}

\textit{Part 1: Dot product}
\begin{align}
\vx\transpose \vy &= (2)(1) + (-1)(4) + (3)(-2) \\
&= 2 - 4 - 6 \\
&= -8
\end{align}

\textit{Part 2: L2 norms}
\begin{align}
\norm{\vx}_2 &= \sqrt{2^2 + (-1)^2 + 3^2} = \sqrt{4 + 1 + 9} = \sqrt{14} \approx 3.742 \\
\norm{\vy}_2 &= \sqrt{1^2 + 4^2 + (-2)^2} = \sqrt{1 + 16 + 4} = \sqrt{21} \approx 4.583
\end{align}

\textit{Part 3: Cosine similarity}
\begin{align}
\text{sim}(\vx, \vy) &= \frac{\vx\transpose \vy}{\norm{\vx}_2 \norm{\vy}_2} \\
&= \frac{-8}{\sqrt{14} \times \sqrt{21}} \\
&= \frac{-8}{\sqrt{294}} \\
&= \frac{-8}{17.146} \\
&\approx -0.467
\end{align}

The negative cosine similarity indicates that the vectors point in somewhat opposite directions (angle $> 90Â°$).


\subsection*{Exercise 1.2}
\textbf{Problem:} For a transformer layer with $d_{\text{model}} = 768$ and feed-forward dimension $d_{ff} = 3072$: (1) Calculate the number of parameters in the two linear transformations, (2) If processing a batch of $B = 32$ sequences of length $n = 512$, what are the dimensions of the input tensor? (3) How many floating-point operations (FLOPs) are required for one forward pass through this layer?

\textbf{Solution:}

\textit{Part 1: Parameter count}

The feed-forward network has two linear transformations:
\begin{align}
\text{First layer:} \quad &\mW_1 \in \R^{3072 \times 768}, \quad \vb_1 \in \R^{3072} \\
&\text{Parameters: } 3072 \times 768 + 3072 = 2{,}362{,}368 \\
\text{Second layer:} \quad &\mW_2 \in \R^{768 \times 3072}, \quad \vb_2 \in \R^{768} \\
&\text{Parameters: } 768 \times 3072 + 768 = 2{,}360{,}064 \\
\text{Total:} \quad &2{,}362{,}368 + 2{,}360{,}064 = 4{,}722{,}432 \text{ parameters}
\end{align}

\textit{Part 2: Input tensor dimensions}

For batch size $B = 32$ and sequence length $n = 512$:
\begin{equation}
\text{Input tensor: } \mX \in \R^{32 \times 512 \times 768}
\end{equation}

\textit{Part 3: FLOPs calculation}

For each linear transformation $\vy = \mW\vx + \vb$, the FLOPs are $2 \times \text{(input dim)} \times \text{(output dim)}$ per token.

For the entire batch:
\begin{align}
\text{First transformation:} \quad &2 \times (B \times n) \times d_{\text{model}} \times d_{ff} \\
&= 2 \times (32 \times 512) \times 768 \times 3072 \\
&= 2 \times 16{,}384 \times 768 \times 3072 \\
&= 77{,}309{,}411{,}328 \text{ FLOPs} \\
\text{Second transformation:} \quad &2 \times (B \times n) \times d_{ff} \times d_{\text{model}} \\
&= 77{,}309{,}411{,}328 \text{ FLOPs} \\
\text{Total:} \quad &154{,}618{,}822{,}656 \approx 154.6 \text{ GFLOPs}
\end{align}

\subsection*{Exercise 1.3}
\textbf{Problem:} Prove that for symmetric matrix $\mA = \mA\transpose$, eigenvectors corresponding to distinct eigenvalues are orthogonal.

\textbf{Solution:}

Let $\vv_1$ and $\vv_2$ be eigenvectors of symmetric matrix $\mA$ with distinct eigenvalues $\lambda_1 \neq \lambda_2$. We need to prove $\vv_1\transpose \vv_2 = 0$.

Starting with the eigenvalue equations:
\begin{align}
\mA \vv_1 &= \lambda_1 \vv_1 \\
\mA \vv_2 &= \lambda_2 \vv_2
\end{align}

Multiply the first equation on the left by $\vv_2\transpose$:
\begin{equation}
\vv_2\transpose \mA \vv_1 = \lambda_1 \vv_2\transpose \vv_1
\end{equation}

Take the transpose of the second eigenvalue equation:
\begin{equation}
\vv_2\transpose \mA\transpose = \lambda_2 \vv_2\transpose
\end{equation}

Since $\mA$ is symmetric ($\mA = \mA\transpose$):
\begin{equation}
\vv_2\transpose \mA = \lambda_2 \vv_2\transpose
\end{equation}

Multiply on the right by $\vv_1$:
\begin{equation}
\vv_2\transpose \mA \vv_1 = \lambda_2 \vv_2\transpose \vv_1
\end{equation}

Now we have two expressions for $\vv_2\transpose \mA \vv_1$:
\begin{align}
\lambda_1 \vv_2\transpose \vv_1 &= \lambda_2 \vv_2\transpose \vv_1 \\
(\lambda_1 - \lambda_2) \vv_2\transpose \vv_1 &= 0
\end{align}

Since $\lambda_1 \neq \lambda_2$, we have $\lambda_1 - \lambda_2 \neq 0$, therefore:
\begin{equation}
\vv_2\transpose \vv_1 = 0
\end{equation}

This proves that eigenvectors corresponding to distinct eigenvalues are orthogonal. $\square$


\subsection*{Exercise 1.4}
\textbf{Problem:} A weight matrix $\mW \in \R^{1024 \times 4096}$ is approximated using SVD with rank $r$. (1) Express the number of parameters as a function of $r$, (2) What value of $r$ achieves 75\% compression? (3) What is the memory savings in MB (assuming 32-bit floats)?

\textbf{Solution:}

\textit{Part 1: Parameters as function of $r$}

The SVD factorization gives $\mW \approx \mW_1 \mW_2$ where:
\begin{align}
\mW_1 &\in \R^{1024 \times r} \quad \text{(contains } 1024r \text{ parameters)} \\
\mW_2 &\in \R^{r \times 4096} \quad \text{(contains } 4096r \text{ parameters)}
\end{align}

Total parameters:
\begin{equation}
P(r) = 1024r + 4096r = 5120r
\end{equation}

\textit{Part 2: Rank for 75\% compression}

Original parameters: $1024 \times 4096 = 4{,}194{,}304$

For 75\% compression, we want 25\% of original parameters:
\begin{align}
5120r &= 0.25 \times 4{,}194{,}304 \\
5120r &= 1{,}048{,}576 \\
r &= \frac{1{,}048{,}576}{5120} \\
r &= 204.8 \approx 205
\end{align}

\textit{Part 3: Memory savings}

Original memory: $4{,}194{,}304 \times 4 \text{ bytes} = 16{,}777{,}216 \text{ bytes} \approx 16.0 \text{ MB}$

Compressed memory (with $r = 205$): $1{,}048{,}576 \times 4 \text{ bytes} = 4{,}194{,}304 \text{ bytes} \approx 4.0 \text{ MB}$

Memory savings: $16.0 - 4.0 = 12.0 \text{ MB}$

\subsection*{Exercise 1.5}
\textbf{Problem:} Consider computing attention scores $\mA = \mQ\mK\transpose$ where $\mQ, \mK \in \R^{B \times n \times d_k}$ with $B = 16$, $n = 1024$, $d_k = 64$. (1) What are the dimensions of the output $\mA$? (2) Calculate the total FLOPs required, (3) Compute the arithmetic intensity (FLOPs per byte transferred, assuming 32-bit floats), (4) Is this operation compute-bound or memory-bound on a GPU with 312 TFLOPS and 1.6 TB/s bandwidth?

\textbf{Solution:}

\textit{Part 1: Output dimensions}

For each batch element, we compute:
\begin{equation}
\underbrace{\mQ}_{n \times d_k} \underbrace{\mK\transpose}_{d_k \times n} = \underbrace{\mA}_{n \times n}
\end{equation}

For the full batch:
\begin{equation}
\mA \in \R^{16 \times 1024 \times 1024}
\end{equation}

\textit{Part 2: FLOPs calculation}

For one batch element: $2 \times n \times d_k \times n = 2 \times 1024 \times 64 \times 1024 = 134{,}217{,}728$ FLOPs

For full batch: $B \times 2n^2d_k = 16 \times 134{,}217{,}728 = 2{,}147{,}483{,}648 \approx 2.15$ GFLOPs

\textit{Part 3: Arithmetic intensity}

Memory transferred (32-bit floats = 4 bytes):
\begin{align}
\text{Read } \mQ: \quad &16 \times 1024 \times 64 \times 4 = 4{,}194{,}304 \text{ bytes} \\
\text{Read } \mK: \quad &16 \times 1024 \times 64 \times 4 = 4{,}194{,}304 \text{ bytes} \\
\text{Write } \mA: \quad &16 \times 1024 \times 1024 \times 4 = 67{,}108{,}864 \text{ bytes} \\
\text{Total: } \quad &75{,}497{,}472 \text{ bytes} \approx 72.0 \text{ MB}
\end{align}

Arithmetic intensity:
\begin{equation}
\frac{2{,}147{,}483{,}648 \text{ FLOPs}}{75{,}497{,}472 \text{ bytes}} \approx 28.4 \text{ FLOP/byte}
\end{equation}

\textit{Part 4: Compute-bound or memory-bound?}

Compute time: $\frac{2.15 \text{ GFLOPs}}{312{,}000 \text{ GFLOPs}} \approx 6.9 \mu\text{s}$

Memory time: $\frac{72.0 \text{ MB}}{1{,}600{,}000 \text{ MB/s}} \approx 45.0 \mu\text{s}$

Since memory time $>$ compute time, this operation is \textbf{memory-bound}. The GPU spends more time waiting for data than computing.


\subsection*{Exercise 1.6}
\textbf{Problem:} An embedding layer has vocabulary size $V = 32{,}000$ and embedding dimension $d = 512$. (1) How many parameters does the embedding matrix contain? (2) What is the memory requirement in MB for 32-bit floats? (3) For a batch of $B = 64$ sequences of length $n = 256$, what is the memory required for the embedded representations? (4) If we use LoRA with rank $r = 16$ to adapt the embeddings, how many trainable parameters are needed?

\textbf{Solution:}

\textit{Part 1: Parameter count}
\begin{equation}
\text{Parameters} = V \times d = 32{,}000 \times 512 = 16{,}384{,}000
\end{equation}

\textit{Part 2: Memory for embedding matrix}
\begin{equation}
\text{Memory} = 16{,}384{,}000 \times 4 \text{ bytes} = 65{,}536{,}000 \text{ bytes} \approx 62.5 \text{ MB}
\end{equation}

\textit{Part 3: Memory for embedded batch}

Embedded tensor dimensions: $\R^{64 \times 256 \times 512}$
\begin{equation}
\text{Memory} = 64 \times 256 \times 512 \times 4 = 33{,}554{,}432 \text{ bytes} \approx 32.0 \text{ MB}
\end{equation}

\textit{Part 4: LoRA trainable parameters}

LoRA adds $\mE' = \mE + \mB\mA$ where $\mB \in \R^{V \times r}$ and $\mA \in \R^{r \times d}$:
\begin{align}
\text{Parameters} &= Vr + rd \\
&= 32{,}000 \times 16 + 16 \times 512 \\
&= 512{,}000 + 8{,}192 \\
&= 520{,}192
\end{align}

This is only $\frac{520{,}192}{16{,}384{,}000} \approx 3.2\%$ of the original parameters!

\subsection*{Exercise 1.7}
\textbf{Problem:} Compare the computational cost of two equivalent operations: (1) Computing $(\mA\mB)\vx$ where $\mA \in \R^{m \times n}$, $\mB \in \R^{n \times p}$, $\vx \in \R^p$, (2) Computing $\mA(\mB\vx)$. For $m = 512$, $n = 2048$, $p = 512$, which order is more efficient and by what factor?

\textbf{Solution:}

\textit{Method 1: $(\mA\mB)\vx$}

First compute $\mC = \mA\mB \in \R^{m \times p}$:
\begin{equation}
\text{FLOPs}_1 = 2mnp = 2 \times 512 \times 2048 \times 512 = 1{,}073{,}741{,}824
\end{equation}

Then compute $\mC\vx \in \R^m$:
\begin{equation}
\text{FLOPs}_2 = 2mp = 2 \times 512 \times 512 = 524{,}288
\end{equation}

Total: $1{,}073{,}741{,}824 + 524{,}288 = 1{,}074{,}266{,}112$ FLOPs

\textit{Method 2: $\mA(\mB\vx)$}

First compute $\vy = \mB\vx \in \R^n$:
\begin{equation}
\text{FLOPs}_1 = 2np = 2 \times 2048 \times 512 = 2{,}097{,}152
\end{equation}

Then compute $\mA\vy \in \R^m$:
\begin{equation}
\text{FLOPs}_2 = 2mn = 2 \times 512 \times 2048 = 2{,}097{,}152
\end{equation}

Total: $2{,}097{,}152 + 2{,}097{,}152 = 4{,}194{,}304$ FLOPs

\textit{Comparison:}
\begin{equation}
\text{Speedup} = \frac{1{,}074{,}266{,}112}{4{,}194{,}304} \approx 256\times
\end{equation}

Method 2 is \textbf{256 times more efficient}! This demonstrates the importance of operation ordering in linear algebra.

\textbf{Key insight:} Always perform matrix-vector multiplications from right to left to minimize intermediate matrix sizes.


\subsection*{Exercise 1.8}
\textbf{Problem:} A matrix multiplication $\mC = \mA\mB$ with $\mA, \mB \in \R^{2048 \times 2048}$ is performed on a GPU. (1) Calculate the total FLOPs, (2) Calculate the memory transferred (assuming matrices are read once and result written once), (3) Compute the arithmetic intensity, (4) If the GPU has 100 TFLOPS compute and 900 GB/s memory bandwidth, what is the theoretical execution time assuming perfect utilization? (5) Which resource (compute or memory) is the bottleneck?

\textbf{Solution:}

\textit{Part 1: FLOPs}
\begin{equation}
\text{FLOPs} = 2n^3 = 2 \times 2048^3 = 17{,}179{,}869{,}184 \approx 17.2 \text{ GFLOPs}
\end{equation}

\textit{Part 2: Memory transferred}
\begin{align}
\text{Read } \mA: \quad &2048^2 \times 4 = 16{,}777{,}216 \text{ bytes} \\
\text{Read } \mB: \quad &2048^2 \times 4 = 16{,}777{,}216 \text{ bytes} \\
\text{Write } \mC: \quad &2048^2 \times 4 = 16{,}777{,}216 \text{ bytes} \\
\text{Total: } \quad &50{,}331{,}648 \text{ bytes} \approx 48.0 \text{ MB}
\end{align}

\textit{Part 3: Arithmetic intensity}
\begin{equation}
\text{AI} = \frac{17{,}179{,}869{,}184}{50{,}331{,}648} \approx 341.3 \text{ FLOP/byte}
\end{equation}

\textit{Part 4: Theoretical execution time}

Compute time:
\begin{equation}
t_{\text{compute}} = \frac{17.2 \text{ GFLOPs}}{100{,}000 \text{ GFLOPs}} = 0.172 \text{ ms}
\end{equation}

Memory time:
\begin{equation}
t_{\text{memory}} = \frac{48.0 \text{ MB}}{900{,}000 \text{ MB/s}} = 0.053 \text{ ms}
\end{equation}

Theoretical time: $\max(0.172, 0.053) = 0.172$ ms

\textit{Part 5: Bottleneck}

Since $t_{\text{compute}} > t_{\text{memory}}$, this operation is \textbf{compute-bound}. The high arithmetic intensity (341 FLOP/byte) means the GPU can perform many operations per byte transferred, making computation the limiting factor.

\section{Chapter 2: Calculus and Optimization}

\subsection*{Exercise 2.1}
\textbf{Problem:} Compute the gradient of $f(\vw) = \vw\transpose \mA \vw + \vb\transpose \vw + c$ where $\mA \in \R^{n \times n}$ is symmetric, $\vw, \vb \in \R^n$, and $c \in \R$.

\textbf{Solution:}

We compute the gradient component by component. For the quadratic term:
\begin{equation}
\frac{\partial}{\partial \vw}(\vw\transpose \mA \vw) = (\mA + \mA\transpose)\vw = 2\mA\vw
\end{equation}
(using the fact that $\mA$ is symmetric, so $\mA = \mA\transpose$)

For the linear term:
\begin{equation}
\frac{\partial}{\partial \vw}(\vb\transpose \vw) = \vb
\end{equation}

For the constant term:
\begin{equation}
\frac{\partial}{\partial \vw}(c) = \mathbf{0}
\end{equation}

Therefore:
\begin{equation}
\nabla_{\vw} f(\vw) = 2\mA\vw + \vb
\end{equation}

\textbf{Verification:} At a critical point, $\nabla f = \mathbf{0}$, giving $2\mA\vw + \vb = \mathbf{0}$, or $\vw = -\frac{1}{2}\mA^{-1}\vb$ (assuming $\mA$ is invertible).


\subsection*{Exercise 2.2}
\textbf{Problem:} Implement backpropagation for a 2-layer network with ReLU activation. Given input $\vx = [1.0, 0.5]\transpose$, weights $\mW^{(1)} \in \R^{3 \times 2}$, $\mW^{(2)} \in \R^{1 \times 3}$, and target $y = 2.0$, compute all gradients.

\textbf{Solution:}

Let's use specific weight values:
\begin{equation}
\mW^{(1)} = \begin{bmatrix} 0.5 & 0.3 \\ -0.2 & 0.8 \\ 0.6 & -0.4 \end{bmatrix}, \quad \mW^{(2)} = \begin{bmatrix} 0.7 & -0.5 & 0.9 \end{bmatrix}
\end{equation}

\textit{Forward pass:}

Layer 1 pre-activation:
\begin{equation}
\vz^{(1)} = \mW^{(1)}\vx = \begin{bmatrix} 0.5(1.0) + 0.3(0.5) \\ -0.2(1.0) + 0.8(0.5) \\ 0.6(1.0) - 0.4(0.5) \end{bmatrix} = \begin{bmatrix} 0.65 \\ 0.20 \\ 0.40 \end{bmatrix}
\end{equation}

Layer 1 activation:
\begin{equation}
\va^{(1)} = \text{ReLU}(\vz^{(1)}) = \begin{bmatrix} 0.65 \\ 0.20 \\ 0.40 \end{bmatrix}
\end{equation}

Layer 2 pre-activation:
\begin{equation}
z^{(2)} = \mW^{(2)}\va^{(1)} = 0.7(0.65) - 0.5(0.20) + 0.9(0.40) = 0.455 + 0.360 = 0.815
\end{equation}

Output: $\hat{y} = z^{(2)} = 0.815$

Loss (MSE): $L = \frac{1}{2}(\hat{y} - y)^2 = \frac{1}{2}(0.815 - 2.0)^2 = \frac{1}{2}(-1.185)^2 = 0.702$

\textit{Backward pass:}

Output gradient:
\begin{equation}
\frac{\partial L}{\partial z^{(2)}} = \hat{y} - y = 0.815 - 2.0 = -1.185
\end{equation}

Gradient w.r.t. $\mW^{(2)}$:
\begin{equation}
\frac{\partial L}{\partial \mW^{(2)}} = \frac{\partial L}{\partial z^{(2)}} \cdot (\va^{(1)})\transpose = -1.185 \begin{bmatrix} 0.65 & 0.20 & 0.40 \end{bmatrix} = \begin{bmatrix} -0.770 & -0.237 & -0.474 \end{bmatrix}
\end{equation}

Gradient w.r.t. $\va^{(1)}$:
\begin{equation}
\frac{\partial L}{\partial \va^{(1)}} = (\mW^{(2)})\transpose \frac{\partial L}{\partial z^{(2)}} = \begin{bmatrix} 0.7 \\ -0.5 \\ 0.9 \end{bmatrix}(-1.185) = \begin{bmatrix} -0.830 \\ 0.593 \\ -1.067 \end{bmatrix}
\end{equation}

Gradient through ReLU (element-wise, ReLU derivative is 1 if input $> 0$, else 0):
\begin{equation}
\frac{\partial L}{\partial \vz^{(1)}} = \frac{\partial L}{\partial \va^{(1)}} \odot \text{ReLU}'(\vz^{(1)}) = \begin{bmatrix} -0.830 \\ 0.593 \\ -1.067 \end{bmatrix} \odot \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} -0.830 \\ 0.593 \\ -1.067 \end{bmatrix}
\end{equation}

Gradient w.r.t. $\mW^{(1)}$:
\begin{equation}
\frac{\partial L}{\partial \mW^{(1)}} = \frac{\partial L}{\partial \vz^{(1)}} \vx\transpose = \begin{bmatrix} -0.830 \\ 0.593 \\ -1.067 \end{bmatrix} \begin{bmatrix} 1.0 & 0.5 \end{bmatrix} = \begin{bmatrix} -0.830 & -0.415 \\ 0.593 & 0.296 \\ -1.067 & -0.533 \end{bmatrix}
\end{equation}

\textbf{Summary of gradients:}
\begin{align}
\frac{\partial L}{\partial \mW^{(2)}} &= \begin{bmatrix} -0.770 & -0.237 & -0.474 \end{bmatrix} \\
\frac{\partial L}{\partial \mW^{(1)}} &= \begin{bmatrix} -0.830 & -0.415 \\ 0.593 & 0.296 \\ -1.067 & -0.533 \end{bmatrix}
\end{align}


\subsection*{Exercise 2.3}
\textbf{Problem:} For Adam optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\alpha = 0.001$: (1) Why is bias correction necessary? (2) What are the effective learning rates after steps $t = 1, 10, 100, 1000$? (3) How does Adam handle sparse gradients compared to SGD?

\textbf{Solution:}

\textit{Part 1: Why bias correction is necessary}

Adam maintains exponential moving averages initialized at zero:
\begin{align}
m_0 &= 0 \quad \text{(first moment)} \\
v_0 &= 0 \quad \text{(second moment)}
\end{align}

Without bias correction, these estimates are biased toward zero, especially in early iterations. For example, after one step:
\begin{align}
m_1 &= \beta_1 m_0 + (1-\beta_1)g_1 = 0.1g_1 \\
v_1 &= \beta_2 v_0 + (1-\beta_2)g_1^2 = 0.001g_1^2
\end{align}

These are only 10\% and 0.1\% of their true values! Bias correction divides by $(1-\beta^t)$ to compensate:
\begin{align}
\hat{m}_t &= \frac{m_t}{1-\beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1-\beta_2^t}
\end{align}

\textit{Part 2: Effective learning rates}

The effective learning rate is:
\begin{equation}
\alpha_{\text{eff}} = \alpha \cdot \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}
\end{equation}

For $t = 1$:
\begin{equation}
\alpha_{\text{eff}} = 0.001 \cdot \frac{\sqrt{1-0.999}}{1-0.9} = 0.001 \cdot \frac{0.0316}{0.1} = 0.000316
\end{equation}

For $t = 10$:
\begin{equation}
\alpha_{\text{eff}} = 0.001 \cdot \frac{\sqrt{1-0.999^{10}}}{1-0.9^{10}} = 0.001 \cdot \frac{0.0999}{0.651} = 0.000153
\end{equation}

For $t = 100$:
\begin{equation}
\alpha_{\text{eff}} = 0.001 \cdot \frac{\sqrt{1-0.999^{100}}}{1-0.9^{100}} \approx 0.001 \cdot \frac{0.302}{1.0} = 0.000302
\end{equation}

For $t = 1000$:
\begin{equation}
\alpha_{\text{eff}} = 0.001 \cdot \frac{\sqrt{1-0.999^{1000}}}{1-0.9^{1000}} \approx 0.001 \cdot \frac{0.632}{1.0} = 0.000632
\end{equation}

The effective learning rate increases from initialization and stabilizes around $0.001 \cdot \frac{1}{\sqrt{0.001}} \approx 0.001 \cdot 31.6 = 0.0316$ for very large $t$.

\textit{Part 3: Handling sparse gradients}

\textbf{SGD with sparse gradients:} When gradient $g_t = 0$ for many steps, the parameter doesn't update. When a non-zero gradient finally appears, SGD takes a full step of size $\alpha g_t$, which can be unstable.

\textbf{Adam with sparse gradients:} Adam maintains separate adaptive learning rates per parameter. For a parameter that rarely receives gradients:
\begin{itemize}
    \item The first moment $m_t$ accumulates the sparse gradient signal
    \item The second moment $v_t$ remains small (close to zero)
    \item The effective learning rate $\frac{\alpha}{\sqrt{v_t} + \epsilon}$ becomes large
    \item This amplifies rare gradient signals, making better use of sparse information
\end{itemize}

Example: If a parameter receives gradient $g = 1.0$ only at steps 1, 100, 200, ..., Adam will maintain a larger effective learning rate for this parameter compared to frequently updated parameters, ensuring it still learns despite sparse updates.


\subsection*{Exercise 2.4}
\textbf{Problem:} A transformer is trained with learning rate warmup over 4000 steps, then inverse square root decay. If $d_{\text{model}} = 512$: (1) Plot the learning rate schedule for 100,000 steps, (2) What is the learning rate at step 1, 4000, and 10,000? (3) Why is warmup beneficial for transformer training?

\textbf{Solution:}

\textit{Part 1: Learning rate schedule}

The schedule is:
\begin{equation}
\text{lr}(t) = d_{\text{model}}^{-0.5} \cdot \min(t^{-0.5}, t \cdot \text{warmup\_steps}^{-1.5})
\end{equation}

For $d_{\text{model}} = 512$ and warmup = 4000:
\begin{equation}
\text{lr}(t) = 512^{-0.5} \cdot \min(t^{-0.5}, t \cdot 4000^{-1.5}) = 0.0442 \cdot \min(t^{-0.5}, t \cdot 3.125 \times 10^{-6})
\end{equation}

\textit{Part 2: Specific learning rates}

At $t = 1$:
\begin{align}
\text{Warmup: } &0.0442 \cdot 1 \cdot 3.125 \times 10^{-6} = 1.38 \times 10^{-7} \\
\text{Decay: } &0.0442 \cdot 1^{-0.5} = 0.0442 \\
\text{lr}(1) &= \min(0.0442, 1.38 \times 10^{-7}) = 1.38 \times 10^{-7}
\end{align}

At $t = 4000$:
\begin{align}
\text{Warmup: } &0.0442 \cdot 4000 \cdot 3.125 \times 10^{-6} = 0.0125 \\
\text{Decay: } &0.0442 \cdot 4000^{-0.5} = 0.000698 \\
\text{lr}(4000) &= \min(0.000698, 0.0125) = 0.000698
\end{align}

At $t = 10{,}000$:
\begin{align}
\text{Warmup: } &0.0442 \cdot 10{,}000 \cdot 3.125 \times 10^{-6} = 0.0313 \\
\text{Decay: } &0.0442 \cdot 10{,}000^{-0.5} = 0.000442 \\
\text{lr}(10{,}000) &= \min(0.000442, 0.0313) = 0.000442
\end{align}

\textit{Part 3: Why warmup is beneficial}

\textbf{Reason 1: Unstable early gradients}
At initialization, the model's predictions are essentially random. Gradients can be very large and point in inconsistent directions. A large learning rate would cause the model to make drastic updates that could destabilize training.

\textbf{Reason 2: Adam optimizer initialization}
Adam's adaptive learning rates start with biased estimates (initialized at zero). During warmup, these estimates stabilize, and bias correction becomes more accurate.

\textbf{Reason 3: Layer normalization statistics}
Transformers use layer normalization, which computes running statistics. Early in training, these statistics are unreliable. Warmup allows them to stabilize before taking large steps.

\textbf{Reason 4: Attention pattern formation}
Self-attention patterns need time to develop meaningful structures. Starting with a small learning rate allows the model to explore the attention space gradually without getting stuck in poor local minima.

\textbf{Empirical evidence:} Training transformers without warmup often leads to:
\begin{itemize}
    \item Training divergence (loss becomes NaN)
    \item Slower convergence
    \item Lower final performance
    \item Unstable attention patterns
\end{itemize}

The warmup period (typically 4000-10000 steps) is a small fraction of total training but crucial for stability.


\section{Chapter 4: Feedforward Networks}

\subsection*{Exercise 4.1}
\textbf{Problem:} Design 3-layer MLP for binary classification of 100-dimensional inputs. Specify layer dimensions, activations, and parameter count.

\textbf{Solution:}

\textbf{Architecture:}
\begin{align}
\text{Input layer:} \quad &\R^{100} \\
\text{Hidden layer 1:} \quad &\R^{100} \to \R^{64} \quad \text{(ReLU activation)} \\
\text{Hidden layer 2:} \quad &\R^{64} \to \R^{32} \quad \text{(ReLU activation)} \\
\text{Output layer:} \quad &\R^{32} \to \R^{1} \quad \text{(Sigmoid activation)}
\end{align}

\textbf{Parameter count:}

Layer 1: $\mW^{(1)} \in \R^{64 \times 100}$, $\vb^{(1)} \in \R^{64}$
\begin{equation}
\text{Parameters: } 64 \times 100 + 64 = 6{,}464
\end{equation}

Layer 2: $\mW^{(2)} \in \R^{32 \times 64}$, $\vb^{(2)} \in \R^{32}$
\begin{equation}
\text{Parameters: } 32 \times 64 + 32 = 2{,}080
\end{equation}

Layer 3: $\mW^{(3)} \in \R^{1 \times 32}$, $\vb^{(3)} \in \R^{1}$
\begin{equation}
\text{Parameters: } 1 \times 32 + 1 = 33
\end{equation}

\textbf{Total parameters: } $6{,}464 + 2{,}080 + 33 = 8{,}577$

\textbf{Forward pass:}
\begin{align}
\vh^{(1)} &= \text{ReLU}(\mW^{(1)}\vx + \vb^{(1)}) \\
\vh^{(2)} &= \text{ReLU}(\mW^{(2)}\vh^{(1)} + \vb^{(2)}) \\
\hat{y} &= \sigma(\mW^{(3)}\vh^{(2)} + \vb^{(3)})
\end{align}

where $\sigma(z) = \frac{1}{1 + e^{-z}}$ is the sigmoid function.

\textbf{Loss function:} Binary cross-entropy
\begin{equation}
L = -[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]
\end{equation}

\subsection*{Exercise 4.2}
\textbf{Problem:} Compute forward pass through 2-layer network with given weights and ReLU activation.

\textbf{Solution:}

Given:
\begin{align}
\vx &= \begin{bmatrix} 2.0 \\ -1.0 \end{bmatrix}, \quad
\mW^{(1)} = \begin{bmatrix} 0.5 & 0.3 \\ -0.2 & 0.8 \end{bmatrix}, \quad
\vb^{(1)} = \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix} \\
\mW^{(2)} &= \begin{bmatrix} 0.7 & -0.5 \end{bmatrix}, \quad
\vb^{(2)} = 0.3
\end{align}

\textit{Layer 1:}
\begin{align}
\vz^{(1)} &= \mW^{(1)}\vx + \vb^{(1)} \\
&= \begin{bmatrix} 0.5 & 0.3 \\ -0.2 & 0.8 \end{bmatrix} \begin{bmatrix} 2.0 \\ -1.0 \end{bmatrix} + \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix} \\
&= \begin{bmatrix} 1.0 - 0.3 \\ -0.4 - 0.8 \end{bmatrix} + \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix} \\
&= \begin{bmatrix} 0.8 \\ -1.4 \end{bmatrix}
\end{align}

\begin{equation}
\vh^{(1)} = \text{ReLU}(\vz^{(1)}) = \begin{bmatrix} 0.8 \\ 0.0 \end{bmatrix}
\end{equation}

\textit{Layer 2:}
\begin{align}
z^{(2)} &= \mW^{(2)}\vh^{(1)} + b^{(2)} \\
&= \begin{bmatrix} 0.7 & -0.5 \end{bmatrix} \begin{bmatrix} 0.8 \\ 0.0 \end{bmatrix} + 0.3 \\
&= 0.56 + 0.3 \\
&= 0.86
\end{align}

\textbf{Final output: } $\hat{y} = 0.86$


\subsection*{Exercise 4.3}
\textbf{Problem:} For layer with 512 inputs and 256 outputs using ReLU: (1) What is He initialization variance? (2) Why different from Xavier? (3) What happens with zero initialization?

\textbf{Solution:}

\textit{Part 1: He initialization variance}

He initialization for ReLU activation:
\begin{equation}
\text{Var}(W_{ij}) = \frac{2}{n_{\text{in}}} = \frac{2}{512} = 0.00391
\end{equation}

Standard deviation:
\begin{equation}
\sigma = \sqrt{0.00391} \approx 0.0625
\end{equation}

Weights are sampled from:
\begin{equation}
W_{ij} \sim \mathcal{N}(0, 0.00391) \quad \text{or} \quad W_{ij} \sim \text{Uniform}(-\sqrt{3 \times 0.00391}, \sqrt{3 \times 0.00391})
\end{equation}

\textit{Part 2: Why different from Xavier?}

\textbf{Xavier initialization:} Designed for linear or tanh activations
\begin{equation}
\text{Var}(W_{ij}) = \frac{2}{n_{\text{in}} + n_{\text{out}}} = \frac{2}{512 + 256} = \frac{2}{768} = 0.00260
\end{equation}

\textbf{Key difference:} ReLU zeros out half the neurons (negative values), effectively halving the variance of activations. He initialization compensates by using $\frac{2}{n_{\text{in}}}$ instead of $\frac{2}{n_{\text{in}} + n_{\text{out}}}$, doubling the initial variance to maintain signal propagation.

\textbf{Mathematical justification:}

For ReLU, the expected value of the activation is:
\begin{equation}
\mathbb{E}[\text{ReLU}(z)] = \mathbb{E}[\max(0, z)] = \frac{1}{2}\mathbb{E}[|z|] \quad \text{(for zero-mean } z \text{)}
\end{equation}

The variance is reduced by approximately half:
\begin{equation}
\text{Var}(\text{ReLU}(z)) \approx \frac{1}{2}\text{Var}(z)
\end{equation}

He initialization's factor of 2 compensates for this reduction.

\textit{Part 3: Zero initialization}

If all weights are initialized to zero:
\begin{equation}
\mW^{(1)} = \mathbf{0}, \quad \vb^{(1)} = \mathbf{0}
\end{equation}

\textbf{Forward pass:}
\begin{equation}
\vh^{(1)} = \text{ReLU}(\mW^{(1)}\vx + \vb^{(1)}) = \text{ReLU}(\mathbf{0}) = \mathbf{0}
\end{equation}

\textbf{Backward pass:} All gradients w.r.t. $\mW^{(1)}$ are identical:
\begin{equation}
\frac{\partial L}{\partial W_{ij}^{(1)}} = \frac{\partial L}{\partial h_i^{(1)}} \cdot x_j = 0 \cdot x_j = 0
\end{equation}

\textbf{Problem:} All neurons in a layer remain identical throughout training (symmetry problem). The network cannot learn different features. This is why random initialization is essential.

\textbf{Bias initialization:} Biases can be initialized to zero without this problem, as weight asymmetry is sufficient to break symmetry.


\subsection*{Exercise 4.4}
\textbf{Problem:} Prove that without nonlinear activations, L-layer network equivalent to single layer.

\textbf{Solution:}

Consider an L-layer network without nonlinear activations:
\begin{align}
\vh^{(1)} &= \mW^{(1)}\vx + \vb^{(1)} \\
\vh^{(2)} &= \mW^{(2)}\vh^{(1)} + \vb^{(2)} \\
&\vdots \\
\vh^{(L)} &= \mW^{(L)}\vh^{(L-1)} + \vb^{(L)}
\end{align}

Substituting recursively:
\begin{align}
\vh^{(2)} &= \mW^{(2)}(\mW^{(1)}\vx + \vb^{(1)}) + \vb^{(2)} \\
&= \mW^{(2)}\mW^{(1)}\vx + \mW^{(2)}\vb^{(1)} + \vb^{(2)}
\end{align}

Continuing for all layers:
\begin{align}
\vh^{(L)} &= \mW^{(L)}\mW^{(L-1)} \cdots \mW^{(2)}\mW^{(1)}\vx + \text{(bias terms)} \\
&= \underbrace{(\mW^{(L)}\mW^{(L-1)} \cdots \mW^{(1)})}_{\mW_{\text{equiv}}}\vx + \underbrace{(\text{combined biases})}_{\vb_{\text{equiv}}} \\
&= \mW_{\text{equiv}}\vx + \vb_{\text{equiv}}
\end{align}

This is equivalent to a single linear layer! The composition of linear transformations is itself a linear transformation.

\textbf{Concrete example:} 2-layer network
\begin{align}
\vh^{(1)} &= \mW^{(1)}\vx + \vb^{(1)} \\
\vh^{(2)} &= \mW^{(2)}\vh^{(1)} + \vb^{(2)} \\
&= \mW^{(2)}(\mW^{(1)}\vx + \vb^{(1)}) + \vb^{(2)} \\
&= (\mW^{(2)}\mW^{(1)})\vx + (\mW^{(2)}\vb^{(1)} + \vb^{(2)})
\end{align}

With $\mW^{(1)} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$, $\mW^{(2)} = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}$:
\begin{equation}
\mW_{\text{equiv}} = \mW^{(2)}\mW^{(1)} = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} = \begin{bmatrix} 23 & 34 \\ 31 & 46 \end{bmatrix}
\end{equation}

\textbf{Conclusion:} Nonlinear activations are essential for deep networks to learn complex functions. Without them, depth provides no additional representational power. $\square$

\section{Chapter 8: Self-Attention}

\subsection*{Exercise 8.1}
\textbf{Problem:} For GPT-2 ($d_{\text{model}} = 1024$, $h = 16$, $n = 1024$): (1) Compute attention matrix memory in MB (float32), (2) Count parameters in one multi-head attention layer, (3) Estimate FLOPs for single forward pass.

\textbf{Solution:}

\textit{Part 1: Attention matrix memory}

For each head, attention matrix is $\R^{n \times n}$. For all heads:
\begin{equation}
\text{Memory} = h \times n \times n \times 4 \text{ bytes} = 16 \times 1024 \times 1024 \times 4 = 67{,}108{,}864 \text{ bytes} \approx 64.0 \text{ MB}
\end{equation}

\textit{Part 2: Parameter count}

Multi-head attention has four weight matrices:
\begin{align}
\mW_Q, \mW_K, \mW_V &\in \R^{d_{\text{model}} \times d_{\text{model}}} \quad \text{(3 matrices)} \\
\mW_O &\in \R^{d_{\text{model}} \times d_{\text{model}}} \quad \text{(1 matrix)}
\end{align}

Total parameters:
\begin{equation}
4 \times d_{\text{model}}^2 = 4 \times 1024^2 = 4{,}194{,}304
\end{equation}

Including biases (if used): $4 \times 1024 = 4{,}096$ additional parameters.

\textit{Part 3: FLOPs for forward pass}

Assuming batch size $B = 1$ for simplicity:

\textbf{Step 1:} Project to Q, K, V (3 matrix multiplications)
\begin{equation}
3 \times (2 \times n \times d_{\text{model}}^2) = 3 \times 2 \times 1024 \times 1024^2 = 6{,}442{,}450{,}944 \text{ FLOPs}
\end{equation}

\textbf{Step 2:} Compute attention scores $\mQ\mK\transpose$ for all heads
\begin{equation}
h \times (2 \times n^2 \times d_k) = 16 \times 2 \times 1024^2 \times 64 = 2{,}147{,}483{,}648 \text{ FLOPs}
\end{equation}
(where $d_k = d_{\text{model}}/h = 64$)

\textbf{Step 3:} Softmax (approximately $5n^2$ operations per head)
\begin{equation}
h \times 5n^2 = 16 \times 5 \times 1024^2 = 83{,}886{,}080 \text{ FLOPs}
\end{equation}

\textbf{Step 4:} Attention-weighted sum $\text{Attention} \times \mV$
\begin{equation}
h \times (2 \times n^2 \times d_k) = 2{,}147{,}483{,}648 \text{ FLOPs}
\end{equation}

\textbf{Step 5:} Output projection
\begin{equation}
2 \times n \times d_{\text{model}}^2 = 2{,}147{,}483{,}648 \text{ FLOPs}
\end{equation}

\textbf{Total:} $\approx 13.0$ GFLOPs per forward pass (single sequence)


\section{Additional Exercise Solutions}

\subsection*{Note on Remaining Exercises}

This solutions chapter provides detailed worked examples for the foundational chapters. For exercises in later chapters (9-23), the solution approach follows similar patterns:

\begin{itemize}
    \item \textbf{Implementation exercises:} Use PyTorch with clear tensor dimension tracking
    \item \textbf{Computational analysis:} Apply FLOP counting formulas from Chapter 1 and 12
    \item \textbf{Memory calculations:} Account for parameters, activations, gradients, and optimizer states
    \item \textbf{Theoretical proofs:} Build on definitions and theorems from earlier chapters
    \item \textbf{Comparison exercises:} Create tables showing trade-offs between methods
\end{itemize}

\subsection*{General Problem-Solving Strategy}

When working through exercises:

\begin{enumerate}
    \item \textbf{Identify the type:} Is it computational, theoretical, or implementation?
    \item \textbf{List knowns and unknowns:} Write down given values and what you need to find
    \item \textbf{Draw diagrams:} Visualize tensor shapes, network architectures, or data flow
    \item \textbf{Check dimensions:} Verify all matrix operations have compatible dimensions
    \item \textbf{Work incrementally:} Break complex problems into smaller steps
    \item \textbf{Verify results:} Check if answers make intuitive sense (e.g., memory should be positive, probabilities sum to 1)
    \item \textbf{Implement and test:} For coding exercises, run on small examples first
\end{enumerate}

\subsection*{Common Pitfalls to Avoid}

\begin{itemize}
    \item \textbf{Dimension errors:} Always track tensor shapes explicitly
    \item \textbf{Off-by-one in FLOPs:} Remember matrix multiplication is $2mnp$ not $mnp$
    \item \textbf{Forgetting batch dimension:} Most operations include batch size $B$
    \item \textbf{Memory units:} Be consistent with bytes, KB, MB, GB
    \item \textbf{Activation memory:} Don't forget to include activations in memory calculations
    \item \textbf{Optimizer states:} Adam requires 2x parameter memory for momentum terms
\end{itemize}

\subsection*{Key Formulas Reference}

\textbf{Matrix multiplication FLOPs:}
\begin{equation}
\mA \in \R^{m \times n}, \mB \in \R^{n \times p} \implies \text{FLOPs} = 2mnp
\end{equation}

\textbf{Attention complexity:}
\begin{equation}
\text{Self-attention: } O(n^2d), \quad \text{FLOPs} = 4n^2d + 2nd^2
\end{equation}

\textbf{Memory for training:}
\begin{equation}
\text{Total} = \text{Parameters} + \text{Gradients} + \text{Optimizer states} + \text{Activations}
\end{equation}

\textbf{Transformer layer parameters:}
\begin{equation}
\text{Attention: } 4d^2, \quad \text{FFN: } 2d \cdot d_{ff}, \quad \text{LayerNorm: } 4d
\end{equation}

\textbf{Arithmetic intensity:}
\begin{equation}
\text{AI} = \frac{\text{FLOPs}}{\text{Bytes transferred}}
\end{equation}

\subsection*{Recommended Practice Approach}

\begin{enumerate}
    \item \textbf{First attempt:} Try solving without looking at solutions
    \item \textbf{Check work:} Compare your approach and answer with the solution
    \item \textbf{Understand differences:} If your answer differs, identify where and why
    \item \textbf{Implement:} For computational exercises, write code to verify
    \item \textbf{Extend:} Try variations (different dimensions, architectures, etc.)
    \item \textbf{Teach:} Explain the solution to someone else or write it out
\end{enumerate}

\subsection*{Additional Resources}

For exercises not fully covered here, refer to:
\begin{itemize}
    \item Chapter text for relevant theorems and examples
    \item PyTorch documentation for implementation details
    \item Research papers cited in each chapter for advanced topics
    \item Online communities for discussion and alternative approaches
\end{itemize}

\section*{Conclusion}

This solutions chapter provides a foundation for understanding how to approach exercises throughout the book. The detailed solutions for early chapters demonstrate the level of rigor and clarity expected. As you progress through later chapters, apply these same principles: show your work, check dimensions, verify results, and always connect back to the underlying theory.

Remember that struggling with exercises is part of the learning process. If a problem seems difficult, break it into smaller pieces, review the relevant chapter sections, and don't hesitate to implement and experiment with code. The goal is not just to get the right answer, but to develop deep understanding of how transformers and deep learning systems work.

\end{document}
