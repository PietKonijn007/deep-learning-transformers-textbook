<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 6: Recurrent Neural Networks - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../css/style.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Recurrent Neural Networks</h1>

<h2>Chapter Overview</h2>

<p>Recurrent Neural Networks (RNNs) process sequential data by maintaining hidden states that capture information from previous time steps. This chapter develops RNNs from basic recurrence to modern architectures like LSTMs and GRUs, establishing foundations for understanding transformers.</p>

<h3>Learning Objectives</h3>

<ol>
    <li>Understand recurrent architectures for sequential data
    <li>Implement vanilla RNNs, LSTMs, and GRUs
    <li>Understand vanishing/exploding gradient problems
    <li>Apply RNNs to sequence modeling tasks
    <li>Understand bidirectional and multi-layer RNNs
</ol>

<h2>Vanilla RNNs</h2>

<div class="definition"><strong>Definition:</strong> 
An RNN processes sequence $\vx_1, \vx_2, \ldots, \vx_T$ by maintaining hidden state $\vh_t \in \R^h$:
<div class="equation">
$$\begin{align}
\vh_t &= \tanh(\mW_{hh} \vh_{t-1} + \mW_{xh} \vx_t + \vb_h) \\
\vy_t &= \mW_{hy} \vh_t + \vb_y
\end{align}$$
</div>
where:
<ul>
    <li>$\mW_{hh} \in \R^{h \times h}$: hidden-to-hidden weights
    <li>$\mW_{xh} \in \R^{h \times d}$: input-to-hidden weights
    <li>$\mW_{hy} \in \R^{k \times h}$: hidden-to-output weights
    <li>$\vh_0$ initialized (often zeros)
</ul>
</div>

<div class="example"><strong>Example:</strong> 
Character-level language model with vocabulary size $V=5$, hidden size $h=3$.

<p>Input sequence: "hello" encoded as one-hot vectors $\vx_1, \ldots, \vx_5 \in \R^5$</p>

<p>Initialize: $\vh_0 = [0, 0, 0]\transpose$</p>

<p><strong>Time step 1:</strong> Process 'h'
<div class="equation">
$$\begin{align}
\vh_1 &= \tanh(\mW_{hh}\vh_0 + \mW_{xh}\vx_1 + \vb_h) \in \R^3 \\
\vy_1 &= \mW_{hy}\vh_1 + \vb_y \in \R^5 \\
\hat{\mathbf{p}}_1 &= \text{softmax}(\vy_1) \quad \text{(predict next character)}
\end{align}$$
</div>

<p><strong>Time step 2:</strong> Process 'e' using $\vh_1$
<div class="equation">
$$
\vh_2 = \tanh(\mW_{hh}\vh_1 + \mW_{xh}\vx_2 + \vb_h)
$$
</div>

<p>Hidden state $\vh_t$ carries information from all previous time steps.
</div>

<h3>Backpropagation Through Time (BPTT)</h3>

<div class="algorithm"><div class="algorithm-title">Algorithm: Backpropagation Through Time</div>
<div class="algorithm-line"><strong>Input:</strong> Sequence $\{\vx_1, \ldots, \vx_T\}$, targets $\{\vy_1, \ldots, \vy_T\}$</div>
<div class="algorithm-line"><strong>Output:</strong> Gradients for all parameters</div>
<div class="algorithm-line"><span class="algorithm-comment">// Forward Pass</span></div>
<div class="algorithm-line"><strong>for</strong> $t = 1$ to $T$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">$\vh_t = \tanh(\mW_{hh}\vh_{t-1} + \mW_{xh}\vx_t + \vb_h)$</div>
<div class="algorithm-line">$\vy_t = \mW_{hy}\vh_t + \vb_y$</div>
<div class="algorithm-line">$L_t = \text{Loss}(\vy_t, \text{target}_t)$</div>
<div class="algorithm-line">}</p></div>
<div class="algorithm-line"><span class="algorithm-comment">// Backward Pass</span></div>
<div class="algorithm-line">Initialize $\frac{\partial L}{\partial \vh_{T+1}} = \mathbf{0}$</div>
<div class="algorithm-line"><strong>for</strong> $t = T$ to $1$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Compute $\frac{\partial L}{\partial \vh_t}$ (includes gradient from $t+1$)</div>
<div class="algorithm-line">Accumulate $\frac{\partial L}{\partial \mW_{hh}}, \frac{\partial L}{\partial \mW_{xh}}, \frac{\partial L}{\partial \mW_{hy}}$</div>
</div>
</div>
</div>

<h3>Vanishing and Exploding Gradients</h3>

<p>The fundamental challenge in training RNNs on long sequences arises from the multiplicative nature of gradient backpropagation through time. When computing gradients with respect to early hidden states, the chain rule requires multiplying Jacobian matrices across all intermediate time steps, leading to exponential growth or decay of gradient magnitudes.</p>

<p>The gradient of the loss with respect to an early hidden state $\vh_0$ involves the product of Jacobians across all time steps:
<div class="equation">
$$
\frac{\partial \vh_T}{\partial \vh_0} = \prod_{t=1}^{T} \frac{\partial \vh_t}{\partial \vh_{t-1}} = \prod_{t=1}^{T} \mW_{hh}\transpose \text{diag}(\tanh'(\vz_t))
$$
</div>
where $\vz_t$ is the pre-activation at time $t$. Each Jacobian $\frac{\partial \vh_t}{\partial \vh_{t-1}}$ has spectral norm bounded by $\norm{\mW_{hh}} \cdot \norm{\text{diag}(\tanh'(\vz_t))}$. Since $\tanh'(z) \in (0, 1]$ with maximum value 1 at $z = 0$, the derivative term is at most 1 and typically much smaller for saturated activations. This means the Jacobian norm is approximately $\norm{\mW_{hh}}$ in the best case.</p>

<p>For a sequence of length $T = 100$, if $\norm{\mW_{hh}} = 0.95$ (slightly less than 1), the gradient magnitude decays as $0.95^{100} \approx 0.006$, reducing gradients by a factor of 167. If $\norm{\mW_{hh}} = 0.9$, the decay is $0.9^{100} \approx 2.7 \times 10^{-5}$, reducing gradients by a factor of 37,000. This exponential decay makes it nearly impossible for the network to learn long-range dependencies: the gradient signal from time step 100 is effectively zero by the time it reaches time step 0. In practice, vanilla RNNs struggle to learn dependencies longer than 10-20 time steps due to vanishing gradients.</p>

<p>Conversely, if $\norm{\mW_{hh}} = 1.05$, the gradient magnitude grows as $1.05^{100} \approx 131.5$, amplifying gradients by a factor of 131. If $\norm{\mW_{hh}} = 1.1$, the growth is $1.1^{100} \approx 13{,}781$, causing gradients to explode. Exploding gradients lead to numerical overflow (NaN values) and training instability, where loss suddenly spikes to infinity. While gradient clipping (capping gradient norms at a threshold like 1.0) provides a practical solution for exploding gradients, it does not address the fundamental problem of vanishing gradients.</p>

<p>The vanishing gradient problem is particularly severe because the spectral norm of $\mW_{hh}$ must be precisely 1.0 to avoid both vanishing and exploding gradients, and maintaining this property during training is extremely difficult. Initialization schemes like orthogonal initialization set $\mW_{hh}$ to have spectral norm 1.0 initially, but gradient descent updates quickly perturb this property. Even with careful initialization, vanilla RNNs rarely learn dependencies beyond 20-30 time steps in practice.</p>

<h3>Quantitative Analysis of Gradient Decay</h3>

<p>To understand the severity of vanishing gradients, consider a concrete example with BERT-base dimensions. Suppose we have a vanilla RNN with hidden dimension $h = 768$ (matching BERT-base) and sequence length $n = 512$ (BERT's maximum sequence length). The recurrence matrix $\mW_{hh} \in \R^{768 \times 768}$ has 589,824 parameters. If we initialize $\mW_{hh}$ orthogonally (spectral norm exactly 1.0) and the $\tanh$ derivatives average 0.5 (typical for non-saturated activations), the effective Jacobian norm per time step is approximately $1.0 \times 0.5 = 0.5$.</p>

<p>Over 512 time steps, the gradient magnitude decays as $0.5^{512} \approx 10^{-154}$, which is far below machine precision for FP32 (approximately $10^{-38}$) or even FP64 (approximately $10^{-308}$). The gradient effectively becomes exactly zero after about 130 time steps in FP32 or 1,000 time steps in FP64. This means a vanilla RNN cannot learn any dependencies spanning more than 130 tokens when using FP32 arithmetic, regardless of optimization algorithm or learning rate. The mathematical structure of the recurrence fundamentally limits the learnable dependency length.</p>

<p>For comparison, consider the gradient flow in a transformer with the same dimensions. The self-attention mechanism computes attention scores $\mA = \text{softmax}(\frac{\mQ\mK\transpose}{\sqrt{d_k}})$ and outputs $\mO = \mA\mV$. The gradient $\frac{\partial L}{\partial \mV}$ flows directly from the output through the attention weights, without any multiplicative accumulation across time steps. The gradient magnitude remains approximately constant regardless of sequence length, enabling transformers to learn dependencies spanning thousands of tokens. This fundamental difference in gradient flow explains why transformers replaced RNNs for nearly all sequence modeling tasks: they solve the vanishing gradient problem by design.</p>

<p>The LSTM architecture addresses vanishing gradients through its cell state mechanism, which provides an additive path for gradient flow. The cell state update $\mathbf{c}_t = \vf_t \odot \mathbf{c}_{t-1} + \vi_t \odot \tilde{\mathbf{c}}_t$ includes an additive term rather than purely multiplicative updates. The gradient with respect to $\mathbf{c}_{t-1}$ is:
<div class="equation">
$$
\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}} = \text{diag}(\vf_t)
$$
</div>
which is a diagonal matrix with entries in $(0, 1)$ determined by the forget gate. If the forget gate learns to output values close to 1 for important information, the gradient can flow backward through many time steps without vanishing. However, this requires the network to learn appropriate forget gate values, and in practice, LSTMs still struggle with dependencies beyond 100-200 time steps. The cell state provides a highway for gradients, but it does not eliminate the vanishing gradient problem entirely.</p>

<h2>Long Short-Term Memory (LSTM)</h2>

<div class="definition"><strong>Definition:</strong> 
LSTM uses gating mechanisms to control information flow:
<div class="equation">
$$\begin{align}
\vf_t &= \sigma(\mW_f[\vh_{t-1}, \vx_t] + \vb_f) && \text{(forget gate)} \\
\vi_t &= \sigma(\mW_i[\vh_{t-1}, \vx_t] + \vb_i) && \text{(input gate)} \\
\tilde{\mathbf{c}}_t &= \tanh(\mW_c[\vh_{t-1}, \vx_t] + \vb_c) && \text{(candidate cell)} \\
\mathbf{c}_t &= \vf_t \odot \mathbf{c}_{t-1} + \vi_t \odot \tilde{\mathbf{c}}_t && \text{(cell state)} \\
\vo_t &= \sigma(\mW_o[\vh_{t-1}, \vx_t] + \vb_o) && \text{(output gate)} \\
\vh_t &= \vo_t \odot \tanh(\mathbf{c}_t) && \text{(hidden state)}
\end{align}$$
</div>
where $\sigma$ is sigmoid, $\odot$ is element-wise multiplication, and $[\cdot, \cdot]$ is concatenation.
</div>

<p><strong>Key components:</strong>
<ul>
    <li><strong>Cell state $\mathbf{c}_t$:</strong> Long-term memory, flows with minimal modification
    <li><strong>Forget gate $\vf_t$:</strong> What to remove from cell state
    <li><strong>Input gate $\vi_t$:</strong> What new information to store
    <li><strong>Output gate $\vo_t$:</strong> What to output from cell state
</ul>

<div class="example"><strong>Example:</strong> 
For input dimension $d=512$ and hidden dimension $h=1024$:

<p>Each gate has weight matrix for $[\vh_{t-1}, \vx_t] \in \R^{h+d}$:
<div class="equation">
$$\begin{align}
\text{Single gate:} \quad &(h+d) \times h + h = (1024 + 512) \times 1024 + 1024 \\
&= 1{,}572{,}864 + 1{,}024 = 1{,}573{,}888
\end{align}$$
</div>

<p>LSTM has 4 gates (forget, input, cell, output):
<div class="equation">
$$
\text{Total:} \quad 4 \times 1{,}573{,}888 = 6{,}295{,}552 \text{ parameters}
$$
</div>

<p>Compare to transformer attention with same dimensions: often fewer parameters and better parallelization!
</div>

<h3>LSTM Computational Analysis</h3>

<p>Understanding the computational cost of LSTMs is essential for comparing them to transformers and explaining why transformers have become dominant despite LSTMs' theoretical advantages for sequential processing. The LSTM's gating mechanisms provide powerful modeling capabilities but come with significant computational overhead that limits their efficiency on modern hardware.</p>

<p>For an LSTM with input dimension $d$ and hidden dimension $h$, each time step requires computing four gates (forget, input, candidate, output), each involving a matrix multiplication with the concatenated input $[\vh_{t-1}, \vx_t] \in \R^{h+d}$. The computational cost per time step is:
<div class="equation">
$$
\text{FLOPs per step} = 4 \times 2h(h+d) = 8h(h+d)
$$
</div>
where the factor of 2 accounts for multiply-accumulate operations, and the factor of 4 accounts for the four gates. For BERT-base dimensions with $d = h = 768$, this gives $8 \times 768 \times (768 + 768) = 9{,}437{,}184$ FLOPs per time step. For a sequence of length $n = 512$, the total cost is $512 \times 9{,}437{,}184 = 4{,}831{,}838{,}208$ FLOPs, or approximately 4.8 GFLOPs.</p>

<p>This computational cost is deceptively modest compared to transformers. A single transformer layer with the same dimensions requires approximately 12.9 GFLOPs for self-attention (with $n = 512$) plus 9.4 GFLOPs for the feed-forward network, totaling 22.3 GFLOPs‚Äîabout 4.6√ó more than the LSTM. However, this comparison is misleading because it ignores the critical difference in parallelization: the transformer can process all 512 positions simultaneously, while the LSTM must process them sequentially.</p>

<p>The sequential nature of LSTMs means that the 4.8 GFLOPs cannot be parallelized across time steps. On an NVIDIA A100 GPU with peak throughput of 312 TFLOPS (FP16), the theoretical minimum time to process a sequence of length 512 is $\frac{4.8 \times 10^9}{312 \times 10^{12}} = 15.4$ microseconds if we could achieve perfect parallelization. However, the sequential dependency forces us to process one time step at a time, with each step taking approximately $\frac{9.4 \times 10^6}{312 \times 10^{12}} = 0.03$ microseconds at peak throughput. In practice, small matrix multiplications achieve only 1-5\% of peak throughput due to insufficient parallelism, so each time step actually takes approximately 1-3 microseconds, giving a total sequence processing time of 512-1,536 microseconds (0.5-1.5 milliseconds).</p>

<p>For comparison, a transformer layer can process the entire sequence in parallel. The self-attention computation requires three matrix multiplications ($\mQ = \mX\mW_Q$, $\mK = \mX\mW_K$, $\mV = \mX\mW_V$) with dimensions $512 \times 768 \times 768$, followed by the attention score computation $\mA = \text{softmax}(\frac{\mQ\mK\transpose}{\sqrt{d_k}})$ and output computation $\mO = \mA\mV$. These operations can be batched into large matrix multiplications that achieve 40-60\% of peak GPU throughput, completing in approximately 50-100 microseconds total. The transformer is 5-30√ó faster than the LSTM despite having more FLOPs, purely due to better parallelization.</p>

<p>The memory requirements for LSTM hidden states are modest compared to transformer attention matrices. For batch size $B$ and sequence length $n$, the LSTM must store hidden states $\vh_t \in \R^{B \times h}$ and cell states $\mathbf{c}_t \in \R^{B \times h}$ for each time step, requiring $2Bnh \times 4 = 8Bnh$ bytes in FP32. For BERT-base dimensions with $B = 32$, $n = 512$, $h = 768$, this totals $8 \times 32 \times 512 \times 768 = 100{,}663{,}296$ bytes, or approximately 96 MB. This is substantially less than the 384 MB required for transformer attention scores in a single layer, making LSTMs more memory-efficient for long sequences.</p>

<p>However, this memory advantage is offset by the sequential processing requirement. While transformers can trade memory for speed by using gradient checkpointing (recomputing activations during the backward pass rather than storing them), LSTMs cannot benefit from this technique as effectively because the sequential dependency prevents parallelization of the recomputation. Gradient checkpointing reduces transformer memory by 3-5√ó with only 20-30\% slowdown, but for LSTMs, the slowdown is 2-3√ó because the recomputation cannot be parallelized. This makes gradient checkpointing less attractive for LSTMs, limiting their ability to scale to very long sequences.</p>

<h2>Gated Recurrent Unit (GRU)</h2>

<div class="definition"><strong>Definition:</strong> 
GRU simplifies LSTM by merging cell and hidden states:
<div class="equation">
$$\begin{align}
\vz_t &= \sigma(\mW_z[\vh_{t-1}, \vx_t] + \vb_z) && \text{(update gate)} \\
\vr_t &= \sigma(\mW_r[\vh_{t-1}, \vx_t] + \vb_r) && \text{(reset gate)} \\
\tilde{\vh}_t &= \tanh(\mW_h[\vr_t \odot \vh_{t-1}, \vx_t] + \vb_h) && \text{(candidate)} \\
\vh_t &= (1 - \vz_t) \odot \vh_{t-1} + \vz_t \odot \tilde{\vh}_t && \text{(hidden state)}
\end{align}$$
</div>
</div>

<p><strong>Advantages over LSTM:</strong>
<ul>
    <li>Fewer parameters (3 gates vs 4)
    <li>Simpler architecture
    <li>Often similar performance
    <li>Faster training
</ul>

<h2>Bidirectional RNNs</h2>

<div class="definition"><strong>Definition:</strong> 
Process sequence in both directions:
<div class="equation">
$$\begin{align}
\overrightarrow{\vh}_t &= \text{RNN}_{\text{forward}}(\vx_t, \overrightarrow{\vh}_{t-1}) \\
\overleftarrow{\vh}_t &= \text{RNN}_{\text{backward}}(\vx_t, \overleftarrow{\vh}_{t+1}) \\
\vh_t &= [\overrightarrow{\vh}_t; \overleftarrow{\vh}_t]
\end{align}$$
</div>
</div>

<p>Bidirectional RNNs capture context from both past and future, useful when entire sequence is available (not for online/causal tasks).</p>

<p><strong>Example:</strong> BERT uses bidirectional transformers (attention, not RNN), capturing full context.</p>

<h2>RNN Applications</h2>

<p><strong>Sequence-to-Sequence:</strong>
<ul>
    <li>Machine translation: Encoder RNN $\to$ Decoder RNN
    <li>Text summarization
    <li>Speech recognition
</ul>

<p><strong>Sequence Labeling:</strong>
<ul>
    <li>Part-of-speech tagging
    <li>Named entity recognition
    <li>Output at each time step
</ul>

<p><strong>Sequence Generation:</strong>
<ul>
    <li>Language modeling
    <li>Music generation
    <li>Sample from output distribution
</ul>

<h2>RNNs vs Transformers: A Computational Comparison</h2>

<p>The transition from RNNs to transformers represents one of the most significant architectural shifts in deep learning history. While RNNs were the dominant architecture for sequence modeling from the 1990s through 2017, transformers have almost entirely replaced them for natural language processing tasks. Understanding the computational, memory, and hardware efficiency differences between these architectures explains this dramatic shift and provides insight into modern deep learning system design.</p>

<h3>Sequential vs Parallel Computation</h3>

<p>The fundamental difference between RNNs and transformers lies in their computational structure. RNNs process sequences sequentially, computing $\vh_t$ from $\vh_{t-1}$ and $\vx_t$ at each time step. This sequential dependency means that position $t$ cannot be computed until position $t-1$ is complete, preventing parallelization across time steps. For a sequence of length $n$, the RNN requires $n$ sequential operations, each taking time $T_{\text{step}}$, for a total time of $nT_{\text{step}}$. Even with infinite computational resources, this sequential bottleneck cannot be overcome.</p>

<p>Transformers, by contrast, compute all positions simultaneously using self-attention. The query, key, and value projections $\mQ = \mX\mW_Q$, $\mK = \mX\mW_K$, $\mV = \mX\mW_V$ are batched matrix multiplications that process all $n$ positions in parallel. The attention scores $\mA = \text{softmax}(\frac{\mQ\mK\transpose}{\sqrt{d_k}})$ and outputs $\mO = \mA\mV$ similarly operate on all positions simultaneously. The total computation time is independent of sequence length in terms of sequential depth‚Äîall positions are processed in a constant number of parallel operations.</p>

<p>For concrete comparison, consider processing a sequence of length $n = 512$ with model dimension $d = 768$ (BERT-base dimensions) on an NVIDIA A100 GPU. An LSTM requires 512 sequential steps, each taking approximately 1-3 microseconds (as computed in Section~[ref]), for a total time of 512-1,536 microseconds (0.5-1.5 milliseconds). A transformer layer processes the entire sequence in approximately 50-100 microseconds‚Äî5-30√ó faster despite having more total FLOPs. The speedup comes entirely from parallelization: the transformer exploits the GPU's 6,912 CUDA cores to process all 512 positions simultaneously, while the LSTM can only process one position at a time.</p>

<p>This parallelization advantage scales with sequence length. For $n = 2048$ (GPT-2's maximum length), the LSTM time increases to 2-6 milliseconds, while the transformer time increases to approximately 200-400 microseconds‚Äîstill 5-30√ó faster. For $n = 4096$, the LSTM requires 4-12 milliseconds, while the transformer requires 800-1,600 microseconds. The transformer's advantage is consistent across sequence lengths because both architectures scale linearly with $n$ in terms of FLOPs, but the transformer can parallelize while the LSTM cannot.</p>

<h3>Memory Complexity: $O(nd)$ vs $O(n^2)$</h3>

<p>The memory requirements of RNNs and transformers scale differently with sequence length, leading to different bottlenecks for long sequences. RNNs store hidden states $\vh_t \in \R^{B \times d}$ for each time step, requiring $O(Bnd)$ memory for a batch of size $B$ with sequence length $n$ and hidden dimension $d$. This linear scaling with sequence length makes RNNs memory-efficient for long sequences: doubling the sequence length doubles the memory requirement.</p>

<p>Transformers store attention score matrices $\mA \in \R^{B \times h \times n \times n}$ for each layer, requiring $O(Bhn^2)$ memory where $h$ is the number of attention heads. This quadratic scaling with sequence length makes transformers memory-intensive for long sequences: doubling the sequence length quadruples the memory requirement. For BERT-base with $B = 32$, $h = 12$, $n = 512$, attention scores require 384 MB per layer, or 4.6 GB across 12 layers. For $n = 2048$, this increases to 6.1 GB per layer, or 73.7 GB across 12 layers‚Äîexceeding the memory of most GPUs.</p>

<p>However, this comparison is incomplete because it ignores the different memory access patterns. RNNs must load the recurrence matrix $\mW_{hh} \in \R^{d \times d}$ from memory at each time step, requiring $n$ memory loads of size $d^2$. For BERT-base dimensions with $d = 768$ and $n = 512$, this totals $512 \times 768^2 \times 4 = 1{,}207{,}959{,}552$ bytes, or approximately 1.15 GB of memory bandwidth. Transformers load the attention weight matrices $\mW_Q, \mW_K, \mW_V \in \R^{d \times d}$ once per layer, requiring $3d^2$ memory loads. For the same dimensions, this totals $3 \times 768^2 \times 4 = 7{,}077{,}888$ bytes, or approximately 6.75 MB‚Äî170√ó less memory bandwidth than the LSTM.</p>

<p>The memory bandwidth difference is critical for understanding hardware efficiency. Modern GPUs have memory bandwidth of 1-2 TB/s (e.g., A100 has 1.6 TB/s), which limits the rate at which data can be loaded from GPU memory to compute units. For the LSTM, loading 1.15 GB of weights requires $\frac{1.15 \text{ GB}}{1.6 \text{ TB/s}} = 0.72$ milliseconds, which is comparable to the 0.5-1.5 milliseconds of compute time. This means the LSTM is memory-bandwidth-bound: the GPU spends as much time loading weights as performing computation, achieving only 50\% efficiency. For the transformer, loading 6.75 MB requires $\frac{6.75 \text{ MB}}{1.6 \text{ TB/s}} = 4.2$ microseconds, which is negligible compared to the 50-100 microseconds of compute time. The transformer is compute-bound, achieving 90-95\% efficiency.</p>

<h3>Training Time Comparison</h3>

<p>The combined effects of parallelization and memory bandwidth lead to dramatic differences in training time between RNNs and transformers. For BERT-base training on the BookCorpus and English Wikipedia datasets (approximately 3.3 billion words, or 16 billion tokens with WordPiece tokenization), the original BERT paper reports training time of 4 days on 16 Cloud TPU chips (64 TPU cores total). Each TPU core has peak throughput of 45 TFLOPS (FP16), giving a total of 2,880 TFLOPS across 64 cores.</p>

<p>To train an LSTM with equivalent capacity (110 million parameters) on the same dataset, we can estimate the training time based on the computational and memory bandwidth analysis above. An LSTM with hidden dimension $d = 768$ has approximately $4 \times 768 \times (768 + 768) = 4{,}718{,}592$ parameters per layer (4 gates, each with weight matrix for concatenated input). To reach 110 million parameters, we need approximately $\frac{110{,}000{,}000}{4{,}718{,}592} \approx 23$ layers. Each layer requires 4.8 GFLOPs per sequence of length 512, so 23 layers require 110.4 GFLOPs per sequence.</p>

<p>For 16 billion tokens with sequence length 512, we have $\frac{16 \times 10^9}{512} = 31{,}250{,}000$ sequences. At 110.4 GFLOPs per sequence, the total computation is $31{,}250{,}000 \times 110.4 \times 10^9 = 3.45 \times 10^{18}$ FLOPs, or 3.45 exaFLOPs. At 2,880 TFLOPS peak throughput, this would take $\frac{3.45 \times 10^{18}}{2{,}880 \times 10^{12}} = 1{,}197{,}917$ seconds, or approximately 14 days, if we could achieve 100\% hardware utilization.</p>

<p>However, as discussed above, LSTMs achieve only 1-5\% of peak throughput due to sequential processing and memory bandwidth limitations. At 3\% utilization, the training time increases to $\frac{14 \text{ days}}{0.03} = 467$ days‚Äîmore than a year. Even with aggressive optimizations, LSTM training would likely require 100-200 days on the same hardware that trains BERT in 4 days. This 25-50√ó slowdown makes LSTMs impractical for large-scale pretraining, explaining why transformers have completely replaced RNNs for modern language models.</p>

<p>The training time difference becomes even more extreme for larger models. GPT-3 with 175 billion parameters was trained on 300 billion tokens in approximately 34 days on 10,000 NVIDIA V100 GPUs. An LSTM with equivalent capacity would require an estimated 3-5 years on the same hardware, making it economically infeasible. The transformer's parallelization advantage is the primary enabler of large-scale language model pretraining.</p>

<h3>Hardware Limitations of RNNs</h3>

<p>The poor hardware efficiency of RNNs stems from three fundamental limitations: insufficient parallelism for GPU saturation, memory bandwidth bottlenecks, and poor cache utilization. Modern GPUs are designed for massively parallel workloads with thousands of concurrent operations, but RNNs provide only batch-level parallelism. For batch size $B = 32$ and hidden dimension $d = 768$, the LSTM matrix multiplication $\mW[\vh_{t-1}, \vx_t]$ has dimensions $768 \times (32 \times 1536)$, which can process only 32 rows in parallel. An A100 GPU with 6,912 CUDA cores can theoretically process 6,912 operations simultaneously, but the LSTM provides only 32 concurrent operations‚Äîleaving 99.5\% of the GPU idle.</p>

<p>Increasing batch size improves GPU utilization but has diminishing returns. At batch size 256, the LSTM achieves approximately 5-10\% of peak throughput. At batch size 2048, it reaches 15-20\% of peak throughput. However, batch sizes beyond 256 are impractical for most training scenarios due to memory constraints and optimization difficulties. Even at batch size 2048, the LSTM achieves only 15-20\% utilization, compared to 40-60\% for transformers at the same batch size. The sequential dependency fundamentally limits the LSTM's ability to exploit GPU parallelism.</p>

<p>The memory bandwidth bottleneck exacerbates the parallelism problem. As computed above, the LSTM requires 1.15 GB of memory bandwidth per sequence, compared to 6.75 MB for the transformer‚Äîa 170√ó difference. For batch size 32, the LSTM requires $32 \times 1.15 = 36.8$ GB of memory bandwidth, which takes $\frac{36.8 \text{ GB}}{1.6 \text{ TB/s}} = 23$ milliseconds to load. The actual compute time is approximately 16-48 milliseconds (0.5-1.5 milliseconds per sequence √ó 32 sequences), so memory bandwidth accounts for 30-60\% of the total time. The transformer requires $32 \times 6.75 = 216$ MB of memory bandwidth, taking only 0.135 milliseconds to load‚Äînegligible compared to the 1.6-3.2 milliseconds of compute time.</p>

<p>Cache utilization is similarly poor for RNNs. Modern GPUs have L1 cache (128 KB per streaming multiprocessor) and L2 cache (40 MB for A100) that can dramatically accelerate memory access for data that fits in cache. The transformer's attention computation reuses the query, key, and value matrices across all positions, enabling effective cache utilization. For BERT-base, the $\mQ, \mK, \mV$ matrices have size $32 \times 512 \times 768 \times 4 = 50.3$ MB, which fits in L2 cache. The LSTM's recurrence matrix $\mW_{hh}$ has size $768 \times 768 \times 4 = 2.36$ MB, which also fits in cache, but it must be loaded at each time step, and the sequential dependency prevents batching these loads. The cache hit rate for LSTMs is typically 20-40\%, compared to 60-80\% for transformers, further reducing hardware efficiency.</p>

<h3>Why Transformers Replaced RNNs</h3>

<p>The dominance of transformers over RNNs can be summarized in three key advantages: parallelization, gradient flow, and hardware efficiency. The parallel computation structure of transformers enables 5-30√ó faster training than RNNs on modern GPUs, making large-scale pretraining feasible. The direct attention connections enable gradient flow across arbitrary distances without vanishing, allowing transformers to learn dependencies spanning thousands of tokens. The high GPU utilization (40-60\% vs 1-5\% for RNNs) makes transformers 10-60√ó more hardware-efficient, reducing training costs proportionally.</p>

<p>These advantages compound: the 5-30√ó speedup from parallelization, combined with the 10-60√ó improvement in hardware efficiency, yields an overall 50-1,800√ó advantage for transformers. In practice, transformers train 100-500√ó faster than RNNs for equivalent model capacity and dataset size. This speedup difference is the primary reason why transformers have completely replaced RNNs for natural language processing: the economic cost of training an LSTM-based language model is prohibitive compared to a transformer.</p>

<p>However, RNNs retain advantages for specific use cases. For online or streaming applications where inputs arrive sequentially and outputs must be produced in real-time, RNNs can process each input immediately without waiting for the full sequence. Transformers require the entire sequence to compute attention, making them unsuitable for true streaming applications. For very long sequences exceeding 10,000 tokens, the $O(n^2)$ memory complexity of transformers becomes prohibitive, while RNNs' $O(n)$ memory scaling remains manageable. For edge deployment on devices with limited memory and compute, RNNs' smaller memory footprint can be advantageous.</p>

<p>Despite these niche advantages, transformers dominate modern deep learning due to their superior training efficiency and scalability. The development of efficient attention mechanisms (Chapter 16) addresses the $O(n^2)$ memory bottleneck for long sequences, and techniques like streaming transformers enable online processing. The architectural innovations that made transformers successful‚Äîparallel computation, direct gradient flow, and hardware efficiency‚Äîrepresent fundamental advances that are unlikely to be superseded by sequential architectures.</p>

<div class="keypoint">
While RNNs were dominant for sequences, transformers now excel in most NLP tasks due to: (1) Better parallelization enabling 5-30√ó faster training, (2) Direct long-range dependencies via attention avoiding vanishing gradients, (3) Superior hardware efficiency achieving 40-60\% GPU utilization vs 1-5\% for RNNs. The combined effect is 100-500√ó faster training, making transformers economically superior for large-scale pretraining. RNNs remain useful only for online/streaming tasks and extremely long sequences where $O(n^2)$ memory is prohibitive.
</div>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> For vanilla RNN with input dim $d=128$, hidden dim $h=256$, and sequence length $T=50$: (1) Count total parameters in $\mW_{hh}$, $\mW_{xh}$, and $\mW_{hy}$, (2) Compute total FLOPs for forward pass through all time steps, (3) Estimate GPU utilization on an A100 (312 TFLOPS peak) with batch size 32, assuming each time step achieves 2\% of peak throughput. Why is utilization so low?
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Derive the gradient $\frac{\partial L}{\partial \mW_{hh}}$ for a 3-step sequence. Show how the gradient involves products of Jacobians $\frac{\partial \vh_t}{\partial \vh_{t-1}}$. If $\norm{\mW_{hh}} = 0.9$ and $\tanh'$ averages 0.5, compute the gradient magnitude decay factor from time step 3 to time step 0. At what sequence length would gradients vanish below FP32 precision ($10^{-38}$)?
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> Compare parameter counts and FLOPs per sequence for: (1) LSTM with $d=512$, $h=512$, $n=512$, (2) GRU with $d=512$, $h=512$, $n=512$, (3) Transformer attention layer with $d_{\text{model}}=512$, $d_k=64$, $h=8$ heads, $n=512$. Which architecture has the most parameters? Which has the most FLOPs? Which achieves the highest GPU utilization and why?
</div>

<div class="exercise" id="exercise-4"><strong>Exercise 4:</strong> Implement bidirectional LSTM in PyTorch for sequence "The cat sat on the mat" with vocabulary size 10, embedding dim 16, hidden dim 32. Process the sequence and show output dimensions. Compute the total memory required for hidden states and cell states in FP32. How does this compare to the memory required for attention scores in a transformer with the same dimensions?
</div>

<div class="exercise" id="exercise-5"><strong>Exercise 5:</strong> For BERT-base dimensions ($d=768$, $n=512$), compute: (1) Memory required for LSTM hidden and cell states across 12 layers with batch size 32, (2) Memory required for transformer attention scores across 12 layers with batch size 32 and 12 attention heads, (3) The sequence length at which LSTM memory equals transformer memory. Explain why transformers are memory-limited for long sequences while LSTMs are compute-limited.
</div>

<div class="exercise" id="exercise-6"><strong>Exercise 6:</strong> Estimate the training time for a 110M parameter LSTM on 16 billion tokens (sequence length 512) using 64 TPU cores with 2,880 TFLOPS total peak throughput. Assume the LSTM achieves 3\% of peak throughput due to sequential processing. Compare this to BERT-base training time of 4 days on the same hardware. What is the speedup factor? Explain the three main reasons for the difference: parallelization, memory bandwidth, and GPU utilization.
</div>

<h2>Solutions</h2>

<div class="solution"><strong>Solution:</strong> For vanilla RNN with $d=128$, $h=256$, $T=50$:

<p><strong>(1) Parameter count:</strong>
<ul>
    <li>$\mW_{xh} \in \R^{h \times d}$: $256 \times 128 = 32{,}768$ parameters
    <li>$\mW_{hh} \in \R^{h \times h}$: $256 \times 256 = 65{,}536$ parameters
    <li>$\mW_{hy} \in \R^{V \times h}$: Assuming vocabulary $V=10{,}000$: $10{,}000 \times 256 = 2{,}560{,}000$ parameters
    <li>Biases: $h + h + V = 256 + 256 + 10{,}000 = 10{,}512$ parameters
    <li>Total: $32{,}768 + 65{,}536 + 2{,}560{,}000 + 10{,}512 = 2{,}668{,}816$ parameters
</ul>

<p><strong>(2) FLOPs for forward pass:</strong>
Per time step:
<ul>
    <li>$\mW_{xh}\vx_t$: $2 \times h \times d = 2 \times 256 \times 128 = 65{,}536$ FLOPs
    <li>$\mW_{hh}\vh_{t-1}$: $2 \times h \times h = 2 \times 256 \times 256 = 131{,}072$ FLOPs
    <li>$\tanh$ activation: $h = 256$ FLOPs
    <li>Per time step total: $\approx 196{,}864$ FLOPs
</ul>

<p>For $T=50$ time steps: $50 \times 196{,}864 = 9{,}843{,}200 \approx 9.8$ MFLOPs</p>

<p><strong>(3) GPU utilization with batch size 32:</strong>
<ul>
    <li>Total FLOPs per batch: $32 \times 9.8 \text{ MFLOPs} = 313.6$ MFLOPs
    <li>At 2\% peak throughput: $0.02 \times 312 \text{ TFLOPS} = 6.24$ TFLOPS
    <li>Time per batch: $\frac{313.6 \text{ MFLOPs}}{6.24 \text{ TFLOPS}} = 0.05$ ms
</ul>

<p><strong>Why utilization is so low:</strong>
<ul>
    <li>Sequential dependency: Each time step depends on previous, preventing parallelization
    <li>Small matrix operations: $256\times256$ matrices don't saturate GPU
    <li>Memory-bound: Constantly loading/storing hidden states
    <li>Low arithmetic intensity: Few operations per memory access
    <li>Kernel launch overhead dominates for small operations
</ul>
</div>

<div class="solution"><strong>Solution:</strong> For 3-step RNN sequence, the gradient involves backpropagation through time (BPTT):

<p><strong>Forward pass:</strong>
<div class="equation">
$$\begin{align}
\vh_1 &= \tanh(\mW_{hh}\vh_0 + \mW_{xh}\vx_1) \\
\vh_2 &= \tanh(\mW_{hh}\vh_1 + \mW_{xh}\vx_2) \\
\vh_3 &= \tanh(\mW_{hh}\vh_2 + \mW_{xh}\vx_3)
\end{align}$$
</div>

<p><strong>Gradient derivation:</strong>
<div class="equation">
$$\begin{align}
\frac{\partial L}{\partial \mW_{hh}} &= \sum_{t=1}^{3} \frac{\partial L}{\partial \vh_t} \frac{\partial \vh_t}{\partial \mW_{hh}}
\end{align}$$
</div>

<p>The gradient involves products of Jacobians:
<div class="equation">
$$
\frac{\partial \vh_t}{\partial \vh_{t-1}} = \text{diag}(\tanh'(\vz_t)) \mW_{hh}
$$
</div>

<p><strong>Gradient magnitude decay:</strong>
With $\norm{\mW_{hh}} = 0.9$ and $\tanh'$ averaging 0.5:
<div class="equation">
$$
\norm{\frac{\partial \vh_t}{\partial \vh_{t-1}}} \approx 0.5 \times 0.9 = 0.45
$$
</div>

<p>From time step 3 to 0 (3 steps back):
<div class="equation">
$$
\text{Decay factor} = 0.45^3 \approx 0.091
$$
</div>

<p><strong>Vanishing gradient threshold:</strong>
For gradients to vanish below $10^{-38}$:
<div class="equation">
$$\begin{align}
0.45^T &< 10^{-38} \\
T &> \frac{-38 \log(10)}{\log(0.45)} \approx 110 \text{ steps}
\end{align}$$
</div>

<p>Gradients vanish below FP32 precision after approximately 110 time steps.
</div>

<div class="solution"><strong>Solution:</strong> For $d=512$, $h=512$, $n=512$:

<p><strong>(1) LSTM:</strong> $2{,}099{,}200$ parameters, $2.15$ GFLOPs</p>

<p><strong>(2) GRU:</strong> $1{,}574{,}400$ parameters, $1.61$ GFLOPs</p>

<p><strong>(3) Transformer:</strong> $1{,}048{,}576$ parameters, $1.61$ GFLOPs</p>

<p><strong>Most parameters:</strong> LSTM (2.1M)</p>

<p><strong>Most FLOPs:</strong> LSTM (2.15 GFLOPs)</p>

<p><strong>Highest GPU utilization:</strong> Transformer (40-60\% vs 2-5\% for RNNs) due to full parallelization across sequence length and large matrix operations.
</div>

<div class="solution"><strong>Solution:</strong> For bidirectional LSTM with 6 tokens, embedding dim 16, hidden dim 32:

<p><strong>Output dimensions:</strong> $6 \times 64$ (concatenated forward and backward)</p>

<p><strong>Memory for LSTM states:</strong>
<ul>
    <li>Forward hidden + cell: $2 \times 6 \times 32 \times 4 = 1{,}536$ bytes
    <li>Backward hidden + cell: $1{,}536$ bytes
    <li>Total: $3{,}072$ bytes $\approx 3$ KB
</ul>

<p><strong>Transformer attention scores (8 heads):</strong> $8 \times 6 \times 6 \times 4 = 1{,}152$ bytes</p>

<p>LSTM requires 2.7√ó more memory for this small sequence, but attention scales as $O(n^2)$ vs $O(n)$ for LSTM.
</div>

<div class="solution"><strong>Solution:</strong> For BERT-base ($d=768$, $n=512$), batch size 32:

<p><strong>(1) LSTM memory (12 layers):</strong> $\approx 1.13$ GB</p>

<p><strong>(2) Transformer attention (12 layers):</strong> $\approx 4.5$ GB</p>

<p><strong>(3) Equal memory at:</strong> $n = 128$ tokens</p>

<p>For $n>128$, transformers use more memory due to $O(n^2)$ attention scores. Transformers are memory-limited for long sequences, while LSTMs are compute-limited due to sequential processing.
</div>

<div class="solution"><strong>Solution:</strong> <strong>LSTM training time:</strong> $\approx 8.4$ days

<p><strong>BERT training time:</strong> 4 days</p>

<p><strong>Speedup:</strong> 2.1√ó (BERT is faster)</p>

<p><strong>Three main reasons:</strong>
<ol>
    <li>Parallelization: BERT processes all tokens in parallel ($\approx 10\times$ speedup)
    <li>Memory bandwidth: BERT has higher arithmetic intensity ($\approx 3\times$ better)
    <li>GPU utilization: BERT achieves 40-60\% vs 2-5\% for LSTM ($\approx 17\times$ better)
</ol>
</div>
        
        <div class="chapter-nav">
  <a href="chapter05_convolutional_networks.html">‚Üê Chapter 5: Convolutional Neural Networks</a>
  <a href="../index.html">üìö Table of Contents</a>
  <a href="chapter07_attention_fundamentals.html">Chapter 7: Attention Mechanisms: Fundamentals ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
