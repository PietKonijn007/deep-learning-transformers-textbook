<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Calculus and Optimization - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../css/style.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                vh: '{\\mathbf{h}}',
                vw: '{\\mathbf{w}}',
                vb: '{\\mathbf{b}}',
                vq: '{\\mathbf{q}}',
                vk: '{\\mathbf{k}}',
                vv: '{\\mathbf{v}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mQ: '{\\mathbf{Q}}',
                mK: '{\\mathbf{K}}',
                mV: '{\\mathbf{V}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mU: '{\\mathbf{U}}',
                mM: '{\\mathbf{M}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
    </nav>

    <main>
        <h1>Calculus and Optimization</h1>

<h2>Chapter Overview</h2>

<p>Training deep learning models requires optimizing complex, high-dimensional functions. This chapter develops the calculus and optimization theory necessary to understand how neural networks learn from data. We cover multivariable calculus, gradient computation, and the optimization algorithms that power modern deep learning.</p>

<p>The centerpiece of this chapter is backpropagation, the algorithm that efficiently computes gradients in neural networks. We derive backpropagation from first principles, showing how the chain rule enables gradient computation through arbitrarily deep computational graphs. We then explore gradient descent and its variants, which use these gradients to iteratively improve model parameters.</p>

<h3>Learning Objectives</h3>

<p>After completing this chapter, you will be able to:</p>

<ol>
    <li>Compute gradients and Jacobians for multivariable functions
    <li>Apply the chain rule to composite functions
    <li>Understand and implement the backpropagation algorithm
    <li>Implement gradient descent and its variants (SGD, momentum, Adam)
    <li>Analyze convergence properties of optimization algorithms
    <li>Apply learning rate schedules and regularization techniques
</ol>

<h2>Multivariable Calculus</h2>

<h3>Partial Derivatives</h3>

<div class="definition"><strong>Definition:</strong> 
For function $f: \R^n \to \R$, the <strong>partial derivative</strong> with respect to $x_i$ is:
<div class="equation">
$$
\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i + h, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}
$$
</div>
</div>

<div class="example"><strong>Example:</strong> 
For $f(x_1, x_2) = x_1^2 + 3x_1 x_2 + x_2^2$:
<div class="equation">
$$\begin{align}
\frac{\partial f}{\partial x_1} &= 2x_1 + 3x_2 \\
\frac{\partial f}{\partial x_2} &= 3x_1 + 2x_2
\end{align}$$
</div>

<p>At point $(x_1, x_2) = (1, 2)$:
<div class="equation">
$$\begin{align}
\frac{\partial f}{\partial x_1}\bigg|_{(1,2)} &= 2(1) + 3(2) = 8 \\
\frac{\partial f}{\partial x_2}\bigg|_{(1,2)} &= 3(1) + 2(2) = 7
\end{align}$$
</div>
</div>

<h3>Gradients</h3>

<div class="definition"><strong>Definition:</strong> 
For function $f: \R^n \to \R$, the <strong>gradient</strong> is the vector of partial derivatives:
<div class="equation">
$$
\nabla f(\vx) = \begin{bmatrix} 
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix} \in \R^n
$$
</div>
</div>

<p>The gradient points in the direction of steepest ascent of the function.</p>

<div class="example"><strong>Example:</strong> 
For mean squared error loss:
<div class="equation">
$$
L(\vw) = \frac{1}{N} \sum_{i=1}^N (y_i - \vw\transpose \vx^{(i)})^2
$$
</div>

<p>The gradient with respect to $\vw$ is:
<div class="equation">
$$
\nabla_{\vw} L = -\frac{2}{N} \sum_{i=1}^N (y_i - \vw\transpose \vx^{(i)}) \vx^{(i)}
$$
</div>

<p>For $N=1$, $\vw = [w_1, w_2]\transpose$, $\vx = [1, 2]\transpose$, $y = 5$, and current prediction $\hat{y} = \vw\transpose \vx = 3$:
<div class="equation">
$$
\nabla_{\vw} L = -2(5 - 3) \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} -4 \\ -8 \end{bmatrix}
$$
</div>

<p>The negative gradient $-\nabla_{\vw} L = [4, 8]\transpose$ points toward better parameters.
</div>

<h3>The Chain Rule</h3>

<div class="theorem"><strong>Theorem:</strong> 
For composite function $h(\vx) = f(g(\vx))$ where $g: \R^n \to \R^m$ and $f: \R^m \to \R$:
<div class="equation">
$$
\frac{\partial h}{\partial x_i} = \sum_{j=1}^m \frac{\partial f}{\partial g_j} \frac{\partial g_j}{\partial x_i}
$$
</div>

<p>In vector form:
<div class="equation">
$$
\nabla_{\vx} h = \mJ_g\transpose \nabla_{\vz} f
$$
</div>
where $\vz = g(\vx)$ and $\mJ_g \in \R^{m \times n}$ is the Jacobian of $g$.
</div>

<div class="example"><strong>Example:</strong> 
For neural network layer: $\vy = \sigma(\mW\vx + \vb)$ where $\sigma$ is applied element-wise.

<p>Let $\vz = \mW\vx + \vb$ (pre-activation). Then:
<div class="equation">
$$
\frac{\partial L}{\partial \vx} = \mW\transpose \left( \frac{\partial L}{\partial \vy} \odot \sigma'(\vz) \right)
$$
</div>

<p>where $\odot$ denotes element-wise multiplication.</p>

<p><strong>Concrete example:</strong> For ReLU activation $\sigma(z) = \max(0, z)$:
<div class="equation">
$$
\sigma'(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}
$$
</div>

<p>If $\vz = [2.0, -1.0, 0.5]\transpose$, then $\sigma'(\vz) = [1, 0, 1]\transpose$.
</div>

<h3>Jacobian and Hessian Matrices</h3>

<div class="definition"><strong>Definition:</strong> 
For function $\mathbf{f}: \R^n \to \R^m$, the <strong>Jacobian matrix</strong> is:
<div class="equation">
$$
\mJ_{\mathbf{f}}(\vx) = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix} \in \R^{m \times n}
$$
</div>
</div>

<div class="definition"><strong>Definition:</strong> 
For function $f: \R^n \to \R$, the <strong>Hessian matrix</strong> contains second derivatives:
<div class="equation">
$$
\mH_f(\vx) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots \\
\vdots & \vdots & \ddots
\end{bmatrix} \in \R^{n \times n}
$$
</div>
</div>

<p>The Hessian describes the local curvature of the function. For smooth functions, $\mH$ is symmetric.</p>

<h2>Gradient Descent</h2>

<h3>The Gradient Descent Algorithm</h3>

<p>Gradient descent iteratively moves parameters in the direction opposite to the gradient:</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: Gradient Descent</div>
\KwIn{Objective function $f(\vw)$, initial parameters $\vw^{(0)}$, learning rate $\eta$, iterations $T$}
\KwOut{Optimized parameters $\vw^{(T)}$}
\For{$t = 0$ \KwTo $T-1$}{
    Compute gradient: $\mathbf{g}^{(t)} = \nabla f(\vw^{(t)})$ \\
    Update parameters: $\vw^{(t+1)} = \vw^{(t)} - \eta \mathbf{g}^{(t)}$
}
\Return{$\vw^{(T)}$}
</div>

<div class="keypoint">
The learning rate $\eta$ controls the step size. Too large: divergence. Too small: slow convergence.
</div>

<div class="example"><strong>Example:</strong> 
Minimize $f(w) = w^2$ starting from $w^{(0)} = 3$ with $\eta = 0.1$:
<div class="equation">
$$\begin{align}
t=0:& \quad w^{(0)} = 3, \quad g^{(0)} = 2w^{(0)} = 6, \quad w^{(1)} = 3 - 0.1(6) = 2.4 \\
t=1:& \quad w^{(1)} = 2.4, \quad g^{(1)} = 4.8, \quad w^{(2)} = 2.4 - 0.1(4.8) = 1.92 \\
t=2:& \quad w^{(2)} = 1.92, \quad g^{(2)} = 3.84, \quad w^{(3)} = 1.92 - 0.1(3.84) = 1.536
\end{align}$$
</div>

<p>The parameters converge to $w^* = 0$ (the minimum).
</div>

<h3>Stochastic Gradient Descent (SGD)</h3>

<p>For large datasets, computing the full gradient is expensive. SGD approximates the gradient using mini-batches.</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: Stochastic Gradient Descent (SGD)</div>
\KwIn{Dataset $\mathcal{D} = \{(\vx^{(i)}, y^{(i)})\}_{i=1}^N$, batch size $B$, learning rate $\eta$, epochs $E$}
\KwOut{Optimized parameters $\vw$}
Initialize $\vw$ randomly \\
\For{epoch $e = 1$ \KwTo $E$}{
    Shuffle dataset $\mathcal{D}$ \\
    \For{each mini-batch $\mathcal{B} \subset \mathcal{D}$ of size $B$}{
        Compute mini-batch gradient: $\mathbf{g} = \frac{1}{B} \sum_{(\vx, y) \in \mathcal{B}} \nabla_{\vw} L(\vw; \vx, y)$ \\
        Update: $\vw \leftarrow \vw - \eta \mathbf{g}$
    }
}
\Return{$\vw$}
</div>

<div class="implementation">
PyTorch SGD implementation:
<pre><code>import torch
import torch.nn as nn

<p># Model and loss
model = nn.Linear(10, 1)
criterion = nn.MSELoss()</p>

<p># SGD optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)</p>

<p># Training loop
for epoch in range(100):
    for x_batch, y_batch in dataloader:
        # Forward pass
        y_pred = model(x_batch)
        loss = criterion(y_pred, y_batch)</p>

<p># Backward pass
        optimizer.zero_grad()  # Clear previous gradients
        loss.backward()         # Compute gradients
        optimizer.step()        # Update parameters
</code></pre>
</div>

<h3>Momentum</h3>

<p>Momentum accelerates SGD by accumulating a velocity vector:</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: SGD with Momentum</div>
\KwIn{Learning rate $\eta$, momentum coefficient $\beta$ (typically 0.9)}
Initialize velocity $\mathbf{v} = \mathbf{0}$ \\
\For{each iteration}{
    Compute gradient $\mathbf{g} = \nabla_{\vw} L(\vw)$ \\
    Update velocity: $\mathbf{v} \leftarrow \beta \mathbf{v} + \mathbf{g}$ \\
    Update parameters: $\vw \leftarrow \vw - \eta \mathbf{v}$
}
</div>

<p>Momentum helps navigate ravines and accelerates convergence in relevant directions.</p>

<h3>Adam Optimizer</h3>

<p>Adam (Adaptive Moment Estimation) combines momentum with adaptive learning rates:</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: Adam Optimizer</div>
\KwIn{Learning rate $\alpha$ (default 0.001), $\beta_1$ = 0.9, $\beta_2$ = 0.999, $\epsilon$ = $10^{-8}$}
Initialize $\mathbf{m}_0 = \mathbf{0}$ (first moment), $\mathbf{v}_0 = \mathbf{0}$ (second moment), $t = 0$ \\
\While{not converged}{
    $t \leftarrow t + 1$ \\
    Compute gradient: $\mathbf{g}_t = \nabla_{\vw} L(\vw_{t-1})$ \\
    Update biased first moment: $\mathbf{m}_t = \beta_1 \mathbf{m}_{t-1} + (1 - \beta_1) \mathbf{g}_t$ \\
    Update biased second moment: $\mathbf{v}_t = \beta_2 \mathbf{v}_{t-1} + (1 - \beta_2) \mathbf{g}_t^2$ \\
    Bias-corrected first moment: $\hat{\mathbf{m}}_t = \mathbf{m}_t / (1 - \beta_1^t)$ \\
    Bias-corrected second moment: $\hat{\mathbf{v}}_t = \mathbf{v}_t / (1 - \beta_2^t)$ \\
    Update parameters: $\vw_t = \vw_{t-1} - \alpha \hat{\mathbf{m}}_t / (\sqrt{\hat{\mathbf{v}}_t} + \epsilon)$
}
</div>

<p>Adam is the most commonly used optimizer for training transformers and large language models.</p>

<h2>Backpropagation</h2>

<p>Backpropagation efficiently computes gradients in neural networks using the chain rule.</p>

<h3>Computational Graphs</h3>

<p>A computational graph represents the sequence of operations in a neural network. Each node is an operation, and edges carry values/gradients.</p>

<div class="example"><strong>Example:</strong> 
For $L = (y - \hat{y})^2$ where $\hat{y} = w_2 \sigma(w_1 x + b_1) + b_2$:

<p><strong>Forward pass:</strong>
<div class="equation">
$$\begin{align}
z_1 &= w_1 x + b_1 = 2.0(1.0) + 0.5 = 2.5 \\
a_1 &= \sigma(z_1) = \sigma(2.5) = 0.924 \quad \text{(sigmoid)} \\
z_2 &= w_2 a_1 + b_2 = 1.5(0.924) + 0.3 = 1.686 \\
L &= (y - z_2)^2 = (3.0 - 1.686)^2 = 1.726
\end{align}$$
</div>

<p><strong>Backward pass:</strong>
<div class="equation">
$$\begin{align}
\frac{\partial L}{\partial z_2} &= 2(z_2 - y) = 2(1.686 - 3.0) = -2.628 \\
\frac{\partial L}{\partial w_2} &= \frac{\partial L}{\partial z_2} \cdot a_1 = -2.628(0.924) = -2.428 \\
\frac{\partial L}{\partial a_1} &= \frac{\partial L}{\partial z_2} \cdot w_2 = -2.628(1.5) = -3.942 \\
\frac{\partial L}{\partial z_1} &= \frac{\partial L}{\partial a_1} \cdot \sigma'(z_1) = -3.942(0.070) = -0.276 \\
\frac{\partial L}{\partial w_1} &= \frac{\partial L}{\partial z_1} \cdot x = -0.276(1.0) = -0.276
\end{align}$$
</div>
</div>

<h3>Backpropagation Algorithm</h3>

<div class="algorithm"><div class="algorithm-title">Algorithm: Backpropagation</div>
\KwIn{Training example $(\vx, y)$, network with $L$ layers}
\KwOut{Gradients $\{\nabla_{\mW^{(\ell)}} L, \nabla_{\vb^{(\ell)}} L\}_{\ell=1}^L$}

<p>\tcp{Forward Pass}
$\vh^{(0)} = \vx$ \\
\For{$\ell = 1$ \KwTo $L$}{
    $\vz^{(\ell)} = \mW^{(\ell)} \vh^{(\ell-1)} + \vb^{(\ell)}$ \\
    $\vh^{(\ell)} = \sigma^{(\ell)}(\vz^{(\ell)})$
}
$\hat{y} = \vh^{(L)}$ \\
Compute loss: $L = \text{Loss}(y, \hat{y})$</p>

<p>\tcp{Backward Pass}
$\boldsymbol{\delta}^{(L)} = \nabla_{\vh^{(L)}} L \odot \sigma'^{(L)}(\vz^{(L)})$ \\
\For{$\ell = L$ \KwTo $1$}{
    $\nabla_{\mW^{(\ell)}} L = \boldsymbol{\delta}^{(\ell)} (\vh^{(\ell-1)})\transpose$ \\
    $\nabla_{\vb^{(\ell)}} L = \boldsymbol{\delta}^{(\ell)}$ \\
    \If{$\ell > 1$}{
        $\boldsymbol{\delta}^{(\ell-1)} = (\mW^{(\ell)})\transpose \boldsymbol{\delta}^{(\ell)} \odot \sigma'^{(\ell-1)}(\vz^{(\ell-1)})$
    }
}
\Return{All gradients}
</div>

<div class="keypoint">
Backpropagation computes gradients in $O(n)$ time where $n$ is the number of parameters, compared to $O(n^2)$ for naive methods. This efficiency enables training of billion-parameter models.
</div>

<h2>Learning Rate Schedules</h2>

<p>Learning rate schedules adjust $\eta$ during training to improve convergence.</p>

<h3>Common Schedules</h3>

<p><strong>Step Decay:</strong>
<div class="equation">
$$
\eta_t = \eta_0 \gamma^{\lfloor t/s \rfloor}
$$
</div>
where $\gamma < 1$ (e.g., 0.1) and $s$ is step size (e.g., every 10 epochs).</p>

<p><strong>Exponential Decay:</strong>
<div class="equation">
$$
\eta_t = \eta_0 e^{-\lambda t}
$$
</div>

<p><strong>Cosine Annealing:</strong>
<div class="equation">
$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t\pi}{T}\right)\right)
$$
</div>

<p><strong>Warmup + Decay (Transformers):</strong>
<div class="equation">
$$
\eta_t = \frac{d_{\text{model}}^{-0.5}}{\max(t, \text{warmup\_steps}^{-0.5})} \cdot \min(t^{-0.5}, t \cdot \text{warmup\_steps}^{-1.5})
$$
</div>

<p>The warmup phase prevents instability in early training of transformers.</p>

<h2>Exercises</h2>

<p>\begin{exercise}
Compute the gradient of $f(\vw) = \vw\transpose \mA \vw + \vb\transpose \vw + c$ where $\mA \in \R^{n \times n}$ is symmetric, $\vw, \vb \in \R^n$, and $c \in \R$.
</div>

<p>\begin{exercise}
Implement backpropagation for a 2-layer network with ReLU activation. Given input $\vx = [1.0, 0.5]\transpose$, weights $\mW^{(1)} \in \R^{3 \times 2}$, $\mW^{(2)} \in \R^{1 \times 3}$, and target $y = 2.0$, compute all gradients.
</div>

<p>\begin{exercise}
For Adam optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\alpha = 0.001$:
<ol>
    <li>Why is bias correction necessary?
    <li>What are the effective learning rates after steps $t = 1, 10, 100, 1000$?
    <li>How does Adam handle sparse gradients compared to SGD?
</ol>
</div>

<p>\begin{exercise}
A transformer is trained with learning rate warmup over 4000 steps, then inverse square root decay. If $d_{\text{model}} = 512$:
<ol>
    <li>Plot the learning rate schedule for 100,000 steps
    <li>What is the learning rate at step 1, 4000, and 10,000?
    <li>Why is warmup beneficial for transformer training?
</ol>
</div>
        
        <div class="chapter-nav">
  <a href="chapter01_linear_algebra.html">‚Üê Chapter 1: Linear Algebra for Deep Learning</a>
  <a href="../index.html">üìö Table of Contents</a>
  <a href="chapter03_probability_information.html">Chapter 3: Probability and Information Theory ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
