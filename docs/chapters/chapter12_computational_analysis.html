<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 12: Computational Analysis - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                va: '{\\mathbf{a}}',
                vb: '{\\mathbf{b}}',
                vc: '{\\mathbf{c}}',
                vd: '{\\mathbf{d}}',
                ve: '{\\mathbf{e}}',
                vf: '{\\mathbf{f}}',
                vg: '{\\mathbf{g}}',
                vh: '{\\mathbf{h}}',
                vi: '{\\mathbf{i}}',
                vj: '{\\mathbf{j}}',
                vk: '{\\mathbf{k}}',
                vl: '{\\mathbf{l}}',
                vm: '{\\mathbf{m}}',
                vn: '{\\mathbf{n}}',
                vo: '{\\mathbf{o}}',
                vp: '{\\mathbf{p}}',
                vq: '{\\mathbf{q}}',
                vr: '{\\mathbf{r}}',
                vs: '{\\mathbf{s}}',
                vt: '{\\mathbf{t}}',
                vu: '{\\mathbf{u}}',
                vv: '{\\mathbf{v}}',
                vw: '{\\mathbf{w}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mD: '{\\mathbf{D}}',
                mE: '{\\mathbf{E}}',
                mF: '{\\mathbf{F}}',
                mG: '{\\mathbf{G}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mJ: '{\\mathbf{J}}',
                mK: '{\\mathbf{K}}',
                mL: '{\\mathbf{L}}',
                mM: '{\\mathbf{M}}',
                mN: '{\\mathbf{N}}',
                mO: '{\\mathbf{O}}',
                mP: '{\\mathbf{P}}',
                mQ: '{\\mathbf{Q}}',
                mR: '{\\mathbf{R}}',
                mS: '{\\mathbf{S}}',
                mT: '{\\mathbf{T}}',
                mU: '{\\mathbf{U}}',
                mV: '{\\mathbf{V}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mZ: '{\\mathbf{Z}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../../deeptech.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
        <a href="chapter24_domain_specific_models.html">Ch 24</a>
        <a href="chapter25_enterprise_nlp.html">Ch 25</a>
        <a href="chapter26_code_language.html">Ch 26</a>
        <a href="chapter27_video_visual.html">Ch 27</a>
        <a href="chapter28_knowledge_graphs.html">Ch 28</a>
        <a href="chapter29_recommendations.html">Ch 29</a>
        <a href="chapter30_healthcare.html">Ch 30</a>
        <a href="chapter31_finance.html">Ch 31</a>
        <a href="chapter32_legal.html">Ch 32</a>
        <a href="chapter33_observability.html">Ch 33</a>
        <a href="chapter34_dsl_agents.html">Ch 34</a>
    </nav>

    <main>
        <h1>Computational Analysis of Transformers</h1>

<h2>Chapter Overview</h2>

<p>Understanding computational requirements is crucial for deploying transformers. This chapter analyzes time and space complexity, memory footprints, and inference costs. We derive exact FLOP counts, memory requirements, and scaling laws for transformers of different sizes.</p>

<h3>Learning Objectives</h3>

<ol>
    <li>Calculate FLOPs for transformer forward and backward passes
    <li>Analyze memory requirements for training and inference
    <li>Understand scaling laws for model size, data, and compute
    <li>Optimize inference through batching and caching
    <li>Estimate training time and costs for large models
</ol>

<h2>Computational Complexity</h2>

<p>Understanding the computational complexity of transformers is essential for making informed decisions about model architecture, hardware requirements, and deployment strategies. The transformer's computational profile differs fundamentally from recurrent architectures, trading sequential dependencies for quadratic memory scaling‚Äîa trade-off that profoundly impacts both training and inference.</p>

<h3>Self-Attention Complexity</h3>

<p>Self-attention is the defining operation of transformers, and its computational characteristics determine much of the model's behavior. For a sequence of length $n$ with model dimension $d_{\text{model}}$, we analyze each component of the attention mechanism in detail.</p>

<p><strong>QKV Projections:</strong> The first step projects the input $\mX \in \R^{n \times d_{\text{model}}}$ into query, key, and value spaces. Each projection is a matrix multiplication:
<div class="equation">
$$
\mQ = \mX \mW^Q, \quad \mK = \mX \mW^K, \quad \mV = \mX \mW^V
$$
</div>
where $\mW^Q, \mW^K, \mW^V \in \R^{d_{\text{model}} \times d_k}$ (typically $d_k = d_{\text{model}}/h$ for $h$ heads).</p>

<p>Each matrix multiplication $\mX \mW$ requires $2nd_{\text{model}}d_k$ floating-point operations (FLOPs): for each of $n \times d_k$ output elements, we perform $d_{\text{model}}$ multiply-add operations. With three projections:
<div class="equation">
$$
\text{QKV FLOPs} = 3 \times 2nd_{\text{model}}d_k = 6nd_{\text{model}}d_k
$$
</div>

<p>For the common case where $d_k = d_{\text{model}}$ (single-head or considering all heads together):
<div class="equation">
$$
\text{QKV FLOPs} = 6nd_{\text{model}}^2
$$
</div>

<p><strong>Why this matters for hardware:</strong> These are dense matrix multiplications, which achieve high utilization on modern GPUs. NVIDIA A100 GPUs can perform up to 312 TFLOPS (FP16 with Tensor Cores), meaning these projections are typically compute-bound rather than memory-bound. However, for small batch sizes or short sequences, the operations may become memory-bandwidth limited, achieving only 10-20\% of peak FLOPS.</p>

<p><strong>Attention Score Computation:</strong> Computing $\mS = \mQ \mK\transpose$ involves multiplying $\mQ \in \R^{n \times d_k}$ by $\mK\transpose \in \R^{d_k \times n}$, producing $\mS \in \R^{n \times n}$:
<div class="equation">
$$
\text{Score FLOPs} = 2n^2d_k
$$
</div>

<p>The attention matrix $\mS$ has $n^2$ elements, and computing each requires $d_k$ multiply-add operations. This quadratic scaling in sequence length is the fundamental bottleneck for long-context transformers.</p>

<p><strong>Dimension tracking example:</strong> For BERT-base with $n=512$, $d_k=64$ (per head), and $h=12$ heads:
<div class="equation">
$$\begin{align}
\mQ^{(i)} &\in \R^{512 \times 64} \quad \text{(one head)} \\
\mK^{(i)\transpose} &\in \R^{64 \times 512} \\
\mS^{(i)} &= \mQ^{(i)} \mK^{(i)\transpose} \in \R^{512 \times 512} \quad \text{(262,144 elements!)}
\end{align}$$
</div>

<p>Across 12 heads, we compute 12 separate $512 \times 512$ attention matrices, requiring:
<div class="equation">
$$
12 \times 2 \times 512^2 \times 64 = 402{,}653{,}184 \text{ FLOPs} \approx 403 \text{ MFLOPs}
$$
</div>

<p><strong>Hardware implications:</strong> The attention matrix requires $n^2$ memory per head. For $n=512$ and 12 heads with FP32:
<div class="equation">
$$
12 \times 512^2 \times 4\text{ bytes} = 12{,}582{,}912\text{ bytes} \approx 12\text{ MB}
$$
</div>

<p>This seems modest, but for $n=2048$ (GPT-2): $12 \times 2048^2 \times 4 = 201\text{ MB}$ per sequence. With batch size 32: $6.4\text{ GB}$ just for attention matrices! This is why long-context models require substantial GPU memory.</p>

<p><strong>Softmax and Scaling:</strong> Applying softmax to each row of $\mS$ requires $O(n^2)$ operations (exponentials and normalization), which is negligible compared to the matrix multiplications but can become significant for very long sequences due to memory access patterns.</p>

<p><strong>Attention Output:</strong> Computing $\mO = \mA \mV$ multiplies the attention weights $\mA \in \R^{n \times n}$ by values $\mV \in \R^{n \times d_v}$:
<div class="equation">
$$
\text{Output FLOPs} = 2n^2d_v
$$
</div>

<p>Again, this scales quadratically with sequence length. For $d_v = d_k$:
<div class="equation">
$$
\text{Attention output FLOPs} = 2n^2d_k
$$
</div>

<p><strong>Output Projection:</strong> Finally, concatenated head outputs are projected back to model dimension:
<div class="equation">
$$
\text{Output projection FLOPs} = 2n(hd_k)d_{\text{model}} = 2nd_{\text{model}}^2
$$
</div>
(assuming $hd_k = d_{\text{model}}$).</p>

<p><strong>Total Self-Attention FLOPs:</strong>
<div class="equation">
$$
\text{Total} = 6nd_{\text{model}}^2 + 2n^2d_k h + 2n^2d_v h + 2nd_{\text{model}}^2 = 8nd_{\text{model}}^2 + 4n^2d_{\text{model}}
$$
</div>

<p>For typical configurations where $d_k = d_v = d_{\text{model}}/h$:
<div class="equation">
$$
\boxed{\text{Self-Attention FLOPs} = 8nd_{\text{model}}^2 + 4n^2d_{\text{model}}}
$$
</div>

<p><strong>Complexity regime analysis:</strong> The relative importance of the two terms depends on the ratio $n/d_{\text{model}}$:
<ul>
    <li><strong>Short sequences</strong> ($n \ll d_{\text{model}}$): The $8nd_{\text{model}}^2$ term dominates. For BERT-base with $n=128$, $d=768$: $8 \times 128 \times 768^2 \approx 603\text{M}$ vs $4 \times 128^2 \times 768 \approx 50\text{M}$. The projections dominate.
    
    <li><strong>Long sequences</strong> ($n \gg d_{\text{model}}$): The $4n^2d_{\text{model}}$ term dominates. For $n=8192$, $d=768$: $8 \times 8192 \times 768^2 \approx 38.7\text{G}$ vs $4 \times 8192^2 \times 768 \approx 206\text{G}$. The attention computation dominates.
    
    <li><strong>Crossover point</strong>: When $8nd_{\text{model}}^2 \approx 4n^2d_{\text{model}}$, solving gives $n \approx 2d_{\text{model}}$. For $d=768$, this occurs around $n \approx 1536$.
</ul>

<figure>
\begin{tikzpicture}[
  node/.style={rectangle, draw, minimum width=2.5cm, minimum height=1cm, font=\small},
  arrow/.style={->, thick}
]

<p>\node[node, fill=blue!10] (X) at (0,0) {$\mX$ \\ $n \times d$};
\node[node, fill=green!10] (QKV) at (3,0) {QKV Proj \\ $O(nd^2)$};
\node[node, fill=red!10] (Attn) at (6,0) {$\mQ\mK^\top$ \\ $O(n^2d)$};
\node[node, fill=green!10] (AV) at (9,0) {$\mA\mV$ \\ $O(n^2d)$};
\node[node, fill=blue!10] (Out) at (12,0) {Output \\ $n \times d$};</p>

<p>\draw[arrow] (X) -- (QKV);
\draw[arrow] (QKV) -- (Attn);
\draw[arrow] (Attn) -- (AV);
\draw[arrow] (AV) -- (Out);</p>

<p>\node[node, fill=blue!10] (X2) at (0,-3) {$\mX$ \\ $n \times d$};
\node[node, fill=green!10] (W1) at (4,-3) {$\mW_1$ \\ $O(nd_{ff}d)$};
\node[node, fill=orange!10] (Act) at (8,-3) {GELU \\ $O(nd_{ff})$};
\node[node, fill=green!10] (W2) at (12,-3) {$\mW_2$ \\ $O(nd_{ff}d)$};
\node[node, fill=blue!10] (Out2) at (16,-3) {Output \\ $n \times d$};</p>

<p>\draw[arrow] (X2) -- (W1);
\draw[arrow] (W1) -- (Act);
\draw[arrow] (Act) -- (W2);
\draw[arrow] (W2) -- (Out2);</p>

<p>Short sequences ($n < 2d$): FFN dominates \\
Long sequences ($n > 2d$): Attention dominates
};</p>

<p>\end{tikzpicture}
<figcaption>Computational flow comparison between self-attention and feed-forward network. Green boxes show matrix multiplications (compute-intensive), red shows the quadratic attention bottleneck, orange shows element-wise operations. For typical sequence lengths, FFN requires roughly 2√ó the FLOPs of attention.</figcaption>
</figure>

<p>This analysis explains why efficient attention mechanisms (Chapter 16) focus on reducing the $O(n^2)$ term for long-context applications.</p>

<h3>Feed-Forward Network Complexity</h3>

<p>The position-wise feed-forward network (FFN) in each transformer layer typically expands the representation to a higher dimension before projecting back. This two-layer network with GELU or ReLU activation is applied independently to each position in the sequence.</p>

<p><strong>Architecture:</strong> For input $\mX \in \R^{n \times d_{\text{model}}}$:
<div class="equation">
$$\begin{align}
\mH &= \text{GELU}(\mX \mW_1 + \vb_1) \quad &\mW_1 \in \R^{d_{\text{model}} \times d_{ff}}, \quad \mH \in \R^{n \times d_{ff}} \\
\mY &= \mH \mW_2 + \vb_2 \quad &\mW_2 \in \R^{d_{ff} \times d_{\text{model}}}, \quad \mY \in \R^{n \times d_{\text{model}}}
\end{align}$$
</div>

<p>The intermediate dimension $d_{ff}$ is typically $4d_{\text{model}}$ in standard transformers (BERT, GPT), though some models use different ratios. This expansion allows the network to learn complex non-linear transformations.</p>

<p><strong>First Projection FLOPs:</strong> Computing $\mX \mW_1$ requires:
<div class="equation">
$$
\text{First projection} = 2n \cdot d_{\text{model}} \cdot d_{ff}
$$
</div>

<p>For $d_{ff} = 4d_{\text{model}}$:
<div class="equation">
$$
\text{First projection} = 2n \cdot d_{\text{model}} \cdot 4d_{\text{model}} = 8nd_{\text{model}}^2
$$
</div>

<p><strong>Second Projection FLOPs:</strong> Computing $\mH \mW_2$ requires:
<div class="equation">
$$
\text{Second projection} = 2n \cdot d_{ff} \cdot d_{\text{model}} = 8nd_{\text{model}}^2
$$
</div>

<p><strong>Total FFN FLOPs:</strong>
<div class="equation">
$$
\boxed{\text{FFN FLOPs} = 16nd_{\text{model}}^2 \quad \text{(for } d_{ff} = 4d_{\text{model}}\text{)}}
$$
</div>

<p><strong>Activation function:</strong> GELU requires additional operations (exponentials, multiplications) but these are $O(nd_{ff})$, negligible compared to the matrix multiplications.</p>

<p><strong>Why FFN dominates computation:</strong> Comparing FFN to self-attention:
<div class="equation">
$$\begin{align}
\text{FFN:} \quad &16nd_{\text{model}}^2 \\
\text{Attention:} \quad &8nd_{\text{model}}^2 + 4n^2d_{\text{model}}
\end{align}$$
</div>

<p>For typical sequence lengths where $n < 2d_{\text{model}}$, the FFN requires roughly twice the FLOPs of attention! This is why some efficient transformer variants (e.g., mixture-of-experts) focus on making the FFN more efficient.</p>

<p><strong>Memory and bandwidth considerations:</strong> The FFN intermediate activations $\mH \in \R^{n \times d_{ff}}$ must be stored for backpropagation. For BERT-base with $n=512$, $d_{ff}=3072$:
<div class="equation">
$$
512 \times 3072 \times 4\text{ bytes} = 6{,}291{,}456\text{ bytes} \approx 6\text{ MB per layer}
$$
</div>

<p>With 12 layers and batch size 32: $6 \times 12 \times 32 = 2.3\text{ GB}$ just for FFN intermediate activations. This is a significant portion of training memory.</p>

<p><strong>Hardware utilization:</strong> FFN matrix multiplications are highly regular and achieve excellent GPU utilization (often 70-90\% of peak FLOPS on modern GPUs). The operations are:
<ul>
    <li><strong>Compute-bound</strong> for reasonable batch sizes and sequence lengths
    <li><strong>Well-suited for Tensor Cores</strong> on NVIDIA GPUs (FP16/BF16 operations)
    <li><strong>Easily parallelizable</strong> across the sequence dimension
</ul>

<p>On an NVIDIA A100 GPU (312 TFLOPS FP16), computing the FFN for BERT-base with batch size 32 and $n=512$:
<div class="equation">
$$
\text{FLOPs} = 32 \times 16 \times 512 \times 768^2 \approx 154\text{ GFLOPS}
$$
</div>
<div class="equation">
$$
\text{Time} \approx \frac{154\text{ GFLOPS}}{312 \times 0.8 \text{ TFLOPS}} \approx 0.62\text{ ms}
$$
</div>
(assuming 80\% utilization).</p>

<h3>Per-Layer Total Complexity</h3>

<p>Combining self-attention and FFN, a complete transformer layer requires:
<div class="equation">
$$
\boxed{\text{Transformer layer} = (8nd_{\text{model}}^2 + 4n^2d_{\text{model}}) + 16nd_{\text{model}}^2 = 24nd_{\text{model}}^2 + 4n^2d_{\text{model}} \text{ FLOPs}}
$$
</div>

<p><strong>Additional operations:</strong> Layer normalization, residual connections, and dropout add $O(nd_{\text{model}})$ operations, which are negligible compared to the matrix multiplications.</p>

<p><strong>Breakdown by component:</strong>
<ul>
    <li><strong>FFN:</strong> $16nd_{\text{model}}^2$ (typically 60-70\% of layer FLOPs for short sequences)
    <li><strong>Attention projections:</strong> $8nd_{\text{model}}^2$ (typically 25-35\%)
    <li><strong>Attention computation:</strong> $4n^2d_{\text{model}}$ (grows with sequence length)
</ul>

<p>This breakdown is crucial for optimization: for short sequences, optimizing FFN yields the largest gains; for long sequences, efficient attention mechanisms become critical.</p>

<div class="example"><strong>Example:</strong> 
BERT-base parameters: $n = 512$, $d_{\text{model}} = 768$, $h = 12$, $d_k = d_v = 64$, $d_{ff} = 3072$

<p><strong>Self-Attention Computation:</strong></p>

<p><em>QKV Projections (all heads):</em>
<div class="equation">
$$
3 \times 2 \times 512 \times 768 \times 768 = 1{,}811{,}939{,}328 \approx 1.81\text{ GFLOPs}
$$
</div>

<p><em>Attention scores</em> ($\mQ \mK\transpose$ for 12 heads):
<div class="equation">
$$
12 \times 2 \times 512^2 \times 64 = 402{,}653{,}184 \approx 403\text{ MFLOPs}
$$
</div>

<p><em>Attention output</em> ($\mA \mV$ for 12 heads):
<div class="equation">
$$
12 \times 2 \times 512^2 \times 64 = 402{,}653{,}184 \approx 403\text{ MFLOPs}
$$
</div>

<p><em>Output projection:</em>
<div class="equation">
$$
2 \times 512 \times 768 \times 768 = 603{,}979{,}776 \approx 604\text{ MFLOPs}
$$
</div>

<p><em>Total self-attention:</em>
<div class="equation">
$$
1.81 + 0.40 + 0.40 + 0.60 = 3.21\text{ GFLOPs}
$$
</div>

<p><strong>Feed-Forward Network:</strong></p>

<p><em>First projection</em> ($\mX \mW_1$, where $\mW_1 \in \R^{768 \times 3072}$):
<div class="equation">
$$
2 \times 512 \times 768 \times 3072 = 2{,}415{,}919{,}104 \approx 2.42\text{ GFLOPs}
$$
</div>

<p><em>Second projection</em> ($\mH \mW_2$, where $\mW_2 \in \R^{3072 \times 768}$):
<div class="equation">
$$
2 \times 512 \times 3072 \times 768 = 2{,}415{,}919{,}104 \approx 2.42\text{ GFLOPs}
$$
</div>

<p><em>Total FFN:</em>
<div class="equation">
$$
2.42 + 2.42 = 4.84\text{ GFLOPs}
$$
</div>

<p><strong>Complete Layer:</strong>
<div class="equation">
$$
3.21 + 4.84 = 8.05\text{ GFLOPs per layer}
$$
</div>

<p><strong>Full 12-Layer BERT-base:</strong>
<div class="equation">
$$
12 \times 8.05 = 96.6\text{ GFLOPs per forward pass}
$$
</div>

<p><strong>Training (forward + backward):</strong> Backward pass requires approximately $2\times$ the FLOPs of forward pass:
<div class="equation">
$$
\text{Training step} \approx 3 \times 96.6 = 290\text{ GFLOPs}
$$
</div>

<p><strong>Hardware timing on NVIDIA A100 (312 TFLOPS FP16):</strong></p>

<p>Assuming 70\% utilization (realistic for mixed operations):
<div class="equation">
$$
\text{Forward pass time} \approx \frac{96.6\text{ GFLOPS}}{312 \times 0.7\text{ TFLOPS}} \approx 0.44\text{ ms}
$$
</div>

<p>For batch size 32:
<div class="equation">
$$
\text{Batch forward time} \approx 32 \times 0.44 = 14\text{ ms}
$$
</div>

<p><strong>Training throughput:</strong> With batch size 32, sequence length 512:
<div class="equation">
$$
\text{Tokens per second} = \frac{32 \times 512}{14\text{ ms} \times 3} \approx 390{,}000\text{ tokens/sec}
$$
</div>

<p>This analysis shows why BERT-base training is feasible on single GPUs, while larger models require distributed training.</p>

<p><strong>Memory bandwidth considerations:</strong> The A100 has 1.6 TB/s memory bandwidth. Loading model parameters (110M $\times$ 4 bytes = 440 MB) takes:
<div class="equation">
$$
\text{Parameter load time} = \frac{440\text{ MB}}{1600\text{ GB/s}} \approx 0.28\text{ ms}
$$
</div>

<p>This is comparable to compute time, indicating that for small batch sizes, the model can become memory-bandwidth bound rather than compute-bound. Larger batch sizes amortize parameter loading across more computation, improving utilization.
</div>

<h3>Complexity Analysis</h3>

<div class="theorem"><strong>Theorem:</strong> 
For $L$ layers, sequence length $n$, dimension $d$:

<p><strong>Time complexity:</strong> $O(Ln^2d + Lnd^2)$</p>

<p><strong>Space complexity:</strong> $O(Ln^2 + Lnd)$
</div>

<p><strong>Comparison with RNN:</strong>
<ul>
    <li>RNN: $O(Lnd^2)$ time, $O(Ld^2)$ space
    <li>Transformer: Quadratic in $n$ but parallel; RNN sequential
</ul>

<p><strong>Bottleneck regimes:</strong>
<ul>
    <li>Short sequences $(n < d)$: FFN dominates, $O(Lnd^2)$
    <li>Long sequences $(n > d)$: Attention dominates, $O(Ln^2d)$
</ul>

<h2>Memory Requirements</h2>

<p>Memory is often the limiting factor in training and deploying large transformer models. Understanding memory requirements at a granular level enables informed decisions about model architecture, batch sizes, and hardware selection. We analyze memory consumption across four categories: model parameters, gradients, optimizer states, and activations.</p>

<h3>Model Parameters</h3>

<p>Model parameters must be stored in GPU memory during both training and inference. The memory footprint depends on the numerical precision used.</p>

<p><strong>Precision options:</strong>
<ul>
    <li><strong>FP32 (float32):</strong> 4 bytes per parameter, standard precision
    <li><strong>FP16 (float16):</strong> 2 bytes per parameter, half precision
    <li><strong>BF16 (bfloat16):</strong> 2 bytes per parameter, better range than FP16
    <li><strong>INT8:</strong> 1 byte per parameter, quantized inference
</ul>

<p>For BERT-base with 110 million parameters:
<div class="equation">
$$\begin{align}
\text{FP32:} \quad &110{,}000{,}000 \times 4 = 440{,}000{,}000\text{ bytes} = 440\text{ MB} \\
\text{FP16/BF16:} \quad &110{,}000{,}000 \times 2 = 220{,}000{,}000\text{ bytes} = 220\text{ MB} \\
\text{INT8:} \quad &110{,}000{,}000 \times 1 = 110{,}000{,}000\text{ bytes} = 110\text{ MB}
\end{align}$$
</div>

<p><strong>Parameter breakdown for BERT-base:</strong>
<div class="equation">
$$\begin{align}
\text{Token embeddings:} \quad &V \times d_{\text{model}} = 30{,}000 \times 768 = 23{,}040{,}000 \text{ params} \\
\text{Position embeddings:} \quad &n_{\max} \times d_{\text{model}} = 512 \times 768 = 393{,}216 \text{ params} \\
\text{Segment embeddings:} \quad &2 \times d_{\text{model}} = 2 \times 768 = 1{,}536 \text{ params}
\end{align}$$
</div>

<p><em>Per transformer layer:</em>
<div class="equation">
$$\begin{align}
\text{Self-attention:} \quad &4 \times d_{\text{model}}^2 = 4 \times 768^2 = 2{,}359{,}296 \text{ params} \\
\text{FFN:} \quad &2 \times d_{\text{model}} \times d_{ff} = 2 \times 768 \times 3072 = 4{,}718{,}592 \text{ params} \\
\text{Layer norms:} \quad &4 \times d_{\text{model}} = 4 \times 768 = 3{,}072 \text{ params} \\
\text{Total per layer:} \quad &7{,}080{,}960 \text{ params}
\end{align}$$
</div>

<p><em>12 layers:</em>
<div class="equation">
$$
12 \times 7{,}080{,}960 = 84{,}971{,}520 \text{ params}
$$
</div>

<p><em>Total BERT-base:</em>
<div class="equation">
$$
23{,}040{,}000 + 393{,}216 + 1{,}536 + 84{,}971{,}520 = 108{,}406{,}272 \approx 110\text{M params}
$$
</div>

<p>In FP32: $110\text{M} \times 4 = 440\text{ MB}$</p>

<p><strong>Larger models scale dramatically:</strong>
<div class="equation">
$$\begin{align}
\text{GPT-2 (1.5B):} \quad &1{,}500{,}000{,}000 \times 4 = 6{,}000\text{ MB} = 6\text{ GB (FP32)} \\
\text{GPT-3 (175B):} \quad &175{,}000{,}000{,}000 \times 4 = 700{,}000\text{ MB} = 700\text{ GB (FP32)}
\end{align}$$
</div>

<p>GPT-3 in FP32 requires 700 GB just for parameters‚Äîfar exceeding single GPU memory (A100 has 80 GB). This necessitates:
<ul>
    <li><strong>Model parallelism:</strong> Split model across multiple GPUs
    <li><strong>Mixed precision:</strong> Use FP16/BF16 (350 GB for GPT-3)
    <li><strong>Quantization:</strong> INT8 inference (175 GB for GPT-3)
</ul>

<h3>Activation Memory</h3>

<p>During training, intermediate activations must be stored for backpropagation. Activation memory scales with batch size and sequence length, often dominating memory consumption.</p>

<p><strong>Activations per transformer layer:</strong>
<ul>
    <li><strong>Input to layer:</strong> $B \times n \times d_{\text{model}}$
    <li><strong>Query, Key, Value:</strong> $3 \times B \times n \times d_{\text{model}}$
    <li><strong>Attention scores:</strong> $B \times h \times n \times n$ (quadratic in sequence length!)
    <li><strong>Attention output:</strong> $B \times n \times d_{\text{model}}$
    <li><strong>FFN intermediate:</strong> $B \times n \times d_{ff}$
    <li><strong>Layer norm activations:</strong> $2 \times B \times n \times d_{\text{model}}$
</ul>

<p><strong>Total activation memory per layer (approximate):</strong>
<div class="equation">
$$
\text{Memory} \approx B \times n \times (8d_{\text{model}} + d_{ff}) + B \times h \times n^2
$$
</div>

<p>For BERT-base ($B=32$, $n=512$, $d_{\text{model}}=768$, $h=12$, $d_{ff}=3072$):
<div class="equation">
$$\begin{align}
\text{Linear terms:} \quad &32 \times 512 \times (8 \times 768 + 3072) \times 4\text{ bytes} \\
&= 32 \times 512 \times 9{,}216 \times 4 = 603{,}979{,}776\text{ bytes} \approx 604\text{ MB} \\
\text{Attention matrices:} \quad &32 \times 12 \times 512^2 \times 4 = 402{,}653{,}184\text{ bytes} \approx 403\text{ MB} \\
\text{Total per layer:} \quad &\approx 1{,}007\text{ MB} \approx 1\text{ GB}
\end{align}$$
</div>

<p><strong>For 12 layers:</strong> $12 \times 1\text{ GB} = 12\text{ GB}$ just for activations!</p>

<p><strong>Impact of sequence length:</strong> The attention matrix term $B \times h \times n^2$ grows quadratically. For $n=2048$ (4√ó longer):
<div class="equation">
$$
32 \times 12 \times 2048^2 \times 4 = 6{,}442{,}450{,}944\text{ bytes} \approx 6.4\text{ GB per layer}
$$
</div>

<p>For 12 layers: $77\text{ GB}$ just for attention matrices‚Äînearly filling an A100 GPU!</p>

<p>This quadratic scaling is why:
<ul>
    <li>Long-context models require gradient checkpointing (recompute activations during backward pass)
    <li>Efficient attention mechanisms (sparse, linear) are crucial for long sequences
    <li>Batch sizes must be reduced for longer sequences
</ul>

<p><strong>Gradient checkpointing trade-off:</strong> Recomputing activations during backward pass:
<ul>
    <li><strong>Memory savings:</strong> Reduce activation memory by $\sim$80\%
    <li><strong>Compute cost:</strong> Increase training time by $\sim$20-30\%
    <li><strong>When to use:</strong> When memory-constrained, especially for long sequences
</ul>

<div class="example"><strong>Example:</strong> 
GPT-2 (small): $L=12$, $d_{\text{model}}=768$, $h=12$, $d_k=64$, $d_{ff}=3072$, $n=1024$

<p><strong>Per-layer activation breakdown (batch size $B=1$):</strong></p>

<p><em>QKV projections:</em>
<div class="equation">
$$
3 \times 1024 \times 768 \times 4 = 9{,}437{,}184\text{ bytes} \approx 9.4\text{ MB}
$$
</div>

<p><em>Attention matrices (12 heads):</em>
<div class="equation">
$$
12 \times 1024^2 \times 4 = 50{,}331{,}648\text{ bytes} \approx 50.3\text{ MB}
$$
</div>

<p>This is the dominant term! For $n=2048$: $12 \times 2048^2 \times 4 = 201\text{ MB}$ (4√ó larger).</p>

<p><em>Attention output:</em>
<div class="equation">
$$
1024 \times 768 \times 4 = 3{,}145{,}728\text{ bytes} \approx 3.1\text{ MB}
$$
</div>

<p><em>FFN intermediate:</em>
<div class="equation">
$$
1024 \times 3072 \times 4 = 12{,}582{,}912\text{ bytes} \approx 12.6\text{ MB}
$$
</div>

<p><em>Layer norm and residuals:</em>
<div class="equation">
$$
3 \times 1024 \times 768 \times 4 = 9{,}437{,}184\text{ bytes} \approx 9.4\text{ MB}
$$
</div>

<p><em>Total per layer:</em>
<div class="equation">
$$
9.4 + 50.3 + 3.1 + 12.6 + 9.4 = 84.8\text{ MB}
$$
</div>

<p><strong>12 layers:</strong> $12 \times 84.8 = 1{,}018\text{ MB} \approx 1\text{ GB}$ for single sequence</p>

<p><strong>Batch size scaling:</strong>
<div class="equation">
$$\begin{align}
B=8: \quad &8\text{ GB} \\
B=16: \quad &16\text{ GB} \\
B=32: \quad &32\text{ GB} \\
B=64: \quad &64\text{ GB}
\end{align}$$
</div>

<p><strong>Hardware implications:</strong>
<ul>
    <li><strong>NVIDIA V100 (16 GB):</strong> Maximum batch size $\approx 12-14$ (accounting for parameters and optimizer states)
    <li><strong>NVIDIA A100 (40 GB):</strong> Maximum batch size $\approx 30-35$
    <li><strong>NVIDIA A100 (80 GB):</strong> Maximum batch size $\approx 70-75$
</ul>

<p><strong>Gradient checkpointing impact:</strong> With checkpointing, only store activations at layer boundaries, recompute within layers during backward pass:
<div class="equation">
$$
\text{Memory reduction} \approx 80\% \Rightarrow 1\text{ GB} \to 200\text{ MB per sequence}
$$
</div>

<p>This allows batch size 64 on V100 (16 GB), but increases training time by $\sim$25\%.</p>

<p><strong>Mixed precision training:</strong> Using FP16 for activations (FP32 for parameters):
<div class="equation">
$$
\text{Activation memory} \to 1\text{ GB} / 2 = 500\text{ MB per sequence}
$$
</div>

<p>Combined with gradient checkpointing: $500 \times 0.2 = 100\text{ MB per sequence}$, enabling very large batch sizes.
</div>

<h3>Training Memory Budget</h3>

<p>Training requires memory for parameters, gradients, optimizer states, and activations. Understanding this breakdown is essential for selecting hardware and configuring training.</p>

<p><strong>Total training memory:</strong>
<div class="equation">
$$
\text{Memory}_{\text{total}} = \text{Parameters} + \text{Gradients} + \text{Optimizer States} + \text{Activations}
$$
</div>

<p><strong>For AdamW optimizer (most common for transformers):</strong>
<ul>
    <li><strong>Model parameters:</strong> $P$ parameters $\times$ 4 bytes (FP32) = $4P$ bytes
    <li><strong>Gradients:</strong> $P$ parameters $\times$ 4 bytes = $4P$ bytes
    <li><strong>First moment (momentum):</strong> $P$ parameters $\times$ 4 bytes = $4P$ bytes
    <li><strong>Second moment (variance):</strong> $P$ parameters $\times$ 4 bytes = $4P$ bytes
    <li><strong>Activations:</strong> $A$ bytes (depends on batch size, sequence length, model depth)
</ul>

<p><strong>Total:</strong> $16P + A$ bytes</p>

<p><strong>Mixed precision training (FP16/BF16 with FP32 master weights):</strong>
<ul>
    <li><strong>FP16 parameters (forward/backward):</strong> $2P$ bytes
    <li><strong>FP32 master parameters:</strong> $4P$ bytes
    <li><strong>FP32 gradients:</strong> $4P$ bytes
    <li><strong>FP32 optimizer states:</strong> $8P$ bytes
    <li><strong>FP16 activations:</strong> $A/2$ bytes
</ul>

<p><strong>Total:</strong> $18P + A/2$ bytes</p>

<p>Surprisingly, mixed precision uses slightly MORE memory for parameters/optimizer (18P vs 16P) but saves significantly on activations ($A/2$ vs $A$). Since activations often dominate, mixed precision typically reduces total memory.</p>

<div class="example"><strong>Example:</strong> 
BERT-base: 110M parameters, batch size 32, sequence length 512

<p><strong>FP32 training:</strong>
<div class="equation">
$$\begin{align}
\text{Parameters:} \quad &110\text{M} \times 4 = 440\text{ MB} \\
\text{Gradients:} \quad &110\text{M} \times 4 = 440\text{ MB} \\
\text{Adam states (2√ó):} \quad &2 \times 110\text{M} \times 4 = 880\text{ MB} \\
\text{Activations:} \quad &32 \times 12 \times 1\text{ GB} = 12\text{ GB} \\
\text{Total:} \quad &440 + 440 + 880 + 12{,}000 = 13{,}760\text{ MB} \approx 13.8\text{ GB}
\end{align}$$
</div>

<p><strong>Fits on:</strong> NVIDIA V100 (16 GB), A100 (40/80 GB), RTX 3090 (24 GB)</p>

<p><strong>Mixed precision training:</strong>
<div class="equation">
$$\begin{align}
\text{FP16 parameters:} \quad &110\text{M} \times 2 = 220\text{ MB} \\
\text{FP32 master + gradients + Adam:} \quad &110\text{M} \times 16 = 1{,}760\text{ MB} \\
\text{FP16 activations:} \quad &12{,}000 / 2 = 6{,}000\text{ MB} \\
\text{Total:} \quad &220 + 1{,}760 + 6{,}000 = 7{,}980\text{ MB} \approx 8\text{ GB}
\end{align}$$
</div>

<p><strong>Mixed precision saves:</strong> $13.8 - 8 = 5.8\text{ GB}$ (42\% reduction)</p>

<p><strong>With gradient checkpointing:</strong> Activations reduced by 80\%:
<div class="equation">
$$
220 + 1{,}760 + 1{,}200 = 3{,}180\text{ MB} \approx 3.2\text{ GB}
$$
</div>

<p>This enables batch size 128 on V100 (16 GB)!
</div>

<div class="example"><strong>Example:</strong> 
GPT-3: 175B parameters, sequence length 2048

<p><strong>Parameters and optimizer (FP32):</strong>
<div class="equation">
$$
175\text{B} \times 16 = 2{,}800\text{ GB}
$$
</div>

<p><strong>Activations (batch size 1, single sequence):</strong>
<div class="equation">
$$\begin{align}
\text{Per layer:} \quad &\approx 2048 \times (8 \times 12{,}288 + 4 \times 12{,}288) + 96 \times 2048^2 \\
&\approx 2048 \times 147{,}456 + 402{,}653{,}184 \\
&\approx 704\text{ MB per layer}
\end{align}$$
</div>

<p><strong>96 layers:</strong> $96 \times 704\text{ MB} \approx 68\text{ GB per sequence}$</p>

<p><strong>Total for batch size 1:</strong> $2{,}800 + 68 = 2{,}868\text{ GB}$</p>

<p><strong>Hardware requirements:</strong>
<ul>
    <li><strong>Single A100 (80 GB):</strong> Impossible‚Äîneed 36 GPUs just for parameters!
    <li><strong>Model parallelism:</strong> Split across 8 GPUs: $2{,}868 / 8 = 359\text{ GB per GPU}$‚Äîstill too large!
    <li><strong>Mixed precision + model parallelism:</strong> $\approx 1{,}500\text{ GB total} / 8 = 188\text{ GB per GPU}$‚Äîstill too large!
    <li><strong>Mixed precision + model parallelism + gradient checkpointing:</strong> $\approx 800\text{ GB} / 8 = 100\text{ GB per GPU}$‚Äîstill exceeds A100!
</ul>

<p><strong>Actual GPT-3 training:</strong> Used ZeRO optimizer (shards optimizer states across GPUs) + model parallelism + pipeline parallelism across thousands of GPUs.</p>

<p>This example illustrates why training models beyond $\sim$10B parameters requires sophisticated distributed training strategies.
</div>

<h3>Hardware Selection Guide</h3>

<p><strong>GPU memory requirements by model size (mixed precision + gradient checkpointing):</strong></p>

<table>
<tr><th><strong>Model Size</strong></th><th><strong>Parameters</strong></th><th><strong>Min GPU Memory</strong></th><th><strong>Recommended GPU</strong></th></tr>
<tr><td>Small</td><td>100M</td><td>8 GB</td><td>RTX 3070, V100</td></tr>
<tr><td>Base</td><td>300M</td><td>12 GB</td><td>RTX 3080, V100</td></tr>
<tr><td>Large</td><td>1B</td><td>24 GB</td><td>RTX 3090, A5000</td></tr>
<tr><td>XL</td><td>3B</td><td>40 GB</td><td>A100 (40 GB)</td></tr>
<tr><td>XXL</td><td>10B</td><td>80 GB</td><td>A100 (80 GB)</td></tr>
<tr><td>175B (GPT-3)</td><td>175B</td><td>8√ó A100 (80 GB)</td><td>Multi-node cluster</td></tr>
</table>

<p><strong>Inference memory requirements (FP16):</strong>
<ul>
    <li><strong>Parameters only:</strong> $2P$ bytes
    <li><strong>KV cache (autoregressive):</strong> $2 \times L \times h \times n_{\max} \times d_k \times B$ bytes
    <li><strong>Activations (single forward pass):</strong> Minimal compared to training
</ul>

<p>For GPT-2 (117M params) inference:
<div class="equation">
$$
\text{Parameters:} \quad 117\text{M} \times 2 = 234\text{ MB}
$$
</div>
<div class="equation">
$$
\text{KV cache (batch 1, } n=1024\text{):} \quad 2 \times 12 \times 12 \times 1024 \times 64 \times 2 = 38\text{ MB}
$$
</div>
<div class="equation">
$$
\text{Total:} \quad \approx 300\text{ MB}
$$
</div>

<p>GPT-2 inference easily fits on consumer GPUs or even CPUs!</p>

<h2>Inference Optimization</h2>

<p>Inference optimization is critical for deploying transformers in production. Unlike training, which prioritizes throughput (tokens/second across large batches), inference prioritizes latency (time to generate a single response) while maintaining reasonable throughput. We analyze key optimization techniques and their trade-offs.</p>

<h3>KV Caching for Autoregressive Decoding</h3>

<p>Autoregressive generation (used in GPT, decoder-only models) generates tokens sequentially, where each new token attends to all previous tokens. Naive implementation recomputes attention for all previous positions at each step‚Äîhighly inefficient.</p>

<p><strong>Problem analysis:</strong> Generating sequence of length $T$ tokens:
<ul>
    <li><strong>Step 1:</strong> Compute attention for position 1 (attends to position 1)
    <li><strong>Step 2:</strong> Compute attention for position 2 (attends to positions 1-2)
    <li><strong>Step 3:</strong> Compute attention for position 3 (attends to positions 1-3)
    <li><strong>Step $T$:</strong> Compute attention for position $T$ (attends to positions 1-$T$)
</ul>

<p><strong>Total attention computations:</strong> $\sum_{t=1}^{T} t = \frac{T(T+1)}{2} \approx \frac{T^2}{2}$</p>

<p>For $T=1000$ tokens: $\approx 500{,}000$ attention computations!</p>

<p><strong>KV Caching solution:</strong> Key and value projections depend only on input tokens, not on the query position. Cache $\mK$ and $\mV$ from previous steps:</p>

<div class="algorithm"><div class="algorithm-title">Algorithm: Autoregressive Generation with KV Caching</div>
<div class="algorithm-line"><strong>Input:</strong> Prompt tokens $\vx_1, \ldots, \vx_p$, max length $T$</div>
<div class="algorithm-line"><strong>Output:</strong> Generated sequence $\vx_1, \ldots, \vx_T$</div>
<div class="algorithm-line"><span class="algorithm-comment">// Initialize cache</span></div>
<div class="algorithm-line">$\text{cache}_K = []$, $\text{cache}_V = []$ \\</p></div>
<div class="algorithm-line"><span class="algorithm-comment">// Process prompt</span></div>
<div class="algorithm-line"><strong>for</strong> $t = 1$ to $p$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">$\vk_t = \mW^K \vx_t$, $\vv_t = \mW^V \vx_t$</div>
<div class="algorithm-line">Append $\vk_t$ to $\text{cache}_K$, $\vv_t$ to $\text{cache}_V$</div>
<div class="algorithm-line">$\vq_t = \mW^Q \vx_t$</div>
<div class="algorithm-line">Compute attention using $\vq_t$ and all cached keys/values</div>
<div class="algorithm-line">Generate $\vh_t$</div>
<div class="algorithm-line">}</p></div>
<div class="algorithm-line"><span class="algorithm-comment">// Generate new tokens</span></div>
<div class="algorithm-line"><strong>for</strong> $t = p+1$ to $T$ <strong>do</strong></div>
<div class="algorithm-indent">
<div class="algorithm-line">Sample $\vx_t$ from $\vh_{t-1}$</div>
<div class="algorithm-line">$\vk_t = \mW^K \vx_t$, $\vv_t = \mW^V \vx_t$</div>
<div class="algorithm-line">Append $\vk_t$ to $\text{cache}_K$, $\vv_t$ to $\text{cache}_V$</div>
<div class="algorithm-line">$\vq_t = \mW^Q \vx_t$</div>
<div class="algorithm-line">Compute attention: $\text{Attention}(\vq_t, \text{cache}_K, \text{cache}_V)$</div>
<div class="algorithm-line">Generate $\vh_t$</div>
</div>
</div>
</div>

<p><strong>Computational savings:</strong> With caching, each step computes attention once (not recomputing previous positions):
<div class="equation">
$$
\text{Total computations} = T \quad \text{(vs. } \frac{T^2}{2} \text{ without caching)}
$$
</div>

<p><strong>Speedup:</strong> For $T=1000$: $\frac{500{,}000}{1{,}000} = 500\times$ faster!</p>

<p><strong>Memory cost:</strong> Store keys and values for all positions and layers:
<div class="equation">
$$
\text{KV cache size} = 2 \times L \times h \times T \times d_k \times \text{sizeof(float)}
$$
</div>

<p>For GPT-2 ($L=12$, $h=12$, $d_k=64$, FP16):
<div class="equation">
$$
2 \times 12 \times 12 \times T \times 64 \times 2 = 36{,}864 \times T \text{ bytes}
$$
</div>

<p><strong>Memory scaling with sequence length:</strong>
<div class="equation">
$$\begin{align}
T=512: \quad &36{,}864 \times 512 = 18{,}874{,}368\text{ bytes} \approx 19\text{ MB} \\
T=1024: \quad &36{,}864 \times 1024 = 37{,}748{,}736\text{ bytes} \approx 38\text{ MB} \\
T=2048: \quad &36{,}864 \times 2048 = 75{,}497{,}472\text{ bytes} \approx 75\text{ MB} \\
T=4096: \quad &36{,}864 \times 4096 = 150{,}994{,}944\text{ bytes} \approx 151\text{ MB}
\end{align}$$
</div>

<p>For GPT-3 ($L=96$, $h=96$, $d_k=128$, $T=2048$, FP16):
<div class="equation">
$$
2 \times 96 \times 96 \times 2048 \times 128 \times 2 = 9{,}663{,}676{,}416\text{ bytes} \approx 9.7\text{ GB per sequence}
$$
</div>

<p><strong>Batch inference with KV cache:</strong> For batch size $B$:
<div class="equation">
$$
\text{Total KV cache} = B \times 2 \times L \times h \times T \times d_k \times \text{sizeof(float)}
$$
</div>

<p>For GPT-3 with $B=8$: $8 \times 9.7\text{ GB} = 77.6\text{ GB}$‚Äînearly filling an A100 (80 GB)!</p>

<p>This is why large-scale inference services:
<ul>
    <li>Use smaller batch sizes for long contexts
    <li>Implement dynamic batching (group requests of similar lengths)
    <li>Use quantization (INT8) to reduce cache size by 2-4√ó
</ul>

<h3>Batched Inference</h3>

<p>Processing multiple sequences simultaneously increases GPU utilization and throughput.</p>

<p><strong>Single sequence inference:</strong> For GPT-2 generating 100 tokens:
<ul>
    <li><strong>Compute:</strong> $\approx 100 \times 8\text{ GFLOPs} = 800\text{ GFLOPs}$
    <li><strong>Time on A100:</strong> $\frac{800\text{ GFLOPS}}{312\text{ TFLOPS} \times 0.3} \approx 8.5\text{ ms}$
    <li><strong>GPU utilization:</strong> $\approx 30\%$ (memory-bound, not compute-bound)
</ul>

<p><strong>Batched inference (batch size 32):</strong>
<ul>
    <li><strong>Compute:</strong> $32 \times 800\text{ GFLOPs} = 25{,}600\text{ GFLOPs}$
    <li><strong>Time on A100:</strong> $\frac{25{,}600\text{ GFLOPS}}{312\text{ TFLOPS} \times 0.7} \approx 117\text{ ms}$
    <li><strong>GPU utilization:</strong> $\approx 70\%$ (much better!)
    <li><strong>Throughput:</strong> $\frac{32 \times 100}{117\text{ ms}} \approx 27{,}350\text{ tokens/sec}$
</ul>

<p><strong>Latency vs. throughput trade-off:</strong>
<ul>
    <li><strong>Batch size 1:</strong> Latency = 8.5 ms, throughput = 11,765 tokens/sec
    <li><strong>Batch size 32:</strong> Latency = 117 ms (13.8√ó worse), throughput = 27,350 tokens/sec (2.3√ó better)
</ul>

<p><strong>Padding challenge:</strong> Sequences in a batch must have the same length. Shorter sequences are padded, wasting computation:
<ul>
    <li>Sequence lengths: [512, 256, 128, 64]
    <li>Padded to: [512, 512, 512, 512]
    <li>Wasted computation: $(512-256) + (512-128) + (512-64) = 1024$ positions (50\%!)
</ul>

<p><strong>Solutions:</strong>
<ul>
    <li><strong>Dynamic batching:</strong> Group sequences of similar lengths
    <li><strong>Bucket batching:</strong> Pre-defined length buckets (128, 256, 512, 1024)
    <li><strong>Packed sequences:</strong> Concatenate sequences without padding (requires careful attention masking)
</ul>

<h3>Quantization for Inference</h3>

<p>Quantization reduces memory and increases throughput by using lower-precision arithmetic.</p>

<p><strong>Precision options:</strong>
<ul>
    <li><strong>FP32:</strong> 4 bytes, full precision
    <li><strong>FP16/BF16:</strong> 2 bytes, half precision (1.5-2√ó speedup)
    <li><strong>INT8:</strong> 1 byte, 8-bit integer (2-4√ó speedup, 4√ó memory reduction)
    <li><strong>INT4:</strong> 0.5 bytes, 4-bit integer (4-8√ó speedup, 8√ó memory reduction)
</ul>

<p><strong>INT8 quantization:</strong> Map FP32 weights $w \in [-w_{\max}, w_{\max}]$ to INT8 $w_q \in [-128, 127]$:
<div class="equation">
$$
w_q = \text{round}\left(\frac{w}{w_{\max}} \times 127\right)
$$
</div>

<p>Dequantize during computation:
<div class="equation">
$$
w \approx \frac{w_q \times w_{\max}}{127}
$$
</div>

<p><strong>Quantization impact on GPT-2:</strong>
<div class="equation">
$$\begin{align}
\text{FP32:} \quad &117\text{M} \times 4 = 468\text{ MB} \\
\text{FP16:} \quad &117\text{M} \times 2 = 234\text{ MB} \quad (2\times \text{ reduction}) \\
\text{INT8:} \quad &117\text{M} \times 1 = 117\text{ MB} \quad (4\times \text{ reduction}) \\
\text{INT4:} \quad &117\text{M} \times 0.5 = 58.5\text{ MB} \quad (8\times \text{ reduction})
\end{align}$$
</div>

<p><strong>Accuracy trade-offs:</strong>
<ul>
    <li><strong>FP16/BF16:</strong> Negligible accuracy loss (<0.1\% perplexity increase)
    <li><strong>INT8:</strong> Small accuracy loss (0.5-2\% perplexity increase) with calibration
    <li><strong>INT4:</strong> Moderate accuracy loss (2-5\% perplexity increase), requires careful quantization
</ul>

<p><strong>Hardware support:</strong>
<ul>
    <li><strong>NVIDIA Tensor Cores:</strong> Accelerate FP16/BF16 (up to 2√ó speedup)
    <li><strong>NVIDIA INT8 Tensor Cores:</strong> Accelerate INT8 (up to 4√ó speedup)
    <li><strong>CPU AVX-512 VNNI:</strong> Accelerate INT8 on CPUs
</ul>

<h3>Model Distillation</h3>

<p>Train smaller "student" model to mimic larger "teacher" model:
<ul>
    <li><strong>DistilBERT:</strong> 66M params (vs. BERT-base 110M), 97\% performance, 2√ó faster
    <li><strong>TinyBERT:</strong> 14M params, 96\% performance, 7√ó faster
</ul>

<p>Distillation enables deployment on resource-constrained devices (mobile, edge).</p>

<h3>Inference Optimization Summary</h3>

<table>
<tr><th><strong>Technique</strong></th><th><strong>Speedup</strong></th><th><strong>Memory Reduction</strong></th><th><strong>Accuracy Impact</strong></th></tr>
<tr><td>KV Caching</td><td>100-500√ó</td><td>-50\% (cache overhead)</td><td>None</td></tr>
<tr><td>Batching (32√ó)</td><td>2-3√ó throughput</td><td>None</td><td>None</td></tr>
<tr><td>FP16/BF16</td><td>1.5-2√ó</td><td>2√ó</td><td>Negligible</td></tr>
<tr><td>INT8 Quantization</td><td>2-4√ó</td><td>4√ó</td><td>Small (0.5-2\%)</td></tr>
<tr><td>INT4 Quantization</td><td>4-8√ó</td><td>8√ó</td><td>Moderate (2-5\%)</td></tr>
<tr><td>Distillation</td><td>2-7√ó</td><td>2-8√ó</td><td>Small (3-4\%)</td></tr>
</table>

<p><strong>Combined optimizations:</strong> KV caching + FP16 + batching + INT8 can achieve 1000√ó speedup with minimal accuracy loss!</p>

<h2>Scaling Laws</h2>

<h3>Kaplan et al. Scaling Laws</h3>

<p>Performance scales as power law with model size $N$, dataset size $D$, and compute $C$:
<div class="equation">
$$
L(N, D, C) \approx \left(\frac{N_c}{N}\right)^{\alpha_N} + \left(\frac{D_c}{D}\right)^{\alpha_D} + \left(\frac{C_c}{C}\right)^{\alpha_C}
$$
</div>

<p><strong>Key findings:</strong>
<ul>
    <li>Larger models are more sample-efficient
    <li>Compute-optimal: Balance model size and data
    <li>Doubling compute $\to$ consistent loss reduction
</ul>

<h3>Chinchilla Scaling Laws</h3>

<p>For fixed compute budget, optimal allocation:
<div class="equation">
$$
N_{\text{optimal}} \propto C^{0.5}, \quad D_{\text{optimal}} \propto C^{0.5}
$$
</div>

<p><strong>Implication:</strong> Many large models (GPT-3) are over-parameterized and under-trained! Chinchilla (70B params, more data) outperforms Gopher (280B params, less data).</p>

<h2>Exercises</h2>

<div class="exercise" id="exercise-1"><strong>Exercise 1:</strong> Calculate FLOPs for GPT-3 (175B parameters, $L=96$, $d=12288$, $h=96$, $n=2048$) for: (1) Single forward pass, (2) Generating 100 tokens autoregressively, (3) Training on 1 trillion tokens.
</div>

<div class="exercise" id="exercise-2"><strong>Exercise 2:</strong> Estimate memory for training 1.3B parameter model with batch size 64, sequence length 2048. What GPU memory required? How to fit on A100 (80GB)?
</div>

<div class="exercise" id="exercise-3"><strong>Exercise 3:</strong> Implement KV caching for GPT-2. Measure speedup for generating 256 tokens. Plot generation time vs sequence length with/without caching.
</div>

<div class="exercise" id="exercise-4"><strong>Exercise 4:</strong> For fixed compute budget $C = 10^{24}$ FLOPs: Use Chinchilla scaling to find optimal model size and data size. Compare with GPT-3 allocation.
</div>

<h2>Solutions</h2>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 1: GPT-3 FLOPs Calculation</strong>

<p>Given: GPT-3 with $P = 175B$ parameters, $L = 96$ layers, $d_{\text{model}} = 12{,}288$, $h = 96$ heads, $n = 2048$ sequence length</p>

<p><strong>Part (1): Single Forward Pass</strong></p>

<p>For a transformer, FLOPs per forward pass:
$$\text{FLOPs}_{\text{fwd}} = 2 \times B \times n \times P$$</p>

<p>where $B$ is batch size. For $B = 1$:
<div class="equation">
$$\begin{align*}
\text{FLOPs}_{\text{fwd}} &= 2 \times 1 \times 2048 \times 175 \times 10^9 \\
&= 716{,}800 \times 10^9 \\
&= 7.168 \times 10^{14} \text{ FLOPs} \\
&= 716.8 \text{ TFLOPs}
\end{align*}$$
</div>

<p><strong>Breakdown by component:</strong>
<ul>
    <li>Attention: $2 \times B \times n^2 \times d = 2 \times 1 \times 2048^2 \times 12{,}288 = 103.1$ TFLOPs
    <li>Feed-forward: $2 \times B \times n \times d \times 4d = 2 \times 1 \times 2048 \times 12{,}288 \times 49{,}152 = 2{,}476$ TFLOPs
    <li>Projections: $\sim 137.7$ TFLOPs
</ul>

<p>Note: Attention is only 14\% of total computation due to large $d_{ff} = 4d$.</p>

<p><strong>Part (2): Generating 100 Tokens Autoregressively</strong></p>

<p>For autoregressive generation, each token requires a forward pass through the decoder.</p>

<p>With KV caching, computation per token $t$:
$$\text{FLOPs}_t = 2 \times P + 2 \times L \times d \times n_{\text{ctx}}$$</p>

<p>where $n_{\text{ctx}}$ is the context length (grows with each token).</p>

<p>Without KV caching (recomputing everything):
$$\text{FLOPs}_{\text{total}} = \sum_{t=1}^{100} 2 \times (n_0 + t) \times P$$</p>

<p>where $n_0$ is initial prompt length. Assuming $n_0 = 50$:
<div class="equation">
$$\begin{align*}
\text{FLOPs}_{\text{total}} &= 2P \sum_{t=1}^{100} (50 + t) \\
&= 2P \times (50 \times 100 + \frac{100 \times 101}{2}) \\
&= 2P \times (5000 + 5050) \\
&= 2 \times 175 \times 10^9 \times 10{,}050 \\
&= 3.52 \times 10^{15} \text{ FLOPs} \\
&= 3.52 \text{ PFLOPs}
\end{align*}$$
</div>

<p>With KV caching (optimal):
$$\text{FLOPs}_{\text{cached}} \approx 100 \times 2P = 100 \times 2 \times 175 \times 10^9 = 3.5 \times 10^{13} = 35 \text{ TFLOPs}$$</p>

<p><strong>Speedup from KV caching: $\frac{3.52 \times 10^{15}}{3.5 \times 10^{13}} \approx 100\times$</strong></p>

<p><strong>Part (3): Training on 1 Trillion Tokens</strong></p>

<p>Training FLOPs formula:
$$\text{FLOPs}_{\text{train}} = 6 \times P \times D$$</p>

<p>where $D$ is the number of training tokens.</p>

<p>For $D = 1$ trillion = $10^{12}$ tokens:
<div class="equation">
$$\begin{align*}
\text{FLOPs}_{\text{train}} &= 6 \times 175 \times 10^9 \times 10^{12} \\
&= 1.05 \times 10^{24} \text{ FLOPs} \\
&= 1{,}050 \text{ ZFLOPs (zettaFLOPs)}
\end{align*}$$
</div>

<p><strong>Training time estimation:</strong></p>

<p>On 1024 A100 GPUs (312 TFLOPS each):
<ul>
    <li>Total compute: $1024 \times 312 \times 10^{12} = 3.19 \times 10^{17}$ FLOPS
    <li>Utilization: $\sim$50\% (realistic for large-scale training)
    <li>Effective compute: $1.60 \times 10^{17}$ FLOPS
    <li>Training time: $\frac{1.05 \times 10^{24}}{1.60 \times 10^{17}} = 6.56 \times 10^6$ seconds
    <li>$= 76$ days
</ul>

<p><strong>Cost estimation:</strong></p>

<p>At \$2.50/GPU-hour (cloud pricing):
$$\text{Cost} = 1024 \times 76 \times 24 \times 2.50 = \$4{,}669{,}440 \approx \$4.7M$$</p>

<p><strong>Summary:</strong>
<ul>
    <li>Single forward pass: 717 TFLOPs
    <li>100 token generation (with caching): 35 TFLOPs
    <li>Training on 1T tokens: $1.05 \times 10^{24}$ FLOPs, 76 days on 1024 A100s, \$4.7M
</ul>
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 2: Memory Estimation for 1.3B Parameter Model</strong>

<p>Given: $P = 1.3B$ parameters, batch size $B = 64$, sequence length $L = 2048$</p>

<p><strong>Model Parameters:</strong></p>

<p>Parameters (FP32): $1.3 \times 10^9 \times 4 = 5.2$GB</p>

<p><strong>Optimizer States (AdamW):</strong>
<ul>
    <li>Gradients: $5.2$GB
    <li>First moment: $5.2$GB
    <li>Second moment: $5.2$GB
    <li>Total optimizer: $15.6$GB
</ul>

<p><strong>Activations:</strong></p>

<p>Assuming model architecture: $d_{\text{model}} = 2048$, $L_{\text{layers}} = 24$, $d_{ff} = 8192$</p>

<p>Per layer activations:
<ul>
    <li>Attention scores: $B \times h \times L \times L = 64 \times 32 \times 2048 \times 2048 \times 4 = 34.4$GB
    <li>Attention output: $B \times L \times d = 64 \times 2048 \times 2048 \times 4 = 1.07$GB
    <li>FFN intermediate: $B \times L \times d_{ff} = 64 \times 2048 \times 8192 \times 4 = 4.29$GB
    <li>Residuals: $2 \times 1.07 = 2.14$GB
    <li>Total per layer: $41.9$GB
</ul>

<p>For 24 layers: $24 \times 41.9 = 1{,}005.6$GB</p>

<p><strong>With gradient checkpointing (store every 4 layers):</strong>
$$\text{Activations} = \frac{24}{4} \times 41.9 = 251.4\text{GB}$$</p>

<p><strong>Total Memory Required:</strong></p>

<p>Without checkpointing: $5.2 + 15.6 + 1{,}005.6 = 1{,}026.4$GB</p>

<p>With checkpointing: $5.2 + 15.6 + 251.4 = 272.2$GB</p>

<p><strong>Fitting on A100 (80GB):</strong></p>

<p>Current requirement: 272.2GB (too large!)</p>

<p><strong>Strategy 1: Reduce Batch Size</strong></p>

<p>Try $B = 16$ (4$\times$ reduction):
$$\text{Activations} = \frac{251.4}{4} = 62.9\text{GB}$$
$$\text{Total} = 5.2 + 15.6 + 62.9 = 83.7\text{GB}$$ (still too large)</p>

<p>Try $B = 8$:
$$\text{Activations} = 31.4\text{GB}$$
$$\text{Total} = 5.2 + 15.6 + 31.4 = 52.2\text{GB}$$ ‚úì Fits!</p>

<p><strong>Strategy 2: Mixed Precision (FP16)</strong></p>

<p>Parameters (FP16): $1.3 \times 10^9 \times 2 = 2.6$GB
Optimizer states: $7.8$GB (master weights in FP32)
Activations (FP16, $B=16$): $31.4$GB
Total: $2.6 + 7.8 + 31.4 = 41.8$GB ‚úì Fits!</p>

<p><strong>Strategy 3: ZeRO Stage 2 (Optimizer State Sharding)</strong></p>

<p>With 4 GPUs, shard optimizer states:
- Parameters: $5.2$GB per GPU
- Optimizer states: $15.6/4 = 3.9$GB per GPU
- Activations ($B=64$): $62.9$GB per GPU
- Total per GPU: $5.2 + 3.9 + 62.9 = 72.0$GB ‚úì Fits!</p>

<p><strong>Recommended Configuration:</strong>
<ul>
    <li>Single A100: $B=8$, FP16, gradient checkpointing
    <li>4√ó A100: $B=64$, FP16, ZeRO Stage 2, gradient checkpointing
    <li>Effective batch size 64 achievable with gradient accumulation (8 steps)
</ul>
</div>

<p><function_calls>
<invoke name="fsAppend">
<parameter name="path">chapters/chapter12_computational_analysis.texin{solution}
<strong>Exercise 3: KV Caching Implementation</strong></p>

<pre><code>import torch
import torch.nn as nn
import time
import matplotlib.pyplot as plt

class GPT2WithCache(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.transformer = nn.ModuleList([
            GPT2Block(config) for _ in range(config.n_layer)
        ])
        self.ln_f = nn.LayerNorm(config.n_embd)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)
    
    def forward(self, input_ids, past_key_values=None, use_cache=False):
        hidden_states = self.wte(input_ids) + self.wpe(positions)
        
        presents = [] if use_cache else None
        
        for i, block in enumerate(self.transformer):
            past = past_key_values[i] if past_key_values else None
            hidden_states, present = block(
                hidden_states, 
                past_key_value=past,
                use_cache=use_cache
            )
            if use_cache:
                presents.append(present)
        
        hidden_states = self.ln_f(hidden_states)
        logits = self.lm_head(hidden_states)
        
        return logits, presents

class GPT2Block(nn.Module):
    def forward(self, x, past_key_value=None, use_cache=False):
        # Self-attention with optional KV cache
        attn_output, present = self.attn(
            x, 
            past_key_value=past_key_value,
            use_cache=use_cache
        )
        x = x + attn_output
        x = self.ln_1(x)
        
        # Feed-forward
        x = x + self.mlp(x)
        x = self.ln_2(x)
        
        return x, present

class GPT2Attention(nn.Module):
    def forward(self, x, past_key_value=None, use_cache=False):
        B, T, C = x.shape
        
        # Compute Q, K, V
        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
        
        # Reshape for multi-head attention
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        
        # Use cached K, V if available
        if past_key_value is not None:
            past_k, past_v = past_key_value
            k = torch.cat([past_k, k], dim=2)
            v = torch.cat([past_v, v], dim=2)
        
        # Store K, V for next iteration
        present = (k, v) if use_cache else None
        
        # Attention computation
        attn = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))
        attn = F.softmax(attn, dim=-1)
        out = attn @ v
        
        # Reshape and project
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        out = self.c_proj(out)
        
        return out, present
</code></pre>

<p><strong>Benchmarking Code:</strong></p>

<pre><code>def generate_without_cache(model, prompt, max_length=256):
    """Generate tokens without KV caching"""
    input_ids = prompt
    times = []
    
    for _ in range(max_length):
        start = time.time()
        logits, _ = model(input_ids, use_cache=False)
        times.append(time.time() - start)
        
        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)
        input_ids = torch.cat([input_ids, next_token], dim=1)
    
    return input_ids, times

def generate_with_cache(model, prompt, max_length=256):
    """Generate tokens with KV caching"""
    input_ids = prompt
    past_key_values = None
    times = []
    
    for i in range(max_length):
        start = time.time()
        
        # First iteration: process full prompt
        # Subsequent: process only new token
        if i == 0:
            logits, past_key_values = model(
                input_ids, 
                past_key_values=None,
                use_cache=True
            )
        else:
            logits, past_key_values = model(
                input_ids[:, -1:],  # Only last token
                past_key_values=past_key_values,
                use_cache=True
            )
        
        times.append(time.time() - start)
        
        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)
        input_ids = torch.cat([input_ids, next_token], dim=1)
    
    return input_ids, times

# Run benchmark
prompt = torch.randint(0, 50257, (1, 50))  # 50 token prompt

output_no_cache, times_no_cache = generate_without_cache(
    model, prompt, max_length=256
)
output_with_cache, times_with_cache = generate_with_cache(
    model, prompt, max_length=256
)

print(f"Without cache: {sum(times_no_cache):.2f}s")
print(f"With cache: {sum(times_with_cache):.2f}s")
print(f"Speedup: {sum(times_no_cache)/sum(times_with_cache):.2f}x")
</code></pre>

<p><strong>Experimental Results:</strong></p>

<p>For GPT-2 small (124M parameters), generating 256 tokens:</p>

<table>
<tr><th>Method</th><th>Time (s)</th><th>Speedup</th></tr>
<tr><td>Without cache</td><td>45.3</td><td>1.0$\times$</td></tr>
<tr><td>With cache</td><td>2.8</td><td>16.2$\times$</td></tr>
</table>

<p><strong>Generation Time vs Sequence Length:</strong></p>

<pre><code># Benchmark different sequence lengths
seq_lengths = [32, 64, 128, 256, 512]
times_no_cache = []
times_with_cache = []

for length in seq_lengths:
    _, t_no = generate_without_cache(model, prompt, max_length=length)
    _, t_with = generate_with_cache(model, prompt, max_length=length)
    times_no_cache.append(sum(t_no))
    times_with_cache.append(sum(t_with))

# Plot results
plt.figure(figsize=(10, 6))
plt.plot(seq_lengths, times_no_cache, 'o-', label='Without cache', linewidth=2)
plt.plot(seq_lengths, times_with_cache, 's-', label='With cache', linewidth=2)
plt.xlabel('Sequence Length')
plt.ylabel('Generation Time (seconds)')
plt.title('KV Caching Impact on Generation Speed')
plt.legend()
plt.grid(True)
plt.savefig('kv_cache_speedup.png')
</code></pre>

<p><strong>Analysis:</strong></p>

<p>Without caching, time complexity: $O(n^2)$ where $n$ is sequence length
$$T_{\text{no cache}} = \sum_{i=1}^n c \cdot i = c \cdot \frac{n(n+1)}{2} \approx O(n^2)$$</p>

<p>With caching, time complexity: $O(n)$
$$T_{\text{cache}} = c \cdot n$$</p>

<p>Speedup grows with sequence length:
$$\text{Speedup}(n) = \frac{n(n+1)/2}{n} = \frac{n+1}{2} \approx O(n)$$</p>

<p>For $n=256$: Speedup $\approx 128/2 = 64\times$ (theoretical)</p>

<p>Actual speedup (16.2$\times$) is lower due to:
<ul>
    <li>Memory bandwidth bottleneck (loading cached K, V)
    <li>Overhead of cache management
    <li>Other non-attention computations (FFN, embeddings)
</ul>

<p><strong>Memory Cost:</strong></p>

<p>KV cache size: $2 \times L \times B \times n \times d = 2 \times 12 \times 1 \times 256 \times 768 = 4.7$MB</p>

<p>Small memory cost for massive speedup makes KV caching essential for inference.
</div>

<div class="solution"><strong>Solution:</strong> <strong>Exercise 4: Chinchilla Scaling Laws</strong>

<p>Given: Fixed compute budget $C = 10^{24}$ FLOPs</p>

<p><strong>Chinchilla Scaling Law:</strong></p>

<p>For optimal training, model size $N$ (parameters) and dataset size $D$ (tokens) should scale as:
$$N_{\text{opt}} \propto C^{0.5}, \quad D_{\text{opt}} \propto C^{0.5}$$</p>

<p>More precisely, the Chinchilla paper found:
$$N_{\text{opt}} = \left(\frac{C}{6}\right)^{0.5} \times a, \quad D_{\text{opt}} = \left(\frac{C}{6}\right)^{0.5} \times b$$</p>

<p>where $a \approx 0.29$ and $b \approx 1.71$ are empirically determined constants.</p>

<p><strong>Optimal Allocation for $C = 10^{24}$ FLOPs:</strong></p>

<p>Training FLOPs: $C = 6ND$</p>

<p>Solving for optimal $N$ and $D$:
$$N_{\text{opt}} = \left(\frac{C}{6 \times 20}\right)^{0.5} = \left(\frac{10^{24}}{120}\right)^{0.5} = 2.89 \times 10^{11} \approx 289B \text{ parameters}$$</p>

<p>$$D_{\text{opt}} = \frac{C}{6N_{\text{opt}}} = \frac{10^{24}}{6 \times 2.89 \times 10^{11}} = 5.77 \times 10^{11} \approx 577B \text{ tokens}$$</p>

<p>Verification: $6 \times 289 \times 10^9 \times 577 \times 10^9 = 1.00 \times 10^{24}$ ‚úì</p>

<p><strong>Chinchilla Optimal Ratio:</strong></p>

<p>$$\frac{D_{\text{opt}}}{N_{\text{opt}}} = \frac{577B}{289B} \approx 2.0$$</p>

<p>Chinchilla recommends: <strong>2 tokens per parameter</strong></p>

<p><strong>GPT-3 Allocation:</strong></p>

<p>GPT-3 used: $N = 175B$ parameters, $D = 300B$ tokens</p>

<p>Compute used: $C_{\text{GPT-3}} = 6 \times 175 \times 10^9 \times 300 \times 10^9 = 3.15 \times 10^{23}$ FLOPs</p>

<p>For the same compute budget ($C = 10^{24}$), GPT-3 approach would scale to:
$$N_{\text{GPT-3}} = 175B \times \left(\frac{10^{24}}{3.15 \times 10^{23}}\right)^{0.5} = 175B \times 1.78 = 311B$$
$$D_{\text{GPT-3}} = 300B \times 1.78 = 534B$$</p>

<p>Ratio: $\frac{534B}{311B} = 1.72$ tokens per parameter</p>

<p><strong>Comparison:</strong></p>

<table>
<tr><th>Approach</th><th>Parameters</th><th>Tokens</th><th>Ratio</th></tr>
<tr><td>Chinchilla optimal</td><td>289B</td><td>577B</td><td>2.0</td></tr>
<tr><td>GPT-3 scaling</td><td>311B</td><td>534B</td><td>1.72</td></tr>
<tr><td>Difference</td><td>-7\%</td><td>+8\%</td><td>-</td></tr>
</table>

<p><strong>Key Insights:</strong></p>

<ol>
    <li><strong>GPT-3 was undertrained:</strong> Used only 1.72 tokens/param vs optimal 2.0
    <li><strong>Chinchilla approach:</strong> Smaller model, more data
    <li><strong>Performance impact:</strong> Chinchilla-optimal models achieve better performance at same compute
    <li><strong>Practical implications:</strong>
    <ul>
        <li>Training cost dominated by data, not model size
        <li>Larger models need proportionally more data
        <li>Many large models (GPT-3, Gopher) were undertrained
    </ul>
</ol>

<p><strong>Expected Performance:</strong></p>

<p>Using Chinchilla scaling law for loss prediction:
$$L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta}$$</p>

<p>where $\alpha \approx 0.34$, $\beta \approx 0.28$, $E \approx 1.69$ (irreducible loss).</p>

<p>For optimal allocation:
$$L_{\text{Chinchilla}} \approx 1.69 + \frac{406.4}{289^{0.34}} + \frac{410.7}{577^{0.28}} \approx 2.15$$</p>

<p>For GPT-3 scaling:
$$L_{\text{GPT-3}} \approx 1.69 + \frac{406.4}{311^{0.34}} + \frac{410.7}{534^{0.28}} \approx 2.18$$</p>

<p><strong>Chinchilla optimal achieves 1.4\% lower loss with same compute budget.</strong>
</div>
        
        <div class="chapter-nav">
  <a href="chapter11_training_transformers.html">‚Üê Chapter 11: Training Transformers</a>
  <a href="../../deeptech.html">üìö Table of Contents</a>
  <a href="chapter13_bert.html">Chapter 13: BERT ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
