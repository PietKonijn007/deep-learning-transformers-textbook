<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 7: Attention Mechanisms: Fundamentals - Deep Learning and Transformers</title>
    <link rel="stylesheet" href="../css/style.css">
    
    <!-- MathJax Configuration (must come before loading MathJax) -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
                R: '{\\mathbb{R}}',
                N: '{\\mathbb{N}}',
                Z: '{\\mathbb{Z}}',
                C: '{\\mathbb{C}}',
                vx: '{\\mathbf{x}}',
                vy: '{\\mathbf{y}}',
                vz: '{\\mathbf{z}}',
                vh: '{\\mathbf{h}}',
                vw: '{\\mathbf{w}}',
                vb: '{\\mathbf{b}}',
                vq: '{\\mathbf{q}}',
                vk: '{\\mathbf{k}}',
                vv: '{\\mathbf{v}}',
                mA: '{\\mathbf{A}}',
                mB: '{\\mathbf{B}}',
                mC: '{\\mathbf{C}}',
                mW: '{\\mathbf{W}}',
                mX: '{\\mathbf{X}}',
                mY: '{\\mathbf{Y}}',
                mQ: '{\\mathbf{Q}}',
                mK: '{\\mathbf{K}}',
                mV: '{\\mathbf{V}}',
                mH: '{\\mathbf{H}}',
                mI: '{\\mathbf{I}}',
                mU: '{\\mathbf{U}}',
                mM: '{\\mathbf{M}}',
                transpose: '{^\\top}',
                norm: ['\\left\\|#1\\right\\|', 1],
                abs: ['\\left|#1\\right|', 1]
            }
        },
        startup: {
            pageReady: () => {
                console.log('MathJax loaded and ready');
                return MathJax.startup.defaultPageReady();
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html">üè† Home</a>
        <a href="preface.html">Preface</a>
        <a href="notation.html">Notation</a>
        <a href="chapter01_linear_algebra.html">Ch 1</a>
        <a href="chapter02_calculus_optimization.html">Ch 2</a>
        <a href="chapter03_probability_information.html">Ch 3</a>
        <a href="chapter04_feedforward_networks.html">Ch 4</a>
        <a href="chapter05_convolutional_networks.html">Ch 5</a>
        <a href="chapter06_recurrent_networks.html">Ch 6</a>
        <a href="chapter07_attention_fundamentals.html">Ch 7</a>
        <a href="chapter08_self_attention.html">Ch 8</a>
        <a href="chapter09_attention_variants.html">Ch 9</a>
        <a href="chapter10_transformer_model.html">Ch 10</a>
        <a href="chapter11_training_transformers.html">Ch 11</a>
        <a href="chapter12_computational_analysis.html">Ch 12</a>
        <a href="chapter13_bert.html">Ch 13</a>
        <a href="chapter14_gpt.html">Ch 14</a>
        <a href="chapter15_t5_bart.html">Ch 15</a>
        <a href="chapter16_efficient_transformers.html">Ch 16</a>
        <a href="chapter17_vision_transformers.html">Ch 17</a>
        <a href="chapter18_multimodal_transformers.html">Ch 18</a>
        <a href="chapter19_long_context.html">Ch 19</a>
        <a href="chapter20_pretraining_strategies.html">Ch 20</a>
        <a href="chapter21_pytorch_implementation.html">Ch 21</a>
        <a href="chapter22_hardware_optimization.html">Ch 22</a>
        <a href="chapter23_best_practices.html">Ch 23</a>
    </nav>

    <main>
        <h1>Attention Mechanisms: Fundamentals</h1>

<h2>Chapter Overview</h2>

<p>Attention mechanisms revolutionized sequence modeling by allowing models to focus on relevant parts of the input when producing each output. This chapter introduces attention from first principles, developing the query-key-value paradigm that underpins modern transformers.</p>

<p>Attention solves a fundamental limitation of RNN encoder-decoder models: compressing entire input sequence into single fixed-size vector. Instead, attention computes dynamic, context-dependent representations by weighted combination of all input positions.</p>

<h3>Learning Objectives</h3>

<ol>
    <li>Understand the motivation for attention in sequence-to-sequence models
    <li>Master the query-key-value attention paradigm
    <li>Implement additive (Bahdanau) and multiplicative (Luong) attention
    <li>Understand scaled dot-product attention
    <li>Compute attention weights and apply to values
    <li>Visualize and interpret attention distributions
</ol>

<h2>Motivation: The Seq2Seq Bottleneck</h2>

<h3>RNN Encoder-Decoder Architecture</h3>

<p><strong>Problem setup:</strong> Translate input sequence $\vx_1, \ldots, \vx_n$ to output sequence $\vy_1, \ldots, \vy_m$</p>

<p><strong>Standard approach (pre-attention):</strong>
<ol>
    <li><strong>Encoder RNN:</strong> Process input, produce final hidden state $\mathbf{c}$ (context vector)
    <div class="equation">
$$
    \vh_t^{\text{enc}} = \text{RNN}(\vx_t, \vh_{t-1}^{\text{enc}}), \quad \mathbf{c} = \vh_n^{\text{enc}}
    $$
</div>

<p><li><strong>Decoder RNN:</strong> Generate output conditioned on $\mathbf{c}$
    <div class="equation">
$$
    \vh_t^{\text{dec}} = \text{RNN}([\vy_{t-1}, \mathbf{c}], \vh_{t-1}^{\text{dec}})
    $$
</div>
</ol>

<p><strong>Bottleneck:</strong> Entire input sequence compressed into single fixed-size vector $\mathbf{c}$! 
<ul>
    <li>Long sequences: information loss
    <li>All input words contribute equally
    <li>Performance degrades with sequence length
</ul>

<h3>Attention Solution</h3>

<p><strong>Key insight:</strong> When generating output word $\vy_t$, different input words have different relevance.</p>

<p><strong>Attention mechanism:</strong>
<ul>
    <li>Compute <strong>context vector</strong> $\mathbf{c}_t$ for each output position $t$
    <li>$\mathbf{c}_t$ is weighted sum of all encoder hidden states
    <li>Weights reflect relevance of each input to current output
</ul>

<div class="example"><strong>Example:</strong> 
Translate "The cat sat on the mat" to "Le chat √©tait assis sur le tapis"

<p>When generating "chat" (cat), attention should focus on "cat" in input.</p>

<p>When generating "assis" (sat), attention should focus on "sat".</p>

<p>Attention weights adapt dynamically based on what decoder is generating!
</div>

<h2>Additive Attention (Bahdanau)</h2>

<div class="definition"><strong>Definition:</strong> 
Given:
<ul>
    <li>Encoder hidden states: $\vh_1, \ldots, \vh_n \in \R^{d_h}$
    <li>Decoder hidden state at time $t$: $\mathbf{s}_t \in \R^{d_s}$
</ul>

<p><strong>Step 1: Compute alignment scores</strong>
<div class="equation">
$$
e_{t,i} = \mathbf{v}\transpose \tanh(\mW_1 \mathbf{s}_t + \mW_2 \vh_i)
$$
</div>
where $\mW_1 \in \R^{d_a \times d_s}$, $\mW_2 \in \R^{d_a \times d_h}$, $\mathbf{v} \in \R^{d_a}$, and $d_a$ is attention dimension.</p>

<p><strong>Step 2: Compute attention weights (softmax)</strong>
<div class="equation">
$$
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^{n} \exp(e_{t,j})}
$$
</div>

<p><strong>Step 3: Compute context vector</strong>
<div class="equation">
$$
\mathbf{c}_t = \sum_{i=1}^{n} \alpha_{t,i} \vh_i
$$
</div>

<p><strong>Step 4: Use in decoder</strong>
<div class="equation">
$$
\mathbf{s}_t = \text{RNN}([\vy_{t-1}, \mathbf{c}_t], \mathbf{s}_{t-1})
$$
</div>
</div>

<div class="keypoint">
Attention weights $\alpha_{t,i}$ form probability distribution: $\alpha_{t,i} \geq 0$ and $\sum_{i=1}^n \alpha_{t,i} = 1$.
Context vector $\mathbf{c}_t$ is weighted average of encoder states.
</div>

<div class="example"><strong>Example:</strong> 
Setup:
<ul>
    <li>Encoder hidden states: $\vh_1, \vh_2, \vh_3 \in \R^{4}$
    <li>Decoder state: $\mathbf{s}_2 \in \R^{4}$
    <li>Attention dimension: $d_a = 3$
</ul>

<p><strong>Step 1:</strong> Compute scores for each encoder position
<div class="equation">
$$\begin{align}
e_{2,1} &= \mathbf{v}\transpose \tanh(\mW_1 \mathbf{s}_2 + \mW_2 \vh_1) \in \R \\
e_{2,2} &= \mathbf{v}\transpose \tanh(\mW_1 \mathbf{s}_2 + \mW_2 \vh_2) \in \R \\
e_{2,3} &= \mathbf{v}\transpose \tanh(\mW_1 \mathbf{s}_2 + \mW_2 \vh_3) \in \R
\end{align}$$
</div>

<p>Suppose: $e_{2,1} = 0.8$, $e_{2,2} = 2.1$, $e_{2,3} = 0.5$</p>

<p><strong>Step 2:</strong> Softmax to get weights
<div class="equation">
$$\begin{align}
\sum_j \exp(e_{2,j}) &= \exp(0.8) + \exp(2.1) + \exp(0.5) \\
&\approx 2.23 + 8.17 + 1.65 = 12.05 \\
\alpha_{2,1} &= \frac{2.23}{12.05} \approx 0.185 \\
\alpha_{2,2} &= \frac{8.17}{12.05} \approx 0.678 \\
\alpha_{2,3} &= \frac{1.65}{12.05} \approx 0.137
\end{align}$$
</div>

<p>Most attention (67.8\%) on position 2!</p>

<p><strong>Step 3:</strong> Compute context
<div class="equation">
$$
\mathbf{c}_2 = 0.185 \vh_1 + 0.678 \vh_2 + 0.137 \vh_3 \in \R^{4}
$$
</div>
</div>

<h2>Scaled Dot-Product Attention</h2>

<div class="definition"><strong>Definition:</strong> 
Given queries $\mQ \in \R^{m \times d_k}$, keys $\mK \in \R^{n \times d_k}$, values $\mV \in \R^{n \times d_v}$:

<p><strong>Step 1: Compute attention scores</strong>
<div class="equation">
$$
\mE = \mQ \mK\transpose \in \R^{m \times n}
$$
</div>
Score $e_{i,j}$ measures compatibility of query $i$ with key $j$.</p>

<p><strong>Step 2: Scale by $\sqrt{d_k</strong>$}
<div class="equation">
$$
\mE_{\text{scaled}} = \frac{\mQ \mK\transpose}{\sqrt{d_k}}
$$
</div>

<p><strong>Step 3: Softmax over keys (row-wise)</strong>
<div class="equation">
$$
\mA = \text{softmax}\left(\frac{\mQ \mK\transpose}{\sqrt{d_k}}\right) \in \R^{m \times n}
$$
</div>

<p><strong>Step 4: Apply attention to values</strong>
<div class="equation">
$$
\text{Attention}(\mQ, \mK, \mV) = \mA \mV \in \R^{m \times d_v}
$$
</div>
</div>

<p><strong>Complete formula:</strong>
<div class="equation">
$$
\text{Attention}(\mQ, \mK, \mV) = \text{softmax}\left(\frac{\mQ \mK\transpose}{\sqrt{d_k}}\right) \mV
$$
</div>

<div class="keypoint">
<strong>Why scale by $\sqrt{d_k</strong>$?}

<p>For large $d_k$, dot products grow large in magnitude, pushing softmax into regions with tiny gradients. Scaling keeps values moderate, maintaining good gradient flow.</p>

<p>If $\mQ$ and $\mK$ have unit variance elements:
<div class="equation">
$$
\text{Var}(\vq\transpose \vk) = d_k \quad \Rightarrow \quad \text{Var}\left(\frac{\vq\transpose \vk}{\sqrt{d_k}}\right) = 1
$$
</div>
</div>

<div class="example"><strong>Example:</strong> 
Query-key-value dimensions: $d_k = 4$, $d_v = 5$

<p>Single query attending to 3 keys:
<div class="equation">
$$
\vq = \begin{bmatrix} 1.0 \\ 0.5 \\ -0.3 \\ 0.8 \end{bmatrix}, \quad
\mK = \begin{bmatrix}
0.8 & 0.2 & -0.1 & 0.5 \\
0.3 & 0.7 & 0.4 & -0.2 \\
-0.5 & 0.1 & 0.9 & 0.6
\end{bmatrix}
$$
</div>

<p><strong>Step 1:</strong> Compute dot products
<div class="equation">
$$\begin{align}
\vq\transpose \vk_1 &= 1.0(0.8) + 0.5(0.2) + (-0.3)(-0.1) + 0.8(0.5) = 1.33 \\
\vq\transpose \vk_2 &= 1.0(0.3) + 0.5(0.7) + (-0.3)(0.4) + 0.8(-0.2) = 0.43 \\
\vq\transpose \vk_3 &= 1.0(-0.5) + 0.5(0.1) + (-0.3)(0.9) + 0.8(0.6) = -0.22
\end{align}$$
</div>

<p><strong>Step 2:</strong> Scale by $\sqrt{d_k} = \sqrt{4} = 2$
<div class="equation">
$$
\text{scores} = [0.665, 0.215, -0.110]
$$
</div>

<p><strong>Step 3:</strong> Softmax
<div class="equation">
$$
\sum_j \exp(\text{score}_j) \approx 1.95 + 1.24 + 0.90 = 4.09
$$
</div>
<div class="equation">
$$
\boldsymbol{\alpha} = [0.477, 0.303, 0.220]
$$
</div>

<p><strong>Step 4:</strong> Apply to values (suppose $\mV \in \R^{3 \times 5}$)
<div class="equation">
$$
\text{output} = 0.477 \vv_1 + 0.303 \vv_2 + 0.220 \vv_3 \in \R^{5}
$$
</div>
</div>

<h2>Query-Key-Value Paradigm</h2>

<h3>Intuition</h3>

<p><strong>Analogy:</strong> Information retrieval system
<ul>
    <li><strong>Query ($\vq$):</strong> What I'm looking for
    <li><strong>Keys ($\vk_i$):</strong> Indexed content descriptions
    <li><strong>Values ($\vv_i$):</strong> Actual content to retrieve
</ul>

<p><strong>Process:</strong>
<ol>
    <li>Compare query to all keys (compute similarity)
    <li>Convert similarities to weights (softmax)
    <li>Retrieve weighted combination of values
</ol>

<h3>Projecting to QKV</h3>

<p>In transformers, $\mQ$, $\mK$, $\mV$ are computed from inputs via learned projections:</p>

<div class="equation">
$$\begin{align}
\mQ &= \mX \mW^Q && \mW^Q \in \R^{d_{\text{model}} \times d_k} \\
\mK &= \mX \mW^K && \mW^K \in \R^{d_{\text{model}} \times d_k} \\
\mV &= \mX \mW^V && \mW^V \in \R^{d_{\text{model}} \times d_v}
\end{align}$$
</div>

<p>where $\mX \in \R^{n \times d_{\text{model}}}$ is the input.</p>

<div class="example"><strong>Example:</strong> 
Input: Sequence of 5 tokens, each $d_{\text{model}} = 512$ dimensions
<div class="equation">
$$
\mX \in \R^{5 \times 512}
$$
</div>

<p>Project to $d_k = d_v = 64$:
<div class="equation">
$$\begin{align}
\mQ &= \mX \mW^Q \in \R^{5 \times 64} \quad (\mW^Q \in \R^{512 \times 64}) \\
\mK &= \mX \mW^K \in \R^{5 \times 64} \quad (\mW^K \in \R^{512 \times 64}) \\
\mV &= \mX \mW^V \in \R^{5 \times 64} \quad (\mW^V \in \R^{512 \times 64})
\end{align}$$
</div>

<p><strong>Attention computation:</strong>
<div class="equation">
$$
\underbrace{\text{softmax}\left(\frac{\mQ \mK\transpose}{\sqrt{64}}\right)}_{\R^{5 \times 5}} \underbrace{\mV}_{\R^{5 \times 64}} = \underbrace{\text{Output}}_{\R^{5 \times 64}}
$$
</div>

<p>Attention matrix $\mA \in \R^{5 \times 5}$: entry $(i,j)$ is attention from position $i$ to position $j$.
</div>

<h2>Attention Variants</h2>

<h3>Self-Attention vs Cross-Attention</h3>

<p><strong>Self-Attention:</strong> $\mQ$, $\mK$, $\mV$ all from same source
<div class="equation">
$$
\mQ = \mK = \mV = \mX \mW
$$
</div>
Used in: Transformer encoder, BERT</p>

<p><strong>Cross-Attention:</strong> Queries from one source, keys and values from another
<div class="equation">
$$
\mQ = \mX_{\text{dec}} \mW^Q, \quad \mK = \mV = \mX_{\text{enc}} \mW^{K/V}
$$
</div>
Used in: Transformer decoder (attending to encoder output)</p>

<h3>Masked Attention</h3>

<p>For autoregressive models (GPT), prevent attending to future positions:
<div class="equation">
$$
\text{Attention}(\mQ, \mK, \mV) = \text{softmax}\left(\frac{\mQ \mK\transpose + \mM}{\sqrt{d_k}}\right) \mV
$$
</div>
where mask $\mM_{ij} = -\infty$ if $j > i$, else $\mM_{ij} = 0$.</p>

<p>After softmax, $\exp(-\infty) = 0$, so no attention to future!</p>

<h2>Exercises</h2>

<p>\begin{exercise}
Compute Bahdanau attention for sequence length 4, decoder state dim 3, attention dim 2. Given specific $\mW_1$, $\mW_2$, $\mathbf{v}$, encoder states, and decoder state, calculate all attention weights.
</div>

<p>\begin{exercise}
For scaled dot-product attention with $\mQ \in \R^{10 \times 64}$, $\mK \in \R^{20 \times 64}$, $\mV \in \R^{20 \times 128}$: (1) What is output dimension? (2) What is attention matrix shape? (3) How many FLOPs for computing $\mQ \mK\transpose$?
</div>

<p>\begin{exercise}
Show that without scaling, for $d_k = 64$ and unit variance elements, dot products have variance 64. Demonstrate numerically how this affects softmax gradients.
</div>

<p>\begin{exercise}
Implement scaled dot-product attention in PyTorch. Test with sequences of length 5 and 10, dimensions $d_k = 32$, $d_v = 48$. Visualize attention weights as heatmap.
</div>
        
        <div class="chapter-nav">
  <a href="chapter06_recurrent_networks.html">‚Üê Chapter 6: Recurrent Neural Networks</a>
  <a href="../index.html">üìö Table of Contents</a>
  <a href="chapter08_self_attention.html">Chapter 8: Self-Attention and Multi-Head Attention ‚Üí</a>
</div>

    </main>

    <footer>
        <p>&copy; 2026 Deep Learning and Transformers Textbook. All rights reserved.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
